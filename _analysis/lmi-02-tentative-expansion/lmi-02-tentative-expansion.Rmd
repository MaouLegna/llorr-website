---
title: "LoR Meta Index (LMI) expansion by using Bo3 data"
description: |
  The LMI is limited in its current setting as it uses only playrates and win-rates. By using Bo3 data we propose a way to expand the LMI with a ban-index
base_url: http://www.llorr-stats.com
preview:
author:
  - name: Valentino (Legna) Vazzoler
date: 09-12-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true    
    toc_depth: 3
    self_contained: false
citation: false
draft: TRUE
twitter:
  site: "@Maou_Legna"
  creator: "@Maou_Legna"
params:
  # prev:  "2021-07-07 21:00:00" #UTC tz / 'previous' week start
  start: "2021-07-07 21:00:00" #UTC tz / 'current' week start
  end:   "2021-08-25 21:00:00" #UTC tz / 'current' week end
  # skip:  1850000  # Patch 2.11 - after removing a few games  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=9,
  fig.height=6,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
source(file.path("C:","LlorR","scripts","lor_main.R" ))

# require(Hmisc)    # provides knitrSet and other functions
xaringanExtra::use_panelset()
#' Python
# py_run_string("print('Hello World')")
# lor_deckcodes <- import("lor_deckcodes")
# py_module_available("lor_deckcodes")
```

```{r twitter-meta, echo = FALSE}
# library(metathis)
metathis::meta() |>
  metathis::meta_description(
    "The LMI is limited in its current setting as it uses only playrates and win-rates. By using Bo3 data we propose a way to expand the LMI with a ban-index"
  ) |> 
  metathis::meta_viewport() |> 
  metathis::meta_social(
    title = "LoR Meta Index (LMI) expansion by using Bo3 data",
    url = "https://www.llorr-stats.com",
    image = "images/LMIv2",
    image_alt = "LMIv2",
    og_type = "website",
    og_author = "Legna",
    twitter_card_type = "summary",
    twitter_creator = "@Maou_Legna"
  )
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(font_family = "Roboto",
                                   active_foreground = "white",
                                   hover_foreground = "black",
                                   hover_border_color = "black",
                                   active_background = "#007fff"
                                   )
```

```{r raw-data}
#' load gameDT
#'############
file.DT <- file.path("C:","LlorR","data","raw","LoR_MatchDT.csv")
# header        <- fread(file.DT, header = FALSE, na.strings = c("",NA), nrows = 1, stringsAsFactors = FALSE)
# LoR.Match.RMD <- fread(file.DT, header = FALSE, na.strings = c("",NA), skip = params$skip ) # ~2.11/2.12
# colnames(LoR.Match.RMD) <- unlist(header,use.names = F)

LoR.Match.RMD <- fread(file.DT, header = TRUE, na.strings = c("",NA) ) 

#' load Account
#'#############
file.Account <- file.path("C:","LlorR","data","raw","LoR_ACCOUNT.csv")
LoR.Account.RMD <- fread(file.Account, header=T, na.strings = c("",NA), encoding = 'UTF-8') %>%
  mutate( RiotID = paste(gameName,tagLine),refID = puuid_4 ) %>%
  pivot_longer(
  cols = c("puuid","puuid_1","puuid_2","puuid_3","puuid_4"),
  names_to = "origin",
  values_to = "puuid"
)

#' load DeckDT
#'############
LoR.Deck        <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))
```

```{r WR-2021-06}
seasonalDate <- "2021-06-19"
filterDate <- ymd_hms(glue::glue("{seasonalDate} 11:55:00")) + lubridate::minutes(5)

WR.DT.Ladder_2021_06 <- LoR.Match.RMD |>
  #' Base filters
  ###############
  filter( game_type == 'Ranked' ) |>
  filter( game_start_time_utc >= (as.POSIXct(filterDate)-days(14)) & game_start_time_utc < as.POSIXct(filterDate) ) |>
  #' 'process' data
  #################
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "player_1")   ,by=c("deck_code_1"="deck_code")) |>
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "opponent_1") ,by=c("deck_code_2"="deck_code")) |>
  mutate( player_2 = opponent_1, opponent_2 = player_1, oppoppuid_1 = puuid_2, oppoppuid_2 = puuid_1 ) |>
  select( match_key,starts_with("player"),starts_with("opponent"),starts_with("game_outcome"),ends_with("_2"),-ends_with("_3"),-ends_with("_4") ) %>%
  # get_dupes(match_key)
  #' melt data
  ############
  melt(id.vars=c("match_key"), measure.vars=patterns( str_sub(
      names(select(.,ends_with("_1")))
      ,end = -3)
  ),
  value.name = str_sub(
    names(select(.,ends_with("_1")))
    ,end = -3)
  ) |>
  # LoR.Melt.Matches.RMD |> 
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

WR.DT_2021_06 <- fread(file.path("C:","LlorR","data","clean","LoR_Seasonal.csv"),na.strings = c("",NA)) |>
  filter( game_start_time_utc > (as.POSIXct(filterDate)-days(2)) & game_start_time_utc < (as.POSIXct(filterDate)+days(2)) ) |>
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )
```


```{r table-ban-2021-06}
#' read game-result
LoR.Seasonal.RMD_202106  <- fread(file.path("C:","LlorR","data","clean","Seasonal_2021_06.csv"),na.strings = c("",NA), encoding = "UTF-8")

#' create lineUp
LineUp.DT_202106 <- LoR.Seasonal.RMD_202106 |>
  select(userID,starts_with("deck_"),LU) |>
  distinct()

#' all deck for bans
deckForBan <- LineUp.DT_202106 %>%
  filter(!is.na(deck_3) ) %>%
  # filter(!is.na(deck_3) & server!="asia") %>%
  select(contains("deck")) %>%
  unlist(.,use.names = F) %>% unique() %>% sort()
        
#' create Ban DT      
################
ban.tbl <- LoR.Seasonal.RMD_202106 |>
  filter(!is.na(deck_3) ) |>
  group_by( ban ) |> # so accounting the cases where I know the bans
  count(ban) |>
  filter( !is.na(ban) )

ban.DT <- tibble( deck = deckForBan ) |>
    left_join(ban.tbl |> select(deck = ban,nBan=n) ,by = "deck")

setDT(ban.DT)

for (i in 1:NROW(ban.DT) ) {
  deck <- pull(ban.DT[i,"deck"])
  
  whichLU <- LoR.Seasonal.RMD_202106 %>%
    filter(!is.na(deck_3) ) %>%
    filter( deck_1 == deck | deck_2 == deck | deck_3 == deck ) %>%
    pull(LU)
  
  ban.DT[i,maxBan := LoR.Seasonal.RMD_202106 |>
    filter( LU %in% whichLU ) |>
    filter( !is.na(ban) ) |>
    NROW() ]
}

WR.DT_2021_06 <- ban.DT |>  # start with ban.DT as it contains the list of all decks that theoretically can appears
  left_join(WR.DT_2021_06,by=c("deck"="player")) |> # add the Seasonal data
  mutate(across(c(nBan,nWin, nGames,playrate), ~replace_na(.x, 0)) ) |>
  # mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill the missing values
  mutate( meanBan = nBan/maxBan ) |>   # since the number of ban considers all the cases with ban information it is some to impute 0
  left_join(WR.DT.Ladder_2021_06 |> select(player,lplayrate=playrate,lWR=WR),by=c("deck"="player")) # add ladder information
  # |> mutate(across(everything(), ~replace_na(.x, 0)) )


# WR.DT_2021_06 %$%
#   cor(WR,lWR)
```

```{r Ladder-Win-Rates-Seasonal-2021-08}
seasonalDate <- "2021-08-14"
filterDate <- ymd_hms(glue::glue("{seasonalDate} 11:55:00")) + lubridate::minutes(5)

WR.DT.Ladder_2021_08 <- LoR.Match.RMD |>
  #' Base filters
  ###############
  filter( game_type == 'Ranked' ) |>
  filter( game_start_time_utc >= (as.POSIXct(filterDate)-days(14)) & game_start_time_utc < as.POSIXct(filterDate) ) |>
  #' 'process' data
  #################
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "player_1")   ,by=c("deck_code_1"="deck_code")) |>
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "opponent_1") ,by=c("deck_code_2"="deck_code")) |>
  mutate( player_2 = opponent_1, opponent_2 = player_1, oppoppuid_1 = puuid_2, oppoppuid_2 = puuid_1 ) |>
  select( match_key,starts_with("player"),starts_with("opponent"),starts_with("game_outcome"),ends_with("_2"),-ends_with("_3"),-ends_with("_4") ) %>%
  # get_dupes(match_key)
  #' melt data
  ############
  melt(id.vars=c("match_key"), measure.vars=patterns( str_sub(
      names(select(.,ends_with("_1")))
      ,end = -3)
  ),
  value.name = str_sub(
    names(select(.,ends_with("_1")))
    ,end = -3)
  ) |>
  # LoR.Melt.Matches.RMD |> 
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

WR.DT_2021_08 <- fread(file.path("C:","LlorR","data","clean","LoR_Seasonal.csv"),na.strings = c("",NA)) |>
  filter( game_start_time_utc > (as.POSIXct(filterDate)-days(2)) & game_start_time_utc < (as.POSIXct(filterDate)+days(2)) ) |>
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

# WR.DT_2021_08 |>
#   arrange(desc(nGames))
```

```{r table-ban-2021-08}
#' read game-result
LoR.Seasonal.RMD_202108  <- fread(file.path("C:","LlorR","data","clean","Seasonal_2021_08.csv"),na.strings = c("",NA), encoding = "UTF-8")

#' create lineUp
LineUp.DT_202108 <- LoR.Seasonal.RMD_202108 |>
  select(userID,starts_with("deck_"),LU) |>
  distinct()

#' all deck for bans
deckForBan <- LineUp.DT_202108 %>%
  filter(!is.na(deck_3) ) %>%
  # filter(!is.na(deck_3) & server!="asia") %>%
  select(contains("deck")) %>%
  unlist(.,use.names = F) %>% unique() %>% sort()
        
#' create Ban DT      
################
ban.tbl <- LoR.Seasonal.RMD_202108 |>
  filter(!is.na(deck_3) ) |>
  group_by( ban ) |>
  count(ban) |>
  filter( !is.na(ban) )

ban.DT <- tibble( deck = deckForBan ) |>
    left_join(ban.tbl |> select(deck = ban,nBan=n) ,by = "deck")

setDT(ban.DT)

for (i in 1:NROW(ban.DT) ) {
  deck <- pull(ban.DT[i,"deck"])
  
  whichLU <- LoR.Seasonal.RMD_202108 %>%
    filter(!is.na(deck_3) ) %>%
    filter( deck_1 == deck | deck_2 == deck | deck_3 == deck ) %>%
    pull(LU)
  
  ban.DT[i,maxBan := LoR.Seasonal.RMD_202108 |>
    filter( LU %in% whichLU ) |>
    filter( !is.na(ban) ) |>
    NROW() ]
}

WR.DT_2021_08 <- ban.DT |>  # start with ban.DT as it contains the list of all decks that theoretically can appears
  left_join(WR.DT_2021_08,by=c("deck"="player")) |> # add the Seasonal data
  mutate(across(c(nBan,nWin, nGames,playrate), ~replace_na(.x, 0)) ) |>
  # mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill the missing values
  mutate( meanBan = nBan/maxBan ) |>   # since the number of ban considers all the cases with ban information it is some to impute 0
  left_join(WR.DT.Ladder_2021_08 |> select(player,lplayrate=playrate,lWR=WR),by=c("deck"="player")) # add ladder information
  # mutate(across(everything(), ~replace_na(.x, 0)) ) # fill again

# WR.DT_2021_08 %$%
#   cor(WR,lWR)
```

# Introduction

Composite Indicator (CI) are a quantitative measure that aggregate multi-dimensional data into a single index. Differently from other aggregation methods as Principal Components Analysis (PCA) or Factorial Analysis(FA) they are not entirely data driven and they are compiled in other to communicate a concept. Mostly used in social or policy evoluation, they allows for a single and direct comparison between their units. Gamers commonly and intuitively use this tool when talking about tier list. In a card game like Hearthstone (HS) a simple but known example of CI is the Meta Score from [viciousSyndicate](https://www.vicioussyndicate.com) (vS).

In Legends of Runeterra (LoR) we created a similar CI defined as LoR-Meta-Index (LMI)[^1]. The index didn't just try to replicate the vS Meta Score but tried to adjust it to the LoR data and its differences from HS. A limitation of the proposed index was that it is 'limited' as it use only two variables, playrates and win-rates of a deck which would have been expanded once we could find additional (and appropriate) variables to add to it. A natural candidate as a third variable is the ban-rate of a deck (in a contest of BoX matches)

An experiment to add a third variable was done earlier this year, for 'Rise of the Underworld - Seasonal Tournament' report[^2].

The inclusion of such variable, was done without checking all the proper steps so that we wouldn't compromise the quality of the CI. In this article we introduce better the concept and framework of a CI, how to add the the information of the ban rates and their results.

Following the necessary steps, the proposed variation of the LMI contradicts the expected theoretical-framework while confirming the past approach.

[^1]: [LMI - early concept](https://www.llorr-stats.com/analysis/lmi/)

[^2]: [LMI with Ban Rate - from Seasonal Rerport](https://www.llorr-stats.com/report/seasonal-001/#lmi---tournament-edition)

# Data

I only consider the decks that appears in the cases of a full-line-up and whose I can extrapolate the banned deck.

Values from the ladder are taken from Master players up to two weeks before the start of the first game of the Seasonal (Asian region)

Lack of data for a deck (no games) is considered 0 playrate and 0 winrate

```{r create-gt-summary}
gtSeasonal <- LoR.Seasonal.RMD |>
  select(server) |>
  gtsummary::tbl_summary() |>
  gtsummary::as_gt() |>
  gt::tab_header(
    title = "Bo3 Data",
    subtitle = "Matches by Server"
  ) |>
  tab_source_note(
    source_note = md(glue::glue("Bo3 Data from Seasonal Open Rounds - Rise of the Underworld Open Rounds Matches - games extracted with Riot API"))
  )

gtSeasonal |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )

# gtSeasonal
```

All games from this analysis are from the 'Rise of the Underworld - Seasonal Tournament - Open Rounds'. This section of the tournament is organized in a series of nine Bo3 Matches with open lists and a ban phase before the start of the games. The smaller amount of games from the asian shard/server is allegedly because of the fewer players taking part of it[^3]

Not all information about the tournament can be derived from the API. There is direct data about the chosen ban deck or the entire line-up brought by a player at this have to be extracted by aggregating the metadata of games from a single match and the matches with other matches.

[^3]: No official data are known at the moment of the writing. Supposition made from a series of points like the fewer amoutn of Master rank players when the cutoff takes place.

# Methods

## Composite Indicators - A Better Introduction

The LMI is a composite indicator (CI) and in a previous article we introduced the tool we gave a brief explanation about how to create them. Here, we want to give a better and more complete overview of the tool.

> For a manual on CI, a commonly referred guide is from the Joint Research Centre (JRC) of the European Commission: [Handbook on Constructing Composite Indicators - METHODOLOGY AND USER GUIDE](https://www.oecd.org/sdd/42495745.pdf)

1.  Theoretical Framework

> Provide the basis for the selection and combination of variables into a meaningfu composite indicator under a fitness-for-porpuse principle (involment of experts and stakeholders is envisaged at this step)

2.  Data selection

> Should be based on the analytical soundness, measurability, coverage and relevance of the indicators to the phenomenon being measured and relationship to each other. The use of proxy variables should be considered when data are scarce (involvement of experts and stakeholders is envisaged at this step)

When defining the CI structure there is also the need to maintain a coherent structure. This means, among other, that values in the same sub-dimension should all follows the same direction. In an increase of a variable imply an increase in the final value of the sub-dimension index then all the other variables should be same.

Example: if we have a sub-dimension index related to "quality of life" containing life expectancy and child mortality, the higher the value of life expectancy the better and higher the final index should be. But, for the values of child mortality it is the opposite, the smaller the value, the better it is. In this case it's not a problem as a common practise is to just use the opposite values by changing the sign as it is a linear transformation and the smaller the value of child mortality (with opposite sign) the better it is in evalutating "quality of life".

The ‘polarity’ of an individual indicator is the sign of the
relation between the indicator and the concept to be measured. For
example, in the case of well-being, “Life expectancy” has positive
polarity, whereas “Unemployment rate” has negative polarity. I

3.  Imputation of missing data

> Is needed in order to provide a complete dataset (e.g by means or multiple imputation).

4.  Multivariate analysis

> Should be used to study the overall structure of the dataset, assess its suitability, and guide subsequent methodological choices (e.g. weighting aggregation).

5.  Normalization

> Should be carried out to render the variables comparable

6.  Weighting and aggregation

> Should be done along the lines of the underying theoretical framework

7.  Uncertainty and sensitivity analysis

> Should be undertaken to assess the robustness of the composite indicator in terms of e.g. the mechanism for indulging or excluding an indicator, the normalization scheme, the imputation of missing data, the choice, the choice of weights, the aggregation method

8.  Back to the data

> Is needed to reveal the main drivers for an overall good or bad performance. Trasparency is promordial to good analysis

9.  Links to other indicators

> Should be made to correlate the composite indicator (or its dimension) with existing (simple or composite) indicators as well as to identify linkages through regressions.

10. Visualization of the results

> Should receive proper attention, given that the visualization can influence (or help to enhance interpretability)

## The LMI Composite Indicator

### Base LMI

The basic LMI is made by aggregating playrates and win-rates

The LMI concept was inspirated by the meta score on vS

It used only two variables

As it was only two variables there are no different way to aggregate the variables

### Adding Banrate Information

As the number of variables increase from the two of the base-LMI to three we now have more options as to combine the variables. Normally, this doesn't mean that each possible choice should be evaluated, the definition we want to communicate should guide our choices and so the characteristics of our variables, e.g. we don't add a variable of Life expectancy in a subdimension of 'Infrastructure quality'

When creating the LMI I described it as a measure of *performance* of a deck and following that giving as performance's definition:

> The performance of a deck is defined by its own strength and popularity inside the metagame.

The definition of performance is probably not be as functional as it should as it seems a bit limiting in what it means. Yet, the term performance still remaing appropriate.

What information can be added to the index? An easy inspiration can be taken from Riot's main games: League of Legends (LoL)

In LoL a common value to describe the performance of a champion, in in addition to playrates and win-rates is the ban-rate of a champion.

The most infamous value of the ban-rate was 95% ban-rate of Kassadin in S3. If we consider the definition of performance given earier we can see that the ban-rate doesn't fir perfectly strength or popularity in the metagame but it is more a case in the middle. It can seen as an aspect of strength as people don't want to deal with it so banning it but it can be as an aspect of popularity or better yet a more general *presence* as while it may have not been played it sort of lingered in the match. It was not present directly in the match but with its spirit (the ban).

In the context of LoR such information can be added once we consider BoX data, currently only Bo3 with the easiest example being the Seasonal Tournaments.

Its value is defined as following:

-   **Ban Rate** - ratio between the number of bans and the number of matches of a deck.

\begin{equation}

BanRate = \frac{\#ban}{\#match}

\end{equation}

Example: 2 Line-Ups contained a Teemo/Ezreal deck, both played all 9 matches and Teemo/Ezreal was banned respectively 3 and 6 times; the ban rate would be $\frac{(3+6)}{(9+9)} = 50\%$

As the variable is deemed appropriate for the LMI purpose the following step is to define the structure of the LMI to account for the new variable.

With just three variables the possible ways to combine them are exactly three as shown in Fig:\@ref(fig:example-LMI-framework-1), Fig:\@ref(fig:example-LMI-framework-2) and Fig:\@ref(fig:example-LMI-framework-3)

```{r example-LMI-framework-1, fig.cap="(1/3) Possible theoretical framework for the LMI - All the variables are part of their own subdimension" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  IP11 [label = 'PlayRate Index']
  
  IB10 [label = 'Fear/Annoyance(?) \n subdimension']
  IB11 [label = 'BanRate Index']
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10,IB10} [dir=back]
  IS10 -> {IS11} [dir=back]
  IP10 -> {IP11} [dir=back]
  IB10 -> {IB11} [dir=back]
  }",
  height = 300)
```

```{r example-LMI-framework-2, fig.cap="(2/3) Possible theoretical framework for the LMI - retaining two main subdimension of the base LMI, ban-rate and win-rate are both used to measure the 'strenght' of a deck" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  IS12 [label = 'BanRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  
  IP11 [label = 'PlayRate Index']
  
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10} [dir=back]
  IS10 -> {IS11,IS12} [dir=back]
  IP10 -> {IP11} [dir=back]
  }",
  height = 300)
```

```{r example-LMI-framework-3, fig.cap="(3/3) Possible theoretical framework for the LMI - retaining two main subdimension of the base LMI, ban-rate and playrate are both used to measure the 'presence' of a deck" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  
  IP11 [label = 'PlayRate Index']
  IP12 [label = 'BanRate Index']
  
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10} [dir=back]
  IS10 -> IS11 [dir=back]
  IP10 -> {IP11,IP12} [dir=back]
  }",
  height = 300)
```

Each different structure correspond to a different way to see the ban-rate relationship with the other variables.

- In the first structure it is considered a different characteristic altogether in comparison to play-rates and win-rates.

- In the second structure the ban-rate of a deck is considered a part of its 'strength', the higher it is the more it means that players don't want to deal with it be it for play-patterns, expected win-rates, or other reasons one may have.

- In the third structure the ban-rate of a deck is considered a part of its 'presence', it may have not been played, but like in the Kassadin example before, it's lingering in the matches as an unseen factor that is still influential to the results. After all the ban or not of a deck, so if it takes an active or passive role in a match can heavily influence the remaining Match Ups.

While the second structure may seems the more intuitive choice, the third one is the one proposed, the motivations rely on the 4°step the multivariate analysis done on the data.

### Statistical Analysis

Let us start by showing the data we will be using.

```{r print-table-ban, echo=FALSE }
WR.DT_2021_08 %>%
  # filter(nGames > 100) %>%
  select(deck,nGames,playrate,WR,contains("meanBan")) %>%
  arrange(desc(playrate)) %>%
  # mutate( playrate = 3*playrate ) |>
  reactable(
    # wrap = FALSE,
    bordered = TRUE,
    highlight = TRUE,
    striped = TRUE,
    searchable = TRUE,
    compact = TRUE, # compact the table
    # fullWidth = FALSE, # don't fill the page
    defaultPageSize = 10,
    defaultColDef = colDef(
      style = list(fontWeight = 500, color = "black",
                   fontFamily = "Work Sans, sans-serif", fontSize = "12px"),
      align = "center",
      headerStyle = list(background = "steelblue",color="white",fontFamily = "Work Sans, sans-serif", fontSize = "14px" )
      ),
    columns = list(
      deck = colDef(name="Deck", align = "left" ),
      nGames = colDef(name="#Games" ),
      playrate = colDef(name="PlayRate", format = colFormat(percent = TRUE, digits = 2)),
      WR = colDef(name="Win Rate", format = colFormat(percent = TRUE, digits = 2) ),
      meanBan = colDef(name="Ban Rate", format = colFormat(percent = TRUE, digits = 2) )
      )
    )
```

The previous table follows the 3°step of dealing with missing values. The possibles missing values were on cases of decks with no information on the ban.

We don't know if it's possible to assume the missing value are MCAR (Missing Completely at Random).

Normally when the missing data are MCAR, it is possible to just opt to remove the data with missing values, if it is ok the remove them in the first place.

When they are not missing at random (MNAR), apply imputation tecniques should be considered (not necessary, but suggested).

In this case we removed all data with missing values reducing the numbers of decks of our dataset from `r nWR.DT.missing` to `r nWR.DT.complete`

#### Correlation

To assess which sttucture should be used we need to check the relationship between variables. This can be done by looking at their correlations.

Using all the data and calculating the correlation would be wrong, as, as showed in the previous LMI article it is better to limit the analysis to a smaller pool of decks with a sufficient amount of games.

At the Seasonal Tournament the number of max games is overall small compared to the number of decks played so we tried a series of possible benchmarks as least number of games played.

```{r compute-correlation}
corData.apply <- lapply(
      X = c(0,10,30,50,100,200),
      FUN = function(x){

        corData <- WR.DT_2021_06 |>
          filter(nGames > x) %>%
          # left_join(WR.DT.Ladder|>
          #             select(player,ladderWR = WR), by="player" ) %>%
          filter(complete.cases(.)) |>
          select( playrate,WR,meanBan )
          # select( playrate,WR,ladderWR,meanBan )

  corData |>
  cor() |>     # start from the correlation matrix
  # corrr::correlate(method = "pearson") %>%
  # corrr::as_matrix() |>
  as.table() |> 
  as.data.frame() |>                        # Marek's answer in TidyVerse format
  rename("Cor"="Freq") |>
  # filter( !is.na(Cor) ) |>
  filter( Var1 != Var2 ) |>
  # subset(Var1 != Var2 & abs(Freq)>0.5) %>% # omit diagonal and keep significant correlations (optional...)
  filter(!duplicated(paste0(pmax(as.character(Var1), as.character(Var2)), pmin(as.character(Var1), as.character(Var2))))) |>
                                           # keep only unique occurrences, as.character because Var1 and Var2 are factors
  add_column( n = x) |>
    add_column( nrow = NROW(corData) )
          # corrr::as_cordf()
      }
    ) |>
  rbindlist()
```

```{r print-gt-summaryCor}
gt.Cor <- corData.apply |>
  distinct(n,nrow) |>
  # rename_all(~c("minGames","#Deck")) |>
  gt() |>
  cols_label(
    n = md("**minGames**"),
    nrow = md("**#Deck**")
  ) |>
  # gt::tab_header(
  #   title = "Bo3 Data",
  #   subtitle = "Matches by Server"
  # ) |>
  tab_source_note(
    source_note = md(glue::glue("Amount of remaining Decks  \n depending on the required min amount of Games"))
  )

gt.Cor |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )

# gt.Cor
```

At the first glance, the 200 games used during the MetaReports seems excessive as it reduce the decks to 1/10th, 10 games is probably not enough as the win-rates would be too unstable making us gravitating mostly on 30,50,100 but we will truly decide being guided by the correlation result.

```{r print-gt-correlation}
gt.corData <- corData.apply |>
  mutate( contrast = glue::glue("{Var1}/{Var2}"),.keep = c("unused"),.before = "Cor" ) |>
  mutate( Cor = round(Cor,2)) |>
  select(-nrow) |>
  pivot_wider(id_cols = "contrast",names_from = "n",values_from = "Cor") |>
  gt() |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `0`,
      rows = `0` >= 0.20 | `0` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `10`,
      rows = `10` >= 0.20 | `10` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `30`,
      rows = `30` >= 0.20 | `30` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `50`,
      rows = `50` >= 0.20 | `50` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `100`,
      rows = `100` >= 0.20 | `100` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `200`,
      rows = `200` >= 0.20 | `200` < -0.20
    )
  ) |>
  cols_label(
    contrast   = md("**Correlation**"),
  ) |>
  tab_source_note(
    source_note = md(glue::glue("Correlation between the three raw variables on different amount of required min amount of Games"))
  ) %>%
  cols_align(
    align = "center"
    # columns = vars(server)
  )

gt.corData |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )

gt.corData
```

```{r compute-seqCor}
# corData <- 
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill again
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            meanBan = scale_quantile(meanBan) )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[3,1:2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WR06","lWR06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill again
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            meanBan = scale_quantile(meanBan) )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[3,1:2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WR08","lWR08") ) 
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |> 
  rename("Correlation"="Freq","minGames"="n","Variable"="Var1") |>
  mutate(Variable = case_when( 
     Variable == "lWR06" ~ "Ladder WinRate June",
     Variable == "WR06" ~ "Seasonal WinRate June",
     Variable == "lWR08" ~ "Ladder WinRate August",
     Variable == "WR08" ~ "Seasonal WinRate August"
    ) 
  ) |>
  mutate( Variable = factor( Variable, levels = c("Seasonal WinRate June","Ladder WinRate June","Seasonal WinRate August","Ladder WinRate August") ) ) |>
  ggplot(aes(x = minGames, y = Correlation, color=Variable, shape = Variable )) +
  geom_point(size = 2 ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red") +
  ylim(-0.75,0.75)
```


```{r compute-corWR}
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    select(nGames,WR,lWR ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR)
            # meanBan = scale_quantile(meanBan) 
    )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("corWR06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    select(nGames,WR,lWR ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("corWR08") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |>
  ggplot(aes(x = n, y = Freq, color=Var1, shape = Var1 )) +
  geom_point(size = 2 ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red") +
  ylim(-0.75,0.75)

# corr <- round(corData, 3)
# p.mat <- cor_pmat(corData)
# ggcorrplot(corr,  type = "lower", col=brewer.pal(n = 3, name = "RdBu"), p.mat = p.mat) +
#   theme_void( ) + 
#   theme(axis.text.x = element_text(angle = 0, debug = FALSE), 
#         axis.text.y = element_text(angle = 0, debug = FALSE)
#         )

# Leave blank on no significant coefficient
# ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE, type = "lower", insig = "blank")
```

```{r}
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    filter( nGames >= x ) |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    rowwise() |>
    mutate( WRind08 = mean(c_across(contains("WR") ) ) ) |>
    ungroup() |>
    select( -nGames,-WR,-lWR, ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WRind06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    filter( nGames >= x ) |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    rowwise() |>
    mutate( WRind08 = mean(c_across(contains("WR") ) ) ) |>
    ungroup() |>
    select( -nGames,-WR,-lWR, ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WRind08") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |>
  ggplot(aes(x = n, y = Freq, color=Var1, shape = Var1 )) +
  geom_point(size = 2 ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red") +
  ylim(-0.75,0.75)
```


```{r normalization-step-1}
LMI.DT <- WR.DT_2021_08 |>
    filter( nGames >= 70 ) |>
    select(playrate,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( PR_ind = scale_quantile(playrate),
            WR_ind = scale_quantile(WR),
            lWR_ind = scale_quantile(lWR),
            meanBan_ind = scale_quantile(meanBan)
    )
```


```{r PCA-1}
# library(factoextra)
# PCA <- prcomp(LMI.DT[,c("WR_ind","lWR_ind")],scale=T)
# # Eigenvalues
# eig.val <- factoextra::get_eigenvalue(PCA)
# eig.val
#   
# # Results for Variables
# res.var <- get_pca_var(PCA)
# res.var$coord          # Coordinates
# res.var$contrib        # Contributions to the PCs
# res.var$cos2           # Quality of representation


PCA <- psych::principal(r = LMI.DT[,c("WR_ind","lWR_ind")]
                        , nfactors = 2, rotate = "none")
PCA <- psych::principal(r = LMI.DT[,c("WR","lWR")] , nfactors = 2, rotate = "none")

mtcars[,1:2] |>
  mutate(across(everything(), scale() ))

PCA <- psych::principal(r = mtcars[,1:2] , nfactors = 2, rotate = "none")

# eigen(cor(LMI.DT[,.(WR_ind,lWR_ind,meanBan_ind)]))
loading=PCA$loadings[,1]
win_pca = loading/sum(loading)
win_pca
# sum(win_pca)

PCA <- tibble( p = rpois(100,5),
        e = floor(rnorm(100,20,6))
) |> psych::principal(nfactors = 2, rotate = "none")


LMI.DT <- LMI.DT |>
  mutate( WIN_ind    = map2_dbl(.x = WR_ind, .y = lWR_ind, ~weighted.mean(x = c(.x,.y),w = win_pca )),.keep = "unused" ) |>
  mutate( WIN_ind = scale_quantile(WIN_ind) )
  

PCA <- psych::principal(r = LMI.DT[,c("WIN_ind","meanBan_ind")]
                        , nfactors = 2, rotate = "none")
# eigen(cor(LMI.DT[,.(WR_ind,lWR_ind,meanBan_ind)]))
loading=PCA$loadings[,1]
str_pca = loading/sum(loading)
str_pca
```


# Results

# LMI - Tournament Edition

When I wrote the basic theory about the index I mentioned how it's a concept that can be expanded whenever I can find more (appropriate) variable to add. At the previous Seasonal I already had an idea but had no time to apply it. In this case I added the information regarding the ban rate of a deck.

Short explanation: Since now we have 3 variables there two options to consider, either they are used independently as before, or a new step is added a mid-tier aggregation. Since ban rate are always associated with pick rates (p&b) I dediced to create a "p&b dimension" that is composed by the play rate and ban rate. I applied the quantile normalization to use the same scale and the play rate, but there was an option to leave the value raw, the important part is probably leave decks with 0% ban rate to remain 0 and this happens with most transformations. the play rate and ban rate are aggregated with a weighed mean (but in this case with equals weights) the resulting p&b-dimension-index is normalized and then finally aggregated to create the LMI. This is just a quick application of the theory but a more rigorous approach will require testing all the steps with the new framework

```{r data-aggregation}
DT.Aggreation <- WR.DT %>%
  filter( nGames > 100 ) %>%
  # filter( playrate > 0.01 ) %>%
  mutate( freq_ind    = scale_quantile(playrate) ) %>%
  mutate( ban_ind     = scale_quantile(meanBan) ) %>%
  mutate( wr_dim      = scale_quantile(WR) ) %>%
  
  rowwise() %>%
  #' pick ban
  mutate( pb_dim    = map2_dbl(.x = freq_ind, .y = ban_ind, ~weighted.mean(x = c(.x,.y),w = c(0.5,0.5))) ) %>%
  ungroup() %>%
  mutate( pb_dim    = scale_quantile(pb_dim) ) %>%
  rowwise() %>%
  mutate( hmeta_ind = harm.mean(c_across(ends_with("_dim")) ) ) %>%
  ungroup()

# DT.Aggreation
```

```{r ggplotly-LMI}
textWRPR <- function(Deck,WR, playrate,ban){
  glue("Deck: {Deck}\nWin Rate: {scales::percent(WR,accuracy = 0.1)}\nPlay Rate: {scales::percent(playrate,accuracy = 0.1)}\nmean Ban Rate: {scales::percent(ban,accuracy = 0.1)}")
}

f <- list(
  family = "Courier New, monospace",
  size = 18,
  color = "#7f7f7f"
)

fig <- DT.Aggreation %>%
  select(player,WR,playrate,meanBan,wr_dim,pb_dim,hmeta_ind) %>%
  mutate_if(is.numeric, funs(round(., 4)) ) %>%
  mutate( tooltip = textWRPR(Deck = player,WR = WR,playrate = playrate,ban = meanBan) ) %>%
  rename("Deck"="player","Win_Rate"="WR","Play_Rate"="playrate","Ban Rate"="meanBan","WR dim"="wr_dim","Freq dim"="pb_dim","LMI"="hmeta_ind") %>%
  plot_ly(
    type = 'scatter',
    mode = 'markers',
    x = ~`WR dim`,
    y = ~`Freq dim`,
    marker = list(size = ~LMI*100, sizeref = 0.1, sizemode = 'area'),
    color = ~LMI,
    text = ~tooltip,
    hovertemplate = paste(
      "LMI:<b>%{marker.size:,}<br>",
      "<b>%{text}</b><br><extra></extra>",sep = ""
      # "%{yaxis.title.text}: %{y:$,.0f}<br>",
      # "%{xaxis.title.text}: %{x:.0%}<br>",
      # "Number Employed: %{marker.size:,}",
      # "<extra></extra>"
      )
    ) %>% layout(xaxis = list(title = "WR dim",titlefont = f),
                 yaxis = list(title = "Freq dim",titlefont = f),
                 title = 'LoR-Meta Index (LMI)'
                 ) %>% suppressWarnings()

fig
```

# Discussion

## Alternative ways

# Conclusions

# Legal bla bla {.unnumbered}

This Meta Report was created under Riot Games' "Legal Jibber Jabber" policy using assets owned by Riot Games. Riot Games does not endorse or sponsor this project.

```{r}
DT <- data.table(participant_1 = c("a","A","b","c"),
           participant_2 = c("b","D","a","d"),
           origin = c(1,2,1,1))

namesDT <- data.table( puuid_1 = c("a","b","c","d"),
            puuid_2 = c("A","B","C","D"))


left_join(DT,namesDT,by=c("participant_1"="puuid_1")) %>%
  # mutate(c1 = replace(c1, origin==1, v2) )
  mutate(participant_1 = ifelse(origin == 1, puuid_2, participant_1))
```

```{r example-digraph }
# DiagrammeR::grViz("digraph flowchart {
#       # node definitions with substituted label text
#       node [fontname = Helvetica, shape = rectangle]
#       tab1 [label = '@@1']
#       tab2 [label = '@@2']
#       tab3 [label = '@@3']
#       tab4 [label = '@@4']
#       tab5 [label = '@@5']
# 
#       # edge definitions with the node IDs
#       tab1 -> tab2;
#       tab2 -> tab3;
#       tab2 -> tab4 -> tab5
#       }
# 
#       [1]: 'Questionnaire sent to n=1000 participants'
#       [2]: 'Participants came to clinic for evaluation n=700'
#       [3]: 'Participants non-eligible for the study n=100'
#       [4]: 'Participants eligible for the study n=600'
#       [5]: 'Study sample n=600'
#       ",
#   height = 300)
```
