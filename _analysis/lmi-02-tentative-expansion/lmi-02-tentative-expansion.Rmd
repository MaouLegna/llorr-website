---
title: "LoR Meta Index (LMI) expansion by using Bo3 data"
description: |
  The LMI is limited in its current setting as it uses only playrates and win-rates. By using Bo3 data we propose a way to expand the LMI with a ban-index
base_url: http://www.llorr-stats.com
preview:
author:
  - name: Valentino (Legna) Vazzoler
date: 09-12-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true    
    toc_depth: 3
    self_contained: false
citation: false
draft: false
twitter:
  site: "@Maou_Legna"
  creator: "@Maou_Legna"
params:
  # prev:  "2021-07-07 21:00:00" #UTC tz / 'previous' week start
  start: "2021-07-07 21:00:00" #UTC tz / 'current' week start
  end:   "2021-08-25 21:00:00" #UTC tz / 'current' week end
  # skip:  1850000  # Patch 2.11 - after removing a few games  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=9,
  fig.height=6,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
source(file.path("C:","LlorR","scripts","lor_main.R" ))

# require(Hmisc)    # provides knitrSet and other functions
xaringanExtra::use_panelset()
#' Python
# py_run_string("print('Hello World')")
# lor_deckcodes <- import("lor_deckcodes")
# py_module_available("lor_deckcodes")
```

```{r twitter-meta, echo = FALSE}
# library(metathis)
metathis::meta() |>
  metathis::meta_description(
    "The LMI is limited in its current setting as it uses only playrates and win-rates. By using Bo3 data we propose a way to expand the LMI with a ban-index"
  ) |> 
  metathis::meta_viewport() |> 
  metathis::meta_social(
    title = "LoR Meta Index (LMI) expansion by using Bo3 data",
    url = "https://www.llorr-stats.com",
    image = "images/LMIv2",
    image_alt = "LMIv2",
    og_type = "website",
    og_author = "Legna",
    twitter_card_type = "summary",
    twitter_creator = "@Maou_Legna"
  )
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(font_family = "Roboto",
                                   active_foreground = "white",
                                   hover_foreground = "black",
                                   hover_border_color = "black",
                                   active_background = "#007fff"
                                   )
```

```{r raw-data}
#' load gameDT
#'############
file.DT <- file.path("C:","LlorR","data","raw","LoR_MatchDT.csv")
# header        <- fread(file.DT, header = FALSE, na.strings = c("",NA), nrows = 1, stringsAsFactors = FALSE)
# LoR.Match.RMD <- fread(file.DT, header = FALSE, na.strings = c("",NA), skip = params$skip ) # ~2.11/2.12
# colnames(LoR.Match.RMD) <- unlist(header,use.names = F)

LoR.Match.RMD <- fread(file.DT, header = TRUE, na.strings = c("",NA) ) 

#' load Account
#'#############
file.Account <- file.path("C:","LlorR","data","raw","LoR_ACCOUNT.csv")
LoR.Account.RMD <- fread(file.Account, header=T, na.strings = c("",NA), encoding = 'UTF-8') %>%
  mutate( RiotID = paste(gameName,tagLine),refID = puuid_4 ) %>%
  pivot_longer(
  cols = c("puuid","puuid_1","puuid_2","puuid_3","puuid_4"),
  names_to = "origin",
  values_to = "puuid"
)

#' load DeckDT
#'############
LoR.Deck        <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))
```

```{r WR-2021-06}
seasonalDate <- "2021-06-19"
filterDate <- ymd_hms(glue::glue("{seasonalDate} 11:55:00")) + lubridate::minutes(5)

WR.DT.Ladder_2021_06 <- LoR.Match.RMD |>
  #' Base filters
  ###############
  filter( game_type == 'Ranked' ) |>
  filter( game_start_time_utc >= (as.POSIXct(filterDate)-days(14)) & game_start_time_utc < as.POSIXct(filterDate) ) |>
  #' 'process' data
  #################
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "player_1")   ,by=c("deck_code_1"="deck_code")) |>
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "opponent_1") ,by=c("deck_code_2"="deck_code")) |>
  mutate( player_2 = opponent_1, opponent_2 = player_1, oppoppuid_1 = puuid_2, oppoppuid_2 = puuid_1 ) |>
  select( match_key,starts_with("player"),starts_with("opponent"),starts_with("game_outcome"),ends_with("_2"),-ends_with("_3"),-ends_with("_4") ) %>%
  # get_dupes(match_key)
  #' melt data
  ############
  melt(id.vars=c("match_key"), measure.vars=patterns( str_sub(
      names(select(.,ends_with("_1")))
      ,end = -3)
  ),
  value.name = str_sub(
    names(select(.,ends_with("_1")))
    ,end = -3)
  ) |>
  # LoR.Melt.Matches.RMD |> 
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

WR.DT_2021_06 <- fread(file.path("C:","LlorR","data","clean","LoR_Seasonal.csv"),na.strings = c("",NA)) |>
  filter( game_start_time_utc > (as.POSIXct(filterDate)-days(2)) & game_start_time_utc < (as.POSIXct(filterDate)+days(2)) ) |>
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )
```


```{r table-ban-2021-06}
#' read game-result
LoR.Seasonal.RMD_202106  <- fread(file.path("C:","LlorR","data","clean","Seasonal_2021_06.csv"),na.strings = c("",NA), encoding = "UTF-8")

#' create lineUp
LineUp.DT_202106 <- LoR.Seasonal.RMD_202106 |>
  select(userID,starts_with("deck_"),LU) |>
  distinct()

#' all deck for bans
deckForBan <- LineUp.DT_202106 %>%
  filter(!is.na(deck_3) ) %>%
  # filter(!is.na(deck_3) & server!="asia") %>%
  select(contains("deck")) %>%
  unlist(.,use.names = F) %>% unique() %>% sort()
        
#' create Ban DT      
################
ban.tbl <- LoR.Seasonal.RMD_202106 |>
  filter(!is.na(deck_3) ) |>
  group_by( ban ) |> # so accounting the cases where I know the bans
  count(ban) |>
  filter( !is.na(ban) )

ban.DT <- tibble( deck = deckForBan ) |>
    left_join(ban.tbl |> select(deck = ban,nBan=n) ,by = "deck")

setDT(ban.DT)

for (i in 1:NROW(ban.DT) ) {
  deck <- pull(ban.DT[i,"deck"])
  
  whichLU <- LoR.Seasonal.RMD_202106 %>%
    filter(!is.na(deck_3) ) %>%
    filter( deck_1 == deck | deck_2 == deck | deck_3 == deck ) %>%
    pull(LU)
  
  ban.DT[i,maxBan := LoR.Seasonal.RMD_202106 |>
    filter( LU %in% whichLU ) |>
    filter( !is.na(ban) ) |>
    NROW() ]
}

WR.DT_2021_06 <- ban.DT |>  # start with ban.DT as it contains the list of all decks that theoretically can appears
  left_join(WR.DT_2021_06,by=c("deck"="player")) |> # add the Seasonal data
  mutate(across(c(nBan,nWin, nGames,playrate), ~replace_na(.x, 0)) ) |>
  # mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill the missing values
  mutate( meanBan = nBan/maxBan ) |>   # since the number of ban considers all the cases with ban information it is some to impute 0
  left_join(WR.DT.Ladder_2021_06 |> select(player,lplayrate=playrate,lWR=WR),by=c("deck"="player")) # add ladder information
  # |> mutate(across(everything(), ~replace_na(.x, 0)) )


# WR.DT_2021_06 %$%
#   cor(WR,lWR)
```

```{r Ladder-Win-Rates-Seasonal-2021-08}
seasonalDate <- "2021-08-14"
filterDate <- ymd_hms(glue::glue("{seasonalDate} 11:55:00")) + lubridate::minutes(5)

WR.DT.Ladder_2021_08 <- LoR.Match.RMD |>
  #' Base filters
  ###############
  filter( game_type == 'Ranked' ) |>
  filter( game_start_time_utc >= (as.POSIXct(filterDate)-days(14)) & game_start_time_utc < as.POSIXct(filterDate) ) |>
  #' 'process' data
  #################
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "player_1")   ,by=c("deck_code_1"="deck_code")) |>
  left_join(LoR.Deck[,.(deck_code,archetype)] |> setnames(old = "archetype", new = "opponent_1") ,by=c("deck_code_2"="deck_code")) |>
  mutate( player_2 = opponent_1, opponent_2 = player_1, oppoppuid_1 = puuid_2, oppoppuid_2 = puuid_1 ) |>
  select( match_key,starts_with("player"),starts_with("opponent"),starts_with("game_outcome"),ends_with("_2"),-ends_with("_3"),-ends_with("_4") ) %>%
  # get_dupes(match_key)
  #' melt data
  ############
  melt(id.vars=c("match_key"), measure.vars=patterns( str_sub(
      names(select(.,ends_with("_1")))
      ,end = -3)
  ),
  value.name = str_sub(
    names(select(.,ends_with("_1")))
    ,end = -3)
  ) |>
  # LoR.Melt.Matches.RMD |> 
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

WR.DT_2021_08 <- fread(file.path("C:","LlorR","data","clean","LoR_Seasonal.csv"),na.strings = c("",NA)) |>
  filter( game_start_time_utc > (as.POSIXct(filterDate)-days(2)) & game_start_time_utc < (as.POSIXct(filterDate)+days(2)) ) |>
  filter(game_outcome!="tie") |>
  select( player,opponent,game_outcome ) |>
  group_by(player) |>
  summarise( nWin   = sum(game_outcome=="win"),
             nGames = n(),
             WR=mean(game_outcome=="win")
  ) |>
  ungroup() |>
  mutate( playrate = nGames/sum(nGames) )

# WR.DT_2021_08 |>
#   arrange(desc(nGames))
```

```{r table-ban-2021-08}
#' read game-result
LoR.Seasonal.RMD_202108  <- fread(file.path("C:","LlorR","data","clean","Seasonal_2021_08.csv"),na.strings = c("",NA), encoding = "UTF-8")

#' create lineUp
LineUp.DT_202108 <- LoR.Seasonal.RMD_202108 |>
  select(userID,starts_with("deck_"),LU) |>
  distinct()

#' all deck for bans
deckForBan <- LineUp.DT_202108 %>%
  filter(!is.na(deck_3) ) %>%
  # filter(!is.na(deck_3) & server!="asia") %>%
  select(contains("deck")) %>%
  unlist(.,use.names = F) %>% unique() %>% sort()
        
#' create Ban DT      
################
ban.tbl <- LoR.Seasonal.RMD_202108 |>
  filter(!is.na(deck_3) ) |>
  group_by( ban ) |>
  count(ban) |>
  filter( !is.na(ban) )

ban.DT <- tibble( deck = deckForBan ) |>
    left_join(ban.tbl |> select(deck = ban,nBan=n) ,by = "deck")

setDT(ban.DT)

for (i in 1:NROW(ban.DT) ) {
  deck <- pull(ban.DT[i,"deck"])
  
  whichLU <- LoR.Seasonal.RMD_202108 %>%
    filter(!is.na(deck_3) ) %>%
    filter( deck_1 == deck | deck_2 == deck | deck_3 == deck ) %>%
    pull(LU)
  
  ban.DT[i,maxBan := LoR.Seasonal.RMD_202108 |>
    filter( LU %in% whichLU ) |>
    filter( !is.na(ban) ) |>
    NROW() ]
}

WR.DT_2021_08 <- ban.DT |>  # start with ban.DT as it contains the list of all decks that theoretically can appears
  left_join(WR.DT_2021_08,by=c("deck"="player")) |> # add the Seasonal data
  mutate(across(c(nBan,nWin, nGames,playrate), ~replace_na(.x, 0)) ) |>
  # mutate(across(everything(), ~replace_na(.x, 0)) ) |> # fill the missing values
  mutate( meanBan = nBan/maxBan ) |>   # since the number of ban considers all the cases with ban information it is some to impute 0
  left_join(WR.DT.Ladder_2021_08 |> select(player,lplayrate=playrate,lWR=WR),by=c("deck"="player")) # add ladder information
  # mutate(across(everything(), ~replace_na(.x, 0)) ) # fill again

# WR.DT_2021_08 %$%
#   cor(WR,lWR)
```

# Introduction

Composite Indicator (CI) are a quantitative measure that aggregate multi-dimensional data into a single index. Differently from other aggregation methods as Principal Components Analysis (PCA) or Factorial Analysis (FA) they are not entirely data driven and they are compiled in other to communicate a concept. Mostly used in social or policy evaluation, they allows for a single and direct comparison between their units. Gamers commonly and intuitively use this tool when talking about tier list. In a card game like Hearthstone (HS) a simple but known example of CI is the Meta Score from [viciousSyndicate](https://www.vicioussyndicate.com) (vS).

In Legends of Runeterra (LoR) we created a similar CI defined as LoR-Meta-Index (LMI)[^1]. The index didn't just try to replicate the vS Meta Score but tried to adjust it to the LoR data and its differences from HS. A limitation of the proposed index is that it only only use two variables, play-rates and win-rates. While they are probably the most important variables of performance they don't fully catch the complexities on the meta-game performances. This can be solved by adding other variables to compose the index and a natural candidate is the ban-rate of a deck (in a contest of BoX matches).

An experiment to add the ban-rate was done earlier this year, for 'Rise of the Underworld - Seasonal Tournament' report[^2].

The inclusion of such variable, was done without checking all the proper steps so that we wouldn't compromise the quality of the CI. In this article we introduce better the concept and framework of a CI, how to add the the information of the ban rates and their results.

Following the necessary steps, the proposed variation of the LMI contradicts the expected theoretical-framework while confirming the past approach.

[^1]: [LMI - early concept](https://www.llorr-stats.com/analysis/lmi/)

[^2]: [LMI with Ban Rate - from Seasonal Rerport](https://www.llorr-stats.com/report/seasonal-001/#lmi---tournament-edition)

# Data

Data are taken from the Seasonal tournaments 'Guardian of the Ancient' and 'Rise of the Underworld' Open Rounds.

The Open Rounds are the are organized in a series of nine Bo3 Matches with open lists and a ban phase before the start of the games. The smaller amount of games from the Asian shard/server is allegedly because of the fewer players taking part of it[^3]

Not all information about the tournament can be derived from the API. There is no direct data about the chosen ban deck or the entire line-up brought by a player at this have to be extracted by aggregating the metadata of games from a single match and the matches with other matches.

[^3]: No official data are known at the moment of the writing. Supposition made from a series of points like the fewer amount of Master rank players when the cut-off takes place.

We only consider the decks that appears in the cases of a full-line-up and whose I can extrapolate the banned deck.

To evaluate win-rates on the ladder, as additional source, we includes Ranked games from Master players (at least one of the two players being Master) from up to two weeks before the start of the first game of the Seasonal (because of the time zone, two weeks, before the start of the Asian Seasonal).

```{r create-gt-summary}
gtSeasonal <- rbind(
  LoR.Seasonal.RMD_202106 |> add_column(Seasonal = "Seasonal - Guardian of the Ancient"),
  LoR.Seasonal.RMD_202108 |> add_column(Seasonal = "Seasonal - Rise of the Underworld")
) |>
  select(server,Seasonal) |>
  gtsummary::tbl_summary(by = Seasonal) |>
  gtsummary::as_gt() |>
  gt::tab_header(
    title = "Bo3 Data",
    subtitle = "Matches by Server"
  ) |>
  tab_source_note(
    source_note = md(glue::glue("Bo3 Data from Seasonal Open Rounds - Rise of the Underworld Open Rounds Matches - games extracted with Riot API"))
  )

gtSeasonal
 
gtSeasonal |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )
```

Example and part of the raw-data used can be found in Appendix \@ref(tab:table-minmax))

# Methods

## Composite Indicators - A Better Introduction

The LMI is a composite indicator (CI) and in a previous article we introduced the tool we gave a brief explanation about how to create them. Here, we want to give a better and more complete overview of the tool.

For a manual on CI, a commonly referred guide is from the Joint Research Centre (JRC) of the European Commission: [Handbook on Constructing Composite Indicators - METHODOLOGY AND USER GUIDE](https://www.oecd.org/sdd/42495745.pdf). In that guide building a composite indicator requires the following 10 steps:

1.  Theoretical Framework

> Provide the basis for the selection and combination of variables into a meaningfu composite indicator under a fitness-for-porpuse principle (involment of experts and stakeholders is envisaged at this step)

2.  Data selection

> Should be based on the analytical soundness, measurability, coverage and relevance of the indicators to the phenomenon being measured and relationship to each other. The use of proxy variables should be considered when data are scarce (involvement of experts and stakeholders is envisaged at this step)

When defining the CI structure there is also the need to maintain a coherent structure. This means, among other, that values in the same sub-dimension should all follows the same direction. In an increase of a variable imply an increase in the final value of the sub-dimension index then all the other variables should be same.

Example: if we have a sub-dimension index related to "quality of life" containing life expectancy and child mortality, the higher the value of life expectancy the better and higher the final index should be. But, for the values of child mortality it is the opposite, the smaller the value, the better it is. In this case it's not a problem as a common practise is to just use the opposite values by changing the sign as it is a linear transformation and the smaller the value of child mortality (with opposite sign) the better it is in evalutating "quality of life".

The ‘polarity’ of an individual indicator is the sign of the relation between the indicator and the concept to be measured. For example, in the case of well-being, “Life expectancy” has positive polarity, whereas “Unemployment rate” has negative polarity. I

3.  Imputation of missing data

> Is needed in order to provide a complete dataset (e.g by means or multiple imputation).

4.  Multivariate analysis

> Should be used to study the overall structure of the dataset, assess its suitability, and guide subsequent methodological choices (e.g. weighting aggregation).

5.  Normalization

> Should be carried out to render the variables comparable

6.  Weighting and aggregation

> Should be done along the lines of the underying theoretical framework

7.  Uncertainty and sensitivity analysis

> Should be undertaken to assess the robustness of the composite indicator in terms of e.g. the mechanism for indulging or excluding an indicator, the normalization scheme, the imputation of missing data, the choice, the choice of weights, the aggregation method

8.  Back to the data

> Is needed to reveal the main drivers for an overall good or bad performance. Trasparency is promordial to good analysis

9.  Links to other indicators

> Should be made to correlate the composite indicator (or its dimension) with existing (simple or composite) indicators as well as to identify linkages through regressions.

10. Visualization of the results

> Should receive proper attention, given that the visualization can influence (or help to enhance interpretability)

---

When creating for the first time a CI these 10 steps aren't done in a strict sequential order, so in this article we will sometimes return to what is are previous steps compared to the one we are mainly referring in a section of the article.

## The LMI Composite Indicator

### Base LMI

The basic LMI is made by aggregating play-rates and win-rates and its creation was was inspired by seeing the meta score on vS.

While the base raw data are from the same concept, they are combined in a different way.

From their F.A.Q.

> **Q: What is the meaning of the Meta Score and how do you compute it?**
>
> The Meta Score is a supplementary metric that measures each archetype's relative standing in the meta, based on both win rate and prevalence, and in comparison to the theoretical "best deck".
>
> How is it computed?
>
> ...
>
> 1.  We take the highest win rate recorded by a current archetype in a specific rank group, and set it to a fixed value of 100. We then determine the fixed value of 0 by deducting the highest win rate from 100%. For example, if the highest win rate recorded is 53%, a win rate of 47% will be set as the fixed value of 0. This is a deck's **Power Score**. The range of 47% -- 53%, whose power score ranges from 0 to 100, will contain "viable" decks. The length of this range will vary depending on the current state of the meta. Needless to say, it is possible for a deck to have a negative power score, but it can never have a power score that exceeds 100.
>
> 2.  We take the highest frequency recorded by a current archetype in a specific rank group, and set it to a fixed value of 100. The fixed value of 0 will then always be 0% popularity. This is a deck's **Frequency Score**. A deck's frequency score cannot be a negative number.
>
> 3.  We calculate the simple average of a deck's Power Score and Frequency Score to find its **vS Meta Score**. The vS Meta Score is a deck's relative distance to the hypothetical strongest deck in the game. Think of Power Score and Frequency Score as the coordinates (x, y) of a deck within a Scatter Plot. The Meta Score represents its relative placement in the plane between the fixed values of (0, 0) and (100,100).
>
> 4.  If a deck records both the highest popularity and the highest win rate, its Meta Score will be 100. It will be, undoubtedly, the best deck in the game.

While for the LMI:

1. Only decks with at least 200 games are considered. A similar filter is most likely being applied by vS too, we just don't know the values for the cut-off

2. Play-rates and win-rates are normalized with a quantile normalization into a Freq-Index and a Win-Index

3. Freq-Index and a Win-Index are aggregated by an harmonic mean of equal weights into the LMI

The reasoning behind these choices compared to other options can be found in the [LMI - early concept](https://www.llorr-stats.com/analysis/lmi/) article.

### Adding Banrate Information

As the number of variables increase from the two of the base-LMI to three we now have more options as to combine them. Normally, this doesn't mean that each possible choice should be evaluated, the definition we want to communicate should guide our choices and so the characteristics of our variables, e.g. we don't add a variable of Life expectancy in a sub-dimension of 'Infrastructure quality'

When creating the LMI we described it as a measure of *performance* of a deck and as definition of performance of a deck as:

> The performance of a deck is defined by its own strength and popularity inside the metagame.

The definition of performance is probably not be as functional as it should as it seems a bit limiting in what it means. Yet, the term performance still remains appropriate.

What information can be added to the index? An easy inspiration can be taken from Riot's main games: League of Legends (LoL)

In LoL a common value to describe the performance of a champion, in in addition to play-rates and win-rates is the ban-rate of a champion.

The most infamous value of the ban-rate was 95% ban-rate of Kassadin in S3. If we consider the definition of performance given earlier we can see that the ban-rate doesn't fir perfectly strength or popularity in the metagame but it is more a case in the middle. It can seen as an aspect of strength as people don't want to deal with it, so banning it, but it can be as an aspect of popularity or better yet a more general *presence* as while it may have not been played it sort of lingers in the match. The banned champion was not present directly in the match but with its spirit (the ban).

In the context of LoR such information can be added once we consider BoX data, currently only Bo3 with the easiest example being the Seasonal Tournaments and a possible way to define it is:

-   **Ban Rate** - ratio between the number of bans and the number of matches of a deck.

\begin{equation}

BanRate = \frac{\#ban}{\#match}

\end{equation}

Example: 2 Line-Ups contained a Teemo/Ezreal deck, both played all 9 matches and Teemo/Ezreal was banned respectively 3 and 6 times; the ban rate would be $\frac{(3+6)}{(9+9)} = 50\%$

As the variable is deemed appropriate for the LMI purpose the following step is to define the structure of the LMI to account for the new variable.

With just three variables the possible ways to combine them are exactly three as shown in Fig:\@ref(fig:example-LMI-framework-1), Fig:\@ref(fig:example-LMI-framework-2) and Fig:\@ref(fig:example-LMI-framework-3)

```{r example-LMI-framework-1, fig.cap="(1/3) Possible theoretical framework for the LMI - All the variables are part of their own subdimension" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  IP11 [label = 'PlayRate Index']
  
  IB10 [label = 'Fear/Annoyance(?) \n subdimension']
  IB11 [label = 'BanRate Index']
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10,IB10} [dir=back]
  IS10 -> {IS11} [dir=back]
  IP10 -> {IP11} [dir=back]
  IB10 -> {IB11} [dir=back]
  }",
  height = 300)
```

```{r example-LMI-framework-2, fig.cap="(2/3) Possible theoretical framework for the LMI - retaining two main subdimension of the base LMI, ban-rate and win-rate are both used to measure the 'strenght' of a deck" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  IS12 [label = 'BanRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  
  IP11 [label = 'PlayRate Index']
  
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10} [dir=back]
  IS10 -> {IS11,IS12} [dir=back]
  IP10 -> {IP11} [dir=back]
  }",
  height = 300)
```

```{r example-LMI-framework-3, fig.cap="(3/3) Possible theoretical framework for the LMI - retaining two main subdimension of the base LMI, ban-rate and playrate are both used to measure the 'presence' of a deck" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I1 [label = 'LMI']
  IS10 [label = 'Strengh \n subdimension']
  IS11 [label = 'WinRate Index']
  
  IP10 [label = 'Popularity/Presence \n subdimension']
  
  IP11 [label = 'PlayRate Index']
  IP12 [label = 'BanRate Index']
  
  
  # edge definitions with the node IDs
  I1 -> {IS10,IP10} [dir=back]
  IS10 -> IS11 [dir=back]
  IP10 -> {IP11,IP12} [dir=back]
  }",
  height = 300)
```

Each different structure correspond to a different way to see the ban-rate relationship with the other variables.

- In the first structure it is considered a different characteristic altogether in comparison to play-rates and win-rates.

- In the second structure the ban-rate of a deck is considered a part of its 'strength', the higher it is the more it means that players don't want to deal with it be it for play-patterns, expected win-rates, or other reasons one may have.

- In the third structure the ban-rate of a deck is considered a part of its 'presence', it may have not been played, but like in the Kassadin example before, it's lingering in the matches as an unseen factor that is still influential to the results. After all the ban or not of a deck, so if it takes an active or passive role in a match can heavily influence the remaining Match Ups.

While the second structure may seems the more intuitive choice, none of these strctures are the one proposed at the end. This is because of the results we found during the statistical analysis.

### Statistical Analysis

In this section we describes how the analysis was done in its entirely and not just the final version of the steps to do in order to create the LMI. This is so to highlight some of the results we found and how we add the account for them.

<!-- We don't know if it's possible to assume the missing value are MCAR (Missing Completely at Random). -->

<!-- Normally when the missing data are MCAR, it is possible to just opt to remove the data with missing values, if it is ok the remove them in the first place. -->

<!-- When they are not missing at random (MNAR), apply imputation tecniques should be considered (not necessary, but suggested). -->

#### Correlation

To assess which structure should be used we need to check the relationship between variables. This can be done by looking at their correlations.

Using all the data and calculating the correlation would be wrong, as, as showed in the previous LMI article it is better to limit the analysis to a smaller pool of decks with a sufficient amount of games.

At the Seasonal Tournament the number of games in total is overall small compared to the number of decks played so we tried a series of possible cut-off as min number of games played and find a compromise between not eliminating too many decks and having enough data to have quality results.

```{r compute-correlation}
corData.apply <- lapply(
      X = c(0,10,30,50,100,200),
      FUN = function(x){

        corData <- WR.DT_2021_08 |>
          filter(nGames >= x) %>%
          select( playrate,WR,meanBan ) %>%
          # left_join(WR.DT.Ladder|>
          #             select(player,ladderWR = WR), by="player" ) %>%
          filter(complete.cases(.))
          
          # select( playrate,WR,ladderWR,meanBan )

  corData |>
  cor() |>     # start from the correlation matrix
  # corrr::correlate(method = "pearson") %>%
  # corrr::as_matrix() |>
  as.table() |> 
  as.data.frame() |>                        # Marek's answer in TidyVerse format
  rename("Cor"="Freq") |>
  # filter( !is.na(Cor) ) |>
  filter( Var1 != Var2 ) |>
  # subset(Var1 != Var2 & abs(Freq)>0.5) %>% # omit diagonal and keep significant correlations (optional...)
  filter(!duplicated(paste0(pmax(as.character(Var1), as.character(Var2)), pmin(as.character(Var1), as.character(Var2))))) |>
                                           # keep only unique occurrences, as.character because Var1 and Var2 are factors
  add_column( n = x) |>
    add_column( nrow = NROW(corData) )
          # corrr::as_cordf()
      }
    ) |>
  rbindlist()
```

We tried to decide the min number of games required by looking at the amount of remaining decks we would have. The effect of different choices can be seen in Tab:\@ref(tab:print-gt-summaryCor).

**Note**: it's the number of decks with also no missing values, as in the 296 decks there is 1 with no mean ban-rate even at cut-off of zero, the overall number is reduced by one.

<caption>

(\#tab:print-gt-summaryCor)Cut-off Table

</caption>

```{r print-gt-summaryCor}
gt.Cor <- corData.apply |>
  distinct(n,nrow) |>
  # rename_all(~c("minGames","#Deck")) |>
  gt() |>
  cols_label(
    n = md("**minGames**"),
    nrow = md("**#Deck**")
  ) |>
  # gt::tab_header(
  #   title = "Bo3 Data",
  #   subtitle = "Matches by Server"
  # ) |>
  tab_source_note(
    source_note = md(glue::glue("Amount of remaining Decks  \n depending on the required min amount of Games"))
  )

gt.Cor |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )

# gt.Cor
```

At the first glance, the 200 games used during the meta-reports seems excessive as it reduce the decks to 1/10th, 10 games is probably not enough as the win-rates would be too unstable making us gravitating mostly on 30,50,100 but we will truly decide being guided by the correlation results as shown in Tab:\@ref(tab:print-gt-correlation).

<caption>

(\#tab:print-gt-correlation)Correlation Table at different cut-off

</caption>

```{r print-gt-correlation}
gt.corData <- corData.apply |>
  mutate( contrast = glue::glue("{Var1}/{Var2}"),.keep = c("unused"),.before = "Cor" ) |>
  mutate( Cor = round(Cor,2)) |>
  select(-nrow) |>
  pivot_wider(id_cols = "contrast",names_from = "n",values_from = "Cor") |>
  gt() |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `0`,
      rows = `0` >= 0.20 | `0` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `10`,
      rows = `10` >= 0.20 | `10` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `30`,
      rows = `30` >= 0.20 | `30` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `50`,
      rows = `50` >= 0.20 | `50` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `100`,
      rows = `100` >= 0.20 | `100` < -0.20
    )
  ) |>
  tab_style(
    style = list(
      cell_fill(color = "#F9E3D6"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `200`,
      rows = `200` >= 0.20 | `200` < -0.20
    )
  ) |>
  cols_label(
    contrast   = md("**Correlation**"),
  ) |>
  tab_source_note(
    source_note = md(glue::glue("Correlation between the three raw variables on different amount of required min amount of Games"))
  ) %>%
  cols_align(
    align = "center"
    # columns = vars(server)
  )

gt.corData |>
 tab_options(
    table.background.color = "transparent",
    table.font.color = "black",
    table.font.color.light = "black"
   )

# gt.corData
```
The resulting correlations took us by surprise in a first moment. Not only the variable with the higher correlation to the ban-rate is the play-rate and not the win-rate (the expected initial result) but the correlation with win-rate seems to be negative. This is not strange, in Bo3 setting after the bans each player tries to enforce the best match-ups among the remaining decks so it easier to have bad match-ups even for highly-performing decks on the ladder. This made us question whatever to aggregate the ban-rate with play-rate, so the structure in Fig:\@ref(fig:example-LMI-framework-3). What we actually did was to consider:

- If the ban-rate of a deck during the Seasonal is negative correlated its win-rate it's probably because many brought counter line-ups to popular and strong decks. So it would also means that the ban-rate is also correlated to the performance of a decks in the ladder before the tournament.

We tried to see if this is the case by looking at the correlation of the ban-rates with not just the win-rates at the Seasonal but also the win-rates from the ladder up to two weeks before the tournament starts. Since the aggregation is not done with the raw values but the normalized transformation the correlation is calculated after the normalization step using the quantile normalization as done in the previous iteration of the LMI. In addition, not to be affected by the particular tournament chosen this is why we also used the data from the 'Seasonal Tournament - Guardian of the Ancient' when the case-study was aimed to compute the LMI for the 'Rise of the Underworld' edition. As seens in Fig:\@ref(fig:print-seqCor) this was a crucial choice as the relationship of the ban-rates with the Seasonal win-rates can change radically. In the June tournament the metagame was more polarized and this probably affected the correlations.

```{r print-seqCor, fig.cap="correlation of Seasonal Win-Rates and Ladder Win-Rates with the Ban-Rates at different benchmarks of minimum amount of games for each deck"}
# corData <- 
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    select(nGames,WR,lWR,meanBan ) %>%
    filter(complete.cases(.)) |>
    # mutate(across(everything(), ~replace_na(.x, 0)) ) |>
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            meanBan = scale_quantile(meanBan) )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[3,1:2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WR06","lWR06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    select(nGames,WR,lWR,meanBan ) %>%
    filter(complete.cases(.)) |>
    # mutate(across(everything(), ~replace_na(.x, 0)) ) |>
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            meanBan = scale_quantile(meanBan) )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[3,1:2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WR08","lWR08") ) 
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |> 
  rename("Correlation"="Freq","minGames"="n","Variable"="Var1") |>
  mutate(Variable = case_when( 
     Variable == "lWR06" ~ "Ladder WinRate June",
     Variable == "WR06" ~ "Seasonal WinRate June",
     Variable == "lWR08" ~ "Ladder WinRate August",
     Variable == "WR08" ~ "Seasonal WinRate August"
    ) 
  ) |>
  mutate( Variable = factor( Variable, levels = c("Seasonal WinRate June","Ladder WinRate June","Seasonal WinRate August","Ladder WinRate August") ) ) |>
  ggplot(aes(x = minGames, y = Correlation, color=Variable, shape = Variable )) +
  geom_point(size = 2 ) +
  labs(title = element_markdown("Correlation of ban-rates and win-rates at different cut-off"),
       caption = element_markdown("Seasonal win-rates and ban-rates from Bo3 Data from \n Seasonal Open Rounds - Gurdian of the Ancient and Rise of the Underworld Open Rounds Matches \n Ladder win-rates from up to two weeks prior the tournament ranked games from Master players up - games extracted with Riot API") 
      ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red")  +
  ylim(-0.75,0.75) +
  theme(plot.title = element_text(family = 'Helvetica', 
                              color = 'grey50', 
                              face = 'bold', 
                              size = 18, 
                              hjust = 0))
  # theme(plot.caption = element_text(hjust = 0, face= "italic") )
```

How to continue was the hardest part of the analysis. The correlation of ladder win-rates have a strong positive orientation with the ban-rates while the correlation with the Sesonal win-rates is more unstable and tends to be negative for certain value. If we wanted to use the structure of Fig:\@ref(fig:example-LMI-framework-1) or Fig:\@ref(fig:example-LMI-framework-2) than our problem would have to aggregate variables with different orientations. While this is not a rule one must always follows in the cases mentioned this would negatively impact the quality of the LMI. We would have for sure that an high value of a deck would decrease the win-rate index and to have an high LMI we may not want an high ban-rates as it's increase may decreae the win-rate. It would be harder to have decks with high LMI and high values of both win-rates and ban-rates. At the same time, can we really say the strength of the deck is the one we measured if the win-rates we observe seems to be a consequence of the metagame from the ladder and the high performances in Bo1 that is reflected by high ban-rates as to suggest people don't want to deal with decks they in the ladder they found are difficult to deal with unless the line-up can handle them?

The solution proposed is to use a win-index that is not created from just the Seasonal win-rates but also the ladder win-rates to include both informations and having the ladder value reduce if not remove the negative correlation of the Seasonal value. Both for this to work we have to check a couple of conditions:

- Win-rates from Seasonal and ladder needs to be correlated and the orientation shouldn't change depending on the tournament, which is a legit worry after seeing the previous figure. A positive correlation can be found and seen in Fig:\@ref(fig:print-corWR).

- The aggregated win-rates have works better as a component to aggregate with ban-rates. The results can be seen in Fig:\@ref(fig:print-corBanWin).

```{r print-corWR, fig.cap="correlation of Seasonal Win-Rates with Ladder Win-Rates at different benchmarks of minimum amount of games for each deck"}
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    select(nGames,WR,lWR ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR)
            # meanBan = scale_quantile(meanBan) 
    )  |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("corWR06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    select(nGames,WR,lWR ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    filter( nGames >= x ) |>
    select( -nGames ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("corWR08") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |> 
  rename("Correlation"="Freq","minGames"="n","Variable"="Var1") |>
  mutate(Variable = case_when( 
     Variable == "corWR06" ~ "Guardian of the Ancient - June",
     Variable == "corWR08" ~ "Rise of the Underworld - August"
    ) 
  ) |>
  mutate( Variable = factor( Variable, levels = c("Guardian of the Ancient - June","Rise of the Underworld - August") ) ) |>
  ggplot(aes(x = minGames, y = Correlation, color=Variable, shape = Variable )) +
  geom_point(size = 2 ) +
  labs(title = element_markdown("Correlation of Seasonal and ladder win-rates at different cut-off"),
       caption = element_markdown("Seasonal win-rates from Bo3 Data from \n Seasonal Open Rounds - Gurdian of the Ancient and Rise of the Underworld Open Rounds Matches \n Ladder win-rates from up to two weeks prior the tournament ranked games from Master players up - games extracted with Riot API") 
      ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red")  +
  ylim(-0.75,0.75) +
  theme(plot.title = element_text(family = 'Helvetica', 
                              color = 'grey50', 
                              face = 'bold', 
                              size = 18, 
                              hjust = 0))

# corr <- round(corData, 3)
# p.mat <- cor_pmat(corData)
# ggcorrplot(corr,  type = "lower", col=brewer.pal(n = 3, name = "RdBu"), p.mat = p.mat) +
#   theme_void( ) + 
#   theme(axis.text.x = element_text(angle = 0, debug = FALSE), 
#         axis.text.y = element_text(angle = 0, debug = FALSE)
#         )

# Leave blank on no significant coefficient
# ggcorrplot(corr, p.mat = p.mat, hc.order = TRUE, type = "lower", insig = "blank")
```

```{r print-corBanWin, fig.cap="correlation of Ban-rates with a Win-Index computed as harmonic mean of Seasonal and ladder win-Rates with Ladder at different benchmarks of minimum amount of games for each deck"}
lapply(seq(0,200,5), function(x)
  {
  cor06 <- WR.DT_2021_06 |>
    filter( nGames >= x ) |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    rowwise() |>
    mutate( WRind08 = mean(c_across(contains("WR") ) ) ) |>
    ungroup() |>
    select( -nGames,-WR,-lWR, ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WRind06") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR06"="WR", "lWR06"="lWR" )
  
  cor08 <- WR.DT_2021_08 |>
    filter( nGames >= x ) |>
    select(nGames,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( WR = scale_quantile(WR),
            lWR = scale_quantile(lWR),
            # meanBan = scale_quantile(meanBan) 
    ) |>
    rowwise() |>
    mutate( WRind08 = mean(c_across(contains("WR") ) ) ) |>
    ungroup() |>
    select( -nGames,-WR,-lWR, ) |>
    cor() %>%
    .[1,2] |>
    as.table() |>
    as.data.frame() |>
    mutate( Var1 = c("WRind08") )
    # pivot_wider(names_from = "Var1",values_from = "Freq") |>
    # rename( "WR08"="WR", "lWR08"="lWR" )
  
  rbind(cor06,cor08) |>
    add_column( n = x )
  }
  ) |>
  rbindlist() |> 
  rename("Correlation"="Freq","minGames"="n","Variable"="Var1") |>
  mutate(Variable = case_when( 
     Variable == "WRind06" ~ "Guardian of the Ancient - June",
     Variable == "WRind08" ~ "Rise of the Underworld - August"
    ) 
  ) |>
  mutate( Variable = factor( Variable, levels = c("Guardian of the Ancient - June","Rise of the Underworld - August") ) ) |>
  ggplot(aes(x = minGames, y = Correlation, color=Variable, shape = Variable )) +
  geom_point(size = 2 ) +
  labs(title = element_markdown("Correlation of ban-rates with a win-index at different cut-off"),
       caption = element_markdown("Ban-rates from Bo3 Data from \n Seasonal Open Rounds - Gurdian of the Ancient and Rise of the Underworld Open Rounds Matches \n Ladder win-rates from up to two weeks prior the tournament ranked games from Master players up - games extracted with Riot API \n Win-Index as harmonic mean of Seasonal and ladder win-rates") 
      ) +
  theme_Publication() +
  ggsci::scale_color_npg() +
  geom_hline(yintercept = 0, color="red")  +
  ylim(-0.75,0.75) +
  theme(plot.title = element_text(family = 'Helvetica', 
                              color = 'grey50', 
                              face = 'bold', 
                              size = 18, 
                              hjust = 0))
```

Considering the overall results the final choices to the LMI frameworks are the following:

- Having the LMI as seen in Fig:\@ref(fig:example-LMI-framework-4). This is an aggregation of three subdimensions: popularity(playrate), ban and a win-subdimension which is made of the ladder and seasonal win rates. This can be justified by the correlation of the ban-index with both the play-index and the win-index. This option can 

```{r example-LMI-framework-4, fig.cap="Possible theoretical framework for the LMI - adding both ban-rate and ladder win-rate, using three main subdimension: ban, playrate and strength" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I [label = 'LMI']
  IS0 [label = 'Strengh \n subdimension']
  IB0 [label = 'Ban \n subdimension']
  IP0 [label = 'Popularity/Presence \n subdimension']
  
  IS1 [label = 'Strength Index']
  IB1 [label = 'BanRate Index']
  IP1 [label = 'PlayRate Index']
  
  IS21 [label = 'Seasonal WinRate Index']
  IS22 [label = 'Ladder WinRate  Index']
  
  # edge definitions with the node IDs
  I -> {IS0,IP0 IB0} [dir=back]
  IS0 -> IS1 [dir=back]
  IB0 -> IB1 [dir=back]
  IP0 -> IP1 [dir=back]
  
  IS1 -> {IS21,IS22} [dir=back]
  }",
  height = 300)
```

- Having the LMI as seen in Fig:\@ref(fig:LMI-framework). The ban-rates shows a stronger causal relationship with the win-rates compared to the playrates (Fig:\@ref(fig:print-seqCor)) so considering it part of the *strenght* of a deck can be considered more appropriate. This the option which tries to follows the definition which was given before: 'The performance of a deck is defined by its own strength and popularity inside the metagame.' and is in fact the framework which is proposed.

```{r LMI-framework, fig.cap="Proposed LMI theoretical framework - mainteining two main subdimension and having the ban-rate as a component of the 'strength' subdimension" }
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  I [label = 'LMI']
  IS0 [label = 'Strengh \n subdimension']
  IP0 [label = 'Popularity/Presence \n subdimension']
  
  IS11 [label = 'BanRate Index']
  IS12 [label = 'Strength Index']
  IP1 [label = 'PlayRate Index']
  
  IS21 [label = 'Seasonal WinRate Index']
  IS22 [label = 'Ladder WinRate  Index']
  
  # edge definitions with the node IDs
  I -> {IS0,IP0} [dir=back]
  IP0 -> IP1 [dir=back]
  IS0 -> {IS11,IS12} [dir=back]
  
  IS12 -> {IS21,IS22} [dir=back]
  }",
  height = 300)
```
```{r compute-LMI}
LMI.DT <- WR.DT_2021_08 |>
    filter( nGames >= 70 ) |>
    # select(playrate,WR,lWR,meanBan ) |>
    mutate(across(everything(), ~replace_na(.x, 0)) ) |> 
    mutate( PR_ind = scale_quantile(playrate),
            WR_ind = scale_quantile(WR),
            lWR_ind = scale_quantile(lWR),
            meanBan_ind = scale_quantile(meanBan)
    ) |>
  mutate( WIN_ind    = map2_dbl(.x = WR_ind, .y = lWR_ind, ~weighted.mean(x = c(.x,.y),w = c(0.5,0.5) )) ) |>
  mutate( WIN_ind = scale_quantile(WIN_ind) ) |>
  mutate( str_dim = map2_dbl(.x = WIN_ind, .y = meanBan_ind, ~weighted.mean(x = c(.x,.y),w = c(0.5,0.5) )) ) |>
  mutate( str_dim = scale_quantile(str_dim) ) |>
  mutate( str_dim = map2_dbl(.x = WIN_ind, .y = meanBan_ind, ~weighted.mean(x = c(.x,.y),w = c(0.5,0.5) )) ) |>
  mutate( str_dim = scale_quantile(str_dim) ) |>
  mutate( LMI = map2_dbl(.x = str_dim, .y = PR_ind, ~weighted.mean(x = c(.x,.y),w = c(0.5,0.5) )) ) |>
  mutate( LMI = scale_quantile(LMI) ) |> 
  mutate(tier = case_when( 
     LMI >= 0.9 ~ "Tier1",
     # 0.90 <= LMI & LMI < 0.975 ~ "Tier1",
     0.60 <= LMI & LMI < 0.90 ~ "Tier2",
     0.30 <= LMI & LMI < 0.60 ~ "Tier3",
     LMI < 03.0 ~ "Tier4 or lower",
    ) 
  )
```

### Weigths

Aggregating variable into a single index this is always done by applying weights to each component, be them equal weights or another vector. The decision can be made by following different method not all being entirely data-driven. A common way to compute the weights is by using the normalized loading factors of the first eigenvalue from the principal components. With the proposed structure both the weights for the win-index and strength index resulted in a equal weights (0.50 and 0.50). As it is both the simplest case and being confirmed by a data-driven approach equal weights are the applied weights.

**Note**: if we used the 3-subdimension structure the resulting weights would have been

```{r PCA}
PCA <- psych::principal(r = LMI.DT[,c("PR_ind","WIN_ind","meanBan_ind")] |>
                          rename("PlayRate Index"="PR_ind","Strength Index"="WIN_ind","BanRate Index"="meanBan_ind") 
                        , nfactors = 3, rotate = "none")
# eigen(cor(LMI.DT[,.(WR_ind,lWR_ind,meanBan_ind)]))
loading=PCA$loadings[,1]
str_pca = loading/sum(loading)
str_pca|>round(3)
```

# Results

If we tried to apply the proposed structure for the LMI and create again the graph from the [Seasonal - Rise of the Underworld report](https://www.llorr-stats.com/report/seasonal-001/#lmi---tournament-edition) there is a clear difference from the past version. With the new framework Akshan/Sivir (DE/SH) is not in a league of its own while still being the best decks

```{r ggplotly-LMI}
textWRPR <- function(LMI,Deck,WR, playrate,ban){
  glue("LMI: {LMI*100}\nDeck: {Deck}\nWin Rate: {scales::percent(WR,accuracy = 0.1)}\nPlay Rate: {scales::percent(playrate,accuracy = 0.1)}\nmean Ban Rate: {scales::percent(ban,accuracy = 0.1)}")
}

f <- list(
  family = "Courier New, monospace",
  size = 18,
  color = "#7f7f7f"
)

fig <- LMI.DT %>%
  slice_max(LMI,n=30) |>
  select(tier,deck,WR,playrate,meanBan,str_dim,PR_ind,LMI) %>%
  mutate( size = scale_minmax(LMI) ) |>
  mutate_if(is.numeric, funs(round(., 4)) ) %>%
  mutate( tooltip = textWRPR(LMI = LMI,Deck = deck,WR = WR,playrate = playrate,ban = meanBan) ) |>
  rename("Tier"="tier","Deck"="deck","Win_Rate"="WR","Play_Rate"="playrate","Ban Rate"="meanBan","WR dim"="str_dim","Freq dim"="PR_ind","LMI"="LMI") %>%
  plot_ly(
    type = 'scatter',
    mode = 'markers',
    x = ~`WR dim`,
    y = ~`Freq dim`,
    color = ~LMI,
    marker = list(size = ~size*100 , sizeref = 0.1, sizemode = 'area'),
    text = ~tooltip,
    hovertemplate = paste("<b>%{text}</b><br><extra></extra>")
    ) %>% layout(xaxis = list(title = "WR dim",titlefont = f),
                 yaxis = list(title = "Freq dim",titlefont = f),
                 title = 'LoR-Meta Index (LMI)'
                 ) %>% suppressWarnings()

fig
```

```{r ggplotly-LMI-tier}
textWRPR <- function(LMI,Deck,WR, playrate,ban){
  glue("LMI: {LMI*100}\nDeck: {Deck}\nWin Rate: {scales::percent(WR,accuracy = 0.1)}\nPlay Rate: {scales::percent(playrate,accuracy = 0.1)}\nmean Ban Rate: {scales::percent(ban,accuracy = 0.1)}")
}

f <- list(
  family = "Courier New, monospace",
  size = 18,
  color = "#7f7f7f"
)

fig <- LMI.DT %>%
  slice_max(LMI,n=30) |>
  select(tier,deck,WR,playrate,meanBan,str_dim,PR_ind,LMI) %>%
  mutate( size = scale_minmax(LMI) ) |>
  mutate_if(is.numeric, funs(round(., 4)) ) %>%
  mutate( tooltip = textWRPR(LMI = LMI,Deck = deck,WR = WR,playrate = playrate,ban = meanBan) ) |>
  rename("Tier"="tier","Deck"="deck","Win_Rate"="WR","Play_Rate"="playrate","Ban Rate"="meanBan","WR dim"="str_dim","Freq dim"="PR_ind","LMI"="LMI") %>%
  plot_ly(
    type = 'scatter',
    mode = 'markers',
    x = ~`WR dim`,
    y = ~`Freq dim`,
    color = ~Tier,
    marker = list(size = ~size*100 , sizeref = 0.1, sizemode = 'area'),
    text = ~tooltip,
    hovertemplate = paste("<b>%{text}</b><br><extra></extra>")
    ) %>% layout(xaxis = list(title = "WR dim",titlefont = f),
                 yaxis = list(title = "Freq dim",titlefont = f),
                 title = 'LoR-Meta Index (LMI)'
                 ) %>% suppressWarnings()

fig
```

# Conclusions

Problems: only decks with at least one games 

Decks with very high ban rate but few games.

Adding the information



# Appendix {.appendix}

<caption>

(\#tab:print-table-ban)Quantile table

</caption>

```{r print-table-ban }
WR.DT_2021_08 %>%
  # filter(nGames > 100) %>%
  select(deck,nGames,playrate,WR,contains("meanBan")) %>%
  arrange(desc(playrate)) %>%
  # mutate( playrate = 3*playrate ) |>
  reactable(
    # wrap = FALSE,
    bordered = TRUE,
    highlight = TRUE,
    striped = TRUE,
    searchable = TRUE,
    compact = TRUE, # compact the table
    # fullWidth = FALSE, # don't fill the page
    defaultPageSize = 10,
    defaultColDef = colDef(
      style = list(fontWeight = 500, color = "black",
                   fontFamily = "Work Sans, sans-serif", fontSize = "12px"),
      align = "center",
      headerStyle = list(background = "steelblue",color="white",fontFamily = "Work Sans, sans-serif", fontSize = "14px" )
      ),
    columns = list(
      deck = colDef(name="Deck", align = "left" ),
      nGames = colDef(name="#Games" ),
      playrate = colDef(name="PlayRate", format = colFormat(percent = TRUE, digits = 2)),
      WR = colDef(name="Win Rate", format = colFormat(percent = TRUE, digits = 2) ),
      meanBan = colDef(name="Ban Rate", format = colFormat(percent = TRUE, digits = 2) )
      )
    )
```

# Legal bla bla {.unnumbered}

This Meta Report was created under Riot Games' "Legal Jibber Jabber" policy using assets owned by Riot Games. Riot Games does not endorse or sponsor this project.