---
params:
  ind: "02"
  patch: "Patch 2.17 - Week 1"
  title: "Defining Archetypes #2: xxx xxx xxx"
  description: "xxx xxx xxx xxx"
  cardlurl: "https://dd.b.pvp.net/latest/set5/en_us/img/cards/05BC116.png"
  # prev:  "2021-09-29 21:00:00" #UTC tz / 'current' previous week start
  start: "2021-09-01 21:00:00" #UTC tz / 'current' week start
  end:   "2021-09-15 21:00:00" #UTC tz / 'current' week end
  skip:  2800000  # ~ Patch 2.16
title: | 
  `r params$title`
description: |
  `r params$patch` - `r params$description`
base_url: https://www.llorr-stats.com
preview: |
  `r params$cardlurl`
author:
  - name: Valentino (Legna) Vazzoler
date: 10-06-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    self_contained: false
citation: false
bibliography: biblio.yaml
draft: TRUE
references:
- type: article-journal
  id: ackerman2011discerning
  author:
  - family: Ackerman
  - family: Margareta
  - family: Ben-David
  - family: Shai
  issued:
    date-parts:
    - - 2011
      - 6
      - 28
  title: 'Discerning linkage-based algorithms among hierarchical clustering methods'
  container-title: Twenty-Second International Joint Conference on Artificial Intelligence
  page: 1140-1145
  DOI: 10.1007/s00357-018-9266-x
  URL: https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/view/3116/3410
  language: en-GB  
# Van Ness, John W. “Admissible Clustering Procedures.” Biometrika, vol. 60, no. 2, [Oxford University Press, Biometrika Trust], 1973, pp. 422–24, https://doi.org/10.2307/2334558.
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=12,
  fig.height=8,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
options(scipen = 999)
source(file.path("C:","LlorR","scripts","lor_main.R" ))
xaringanExtra::use_panelset()
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(
  font_family = "Helvetica",
  active_foreground = "white",
  hover_foreground = "black",
  hover_border_color = "black",
  active_background = "#007fff"
  )
```

```{r functions}
#' extract the top n most played list for 'archetypes provided'
top_n_codes <- \(DT = LoR.Melt.Matches.RMD,archetypes,n,dopull=T) DT |>
  dplyr::filter(archetype%in%{{archetypes}}) |>
  dplyr::group_by(archetype) |>
  dplyr::count(deck_code) |>
  dplyr::slice_max(deck_code,n=n,with_ties=F) |>
  ungroup()|>{\(x) if( dopull==T ) dplyr::pull(x,deck_code) else x}()


#' distance Matrix given deck_codes
dsimMatrix <- function( codes ) {
  deck.matrix <- LoR.Card$cardCode |> purrr::map_dfc(setNames, object = list(numeric()))
  for (i in 1:length(codes)) {
    if (i%%100==0) glue::glue("Fill parse matrix #{i} - {Sys.time()}") |> message()
    decklist <- codes[i] |> lordecks::get_decklist_from_code()
    deck.matrix[NROW(deck.matrix)+1, decklist$cardcode] <- as.list(decklist$count)
  }
  deck.matrix |> mutate( across(everything(), ~replace_na(.x, 0)) ) 
}

#' distance Matrix given deck_codes
cos.dsimMatrix <- function( codes ) {
  deck.matrix <- LoR.Card$cardCode |> purrr::map_dfc(setNames, object = list(numeric()))
  for (i in 1:length(codes)) {
    if (i%%500==0) glue::glue("Fill parse matrix #{i} - {Sys.time()}") |> message()
    decklist <- codes[i] |> lordecks::get_decklist_from_code()
    deck.matrix[NROW(deck.matrix)+1, decklist$cardcode] <- as.list(decklist$count)
  }
  deck.matrix <- deck.matrix |> mutate( across(everything(), ~replace_na(.x, 0)) ) 
  eisen_cos.sim(deck.matrix)
}
```

```{r raw-data-all}
# skip.dt <-fread(file.path("C:","LlorR","data","raw","skip_row.csv")  )
# skip.dt

#' load gameDT
#'############
# file.DT <- file.path("C:","LlorR","data","raw","LoR_MatchDT.csv")
# header        <- fread(file.DT, header = FALSE, na.strings = c("",NA), nrows = 1, stringsAsFactors = FALSE)
# LoR.Match.RMD <- fread(file.DT, header = FALSE, na.strings = c("",NA), skip = params$skip )
# colnames(LoR.Match.RMD) <- unlist(header,use.names = F)
# 
# #' load Account
# #'#############
# LoR.Account.RMD <- fread(file.path("C:","LlorR","data","raw","LoR_ACCOUNT.csv"),
#                          header=T, na.strings = c("",NA), encoding = 'UTF-8') |>
#   mutate( RiotID = sprintf("%s#%s",gameName,tagLine) )
# 
# #' load DeckDT
# #'############
# LoR.Deck.RMD        <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))[ !is.na(archetype_pretty), archetype:=archetype_pretty ]
 
# #' patch 2.14 to 2.15
# #'###################
# patch214215code <- LoR.Match.RMD |>
#   filter( game_type=="Ranked" ) |>
#   filter( game_start_time_utc >= as.POSIXct(params$start, tz = "UTC") & game_start_time_utc < as.POSIXct(params$end, tz = "UTC") ) |>
#   select( -ends_with("_3"),-ends_with("_4")  ) |>
#   select( starts_with("deck_code") ) |>
#   pivot_longer(cols = c(ends_with("_1"),ends_with("_2")),
#                names_to = c(".value"),
#                names_pattern = "(.*)_[0-9]"
#                ) |>
#   left_join(LoR.Deck.RMD[,.(deck_code,archetype)],  by=c("deck_code") )

# NROW(patch214215code) # 155214
# patch214215code |> fwrite(file.path("C:","LlorR","data","clean","deckcode_patch214215.csv")  )

# length(unique(patch214215code$deck_code)) # 17772
# NROW(DSim_patch214215) # 17772

#' load parse Matrix
#'##################
# DSim_patch214215 <- fread(file.path("C:","LlorR","data","clean","DSimMatrix_0214to0215.csv"))
# cos.DSim_patch214215 <- fread(file.path("C:","LlorR","data","clean","cos.DSimMatrix_0215to0216.csv"), header=T) |> as.matrix()
# fwrite(DSim_patch215216,file.path("C:","LlorR","data","clean","DSimMatrix_0215to0216.csv"))

# cos.DSim_patch214215[1:10,1:10]
# DSim_patch214215[1:10] |> eisen_cos.sim()

#' Archetype-Fix
#'##############
# source(file.path("C:","LlorR","scripts","functions","lor_archetype_rmd.R"))
```

```{r}
# how I selecter the decks, it is reproducible as more decks will be inserted in the DB

# set.seed(123)
# Rep1 <- LoR.Deck.RMD |>
#   filter( archetype == "Ashe/LeBlanc" & is.na(archetype_pretty) ) |>
#   slice_sample(n = 75) |>
#   pull(deck_code)
# 
# set.seed(123)
# Rep2 <- LoR.Deck.RMD |>
#   filter( archetype_pretty == "Marauder" ) |>
#   slice_sample(n = 25) |>
#   pull(deck_code)
# 
# set.seed(123)
# AI <- LoR.Deck.RMD |>
#   filter( archetype == "Azir/Irelia", ) |>
#   slice_sample(n = 100) |>
#   pull(deck_code)
# 
# set.seed(123)
# Dragon1 <- LoR.Deck.RMD |>
#   filter( archetype == "Aurelion Sol/Jarvan IV/Shyvana" ) |>
#   slice_sample(n = 30) |>
#   pull(deck_code)
# 
# set.seed(123)
# Dragon2 <- LoR.Deck.RMD |>
#   filter( archetype == "Aurelion Sol/Shyvana" ) |>
#   slice_sample(n = 70) |>
#   pull(deck_code)
# 
# set.seed(123)
# Sion1 <- LoR.Deck.RMD |>
#   filter( archetype == "Draven/Sion (NX/PZ)" & is.na(archetype_pretty) ) |>
#   slice_sample(n = 80) |>
#   pull(deck_code)
# 
# set.seed(123)
# Sion2 <- LoR.Deck.RMD |>
#   filter( archetype_pretty == "RubinBait - Draven/Sion", ) |>
#   slice_sample(n = 20) |>
#   pull(deck_code)
# 
# set.seed(123)
# Mist1 <- LoR.Deck.RMD |>
#   filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Targon") ) |>
#   slice_sample(n = 50) |>
#   pull(deck_code)
# 
# set.seed(123)
# Mist2 <- LoR.Deck.RMD |>
#   filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Piltover") ) |>
#   slice_sample(n = 50) |>
#   pull(deck_code)
# 
# #' Archetype names fix
# ######################
# LoR.Deck.RMD[ !is.na(archetype_pretty), archetype:=archetype_pretty ]
```

```{r prepare-examples}
#' archetypes names
archetypes <- c( 
  "Ashe/LeBlanc",           # 75/25 noMarauder/Marauder
  "Azir/Irelia",            # 100
  "Dragons (DE/MT)",        # 30/70 J4/PureDrake "Aurelion Sol/Jarvan IV/Shyvana",  "Aurelion Sol/Shyvana",
  "Draven/Sion (NX/PZ)",    # 80/20 DravenSion/RubinBait
  "Mistwraith Allegiance"   # 50/50 Targon/Piltover
)

# LoR.Archetype.Ex <- rbind(
#   LoR.Deck.RMD[ deck_code %in% Rep1, ],
#   LoR.Deck.RMD[ deck_code %in% Rep2, ],
#   LoR.Deck.RMD[ deck_code %in% AI, ],
#   LoR.Deck.RMD[ deck_code %in% Dragon1, ],
#   LoR.Deck.RMD[ deck_code %in% Dragon2, ],
#   LoR.Deck.RMD[ deck_code %in% Sion1, ],
#   LoR.Deck.RMD[ deck_code %in% Sion2, ],
#   LoR.Deck.RMD[ deck_code %in% Mist1, ],
#   LoR.Deck.RMD[ deck_code %in% Mist2, ]
# )
  
# LoR.Archetype.Exe[ !is.na(archetype_pretty), archetype:=archetype_pretty ]
# LoR.Archetype.Exe[ archetype == "RubinBait - Draven/Sion" , archetype:="Draven/Sion (NX/PZ)" ]
# LoR.Archetype.Exe[ archetype == "Marauder" , archetype:="Ashe/LeBlanc" ]

# mini.ex <- c(Rep1[1:7], # Ashe/LB
#              Rep2[1:3], # Marauder
#              AI[1:10],  # AI
#              Dragon1[1:3], # with J4
#              Dragon2[1:7], # no J4
#              Sion1[1:8],   # OG
#              Sion2[1:2],   # fake-burn
#              Mist1[1:5],   # MT
#              Mist2[1:5])   # PnZ
# 
# LoR.Archetype.Mini <- LoR.Archetype.Ex |>
#   filter(deck_code %in% mini.ex )

# fwrite(LoR.Archetype.Ex, "./data/example_archetye.csv")
# fwrite(LoR.Archetype.Mini, "./data/example_archetye_mini.csv")

LoR.Archetype.Ex   <- fread("./data/example_archetye.csv")
LoR.Archetype.Mini <- fread("./data/example_archetye_mini.csv")

# #' distance matrix
sparseMatrix.Ex   <- dsimMatrix(codes =  LoR.Archetype.Ex$deck_code )
sparseMatrix.mini <- dsimMatrix(codes =  LoR.Archetype.Mini$deck_code )

rownames.archetype <- lapply(archetypes, function(x) sprintf("%s.%s",x,c(1:10))) |> unlist()

sparseMatrix.mini <- as.data.frame(sparseMatrix.mini)
rownames(sparseMatrix.mini) <- rownames.archetype

DSim_ex   <- eisen_cos.sim(sparseMatrix.Ex)
DSim_mini <- eisen_cos.sim(sparseMatrix.mini)
```

```{r}
# DSim_patch215216 <- unique(patch215216code$deck_code) |> dsimMatrix()
# fwrite(DSim_patch215216,file.path("C:","LlorR","data","clean","DSimMatrix_0215to0216.csv"))

# DSim <- DSim_patch215216 |> eisen_cos.sim()

# fwrite(as.matrix(DSim),file.path("C:","LlorR","data","clean","cos.DSimMatrix_0215to0216.csv"))
```

# Introduction

Defining archetypes on Legends of Runeterra is both a complex and simple problem.
It's simple if we consider that it's possible to define decks by the combination of champions and regions of choice but it's also complex by the fact that such definition is quite limited.

On our [previous article/analysis](https://llorr-stats.netlify.app/analysis/defining-archetypes-01/) we gave a possible method about how to compare archetypes and see if they can be considered from a shared common archetype or not.
The proposed method makes use of inferential statistical analysis to reach a conclusion.
Sadly, it's also a methodology that's more fitting a posterior analysis, when hypothetical archetypes are already defined, a tool more fitted to refine the results and not to define archetypes.

A more fitting methodology to find archetype is a form of exploratory data analysis (EDA) known as *Clustering Analysis* (CA).
Its aim is to find subgroups (or clusters) in our data without relying on a *response variable*, also the reason why it's called *unsupervised learning*.

As useful as it is a CA suffer from a fundamental problem of not being able to check out the quality of the results.
With a vast array of different algorithms and hyper-parameters this also means that finding the the "correct" way to use a CA to define archetypes (which was supposed to be the aim of this article) is not only impossible, it's also seeing the CA in the wrong way.
Surely some choices are better than others but there is no perfect answer and to be fair, this was making us, was making me, procrastinating the writing of this article.
The result, or maybe compromise was to reduce the scale on this article which will be small dive into the cluster analysis.
While I want to provide some food for thought for others in the end the main recipient of the article is myself, to provide me a more solid foundation on the topic and how to approach it knowing the basic limitations of what I'm planning to use.

# Clustering

Clustering is a basic data mining task with a wide variety of applications.
not surprisingly, there exist many clustering algorithms.
However, clustering is an ill defined problem - given a data set, it is not clear what a "correct" clustering for that set is.
Indeed, different algorithms may yield dramatically different outputs for the same input sets.
In spite of the wide use of clustering in many practical applications, currently, there exist no principled method to guide the selection of a clustering algorithm.
Currently, such decisions are often made in a very ad hoc, if not completely random, manner.
Users are aware of the cost involved in employing different clustering algorithms, such as running times, memory requirement, and software.

Clustering is applied in a wide range of disciplines, from astronomy to zoology, yet its theoretical underpinnings are still poorly understood.
Even the fairy basic problem of which algorithm to select for a given application (known as "the user dilemma") is left to ad hoc solutions, as theory is only starting to address fundamental differences between clustering methods.

Clustering refers to a very broad set of techniques for findings subgroups or *clusters*, in a data set.
When we cluster the observations of a data set, we seek partition then into distinct groups so that the observations within each group are quite similar to each other, while observation in a different groups are quite different from each other.
Of course, to make this concrete, we must define what it means for two or more observations to be *similar or different*

Since clustering is popular in many fields, there exist a great number of clustering methods

## K-Mean

K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters.
To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters.
After the initialization step, the algorithm iterate until cluster assignments stop changing:

-   For each of the K clusters, compute the *cluster centroid*.
    The kth cluster centroid is the vector of the p-feature means for the observations in the kth cluster

-   Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance)

As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data.
The problem of selecting K is far from simple.
This issue, along with other practical considerations that arise performing K-means clustering like is variables should be first be standardized in some way?

As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters.
To aid the analyst, the following explains the three popular methods for determining the optimal clusters, which includes

-   Elbow method

Recall that, the basic idea behind cluster partitioning methods, such as K-means clustrering, is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square is minimized:)

$minimize(\sum_{k=1}^KW(C_k))$

The total within-cluster sum of squares (wss) measures the compactness of the clustering and we want it to be as small as possible.
Thus, we can use the following algorithm to define the optimal clusters

Plot the curve of wss according to the number of clusters k The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters

This is because normally the wss is related to the number of cluster by a monotone decreasing function, meaning that is the aim is simply to minimise wss then having k = n clusters each of them corresponding to a single element would give the min-wss.
One must search the optimal value we are searching for is in 1 \< k \< \|X\| finding a comprise bewtween the reduction of the objective function wss and the dimension of the hyper-parameter n.
The common practise is to use the *Elbow method* as it tells us when increasing n is less "worth" compared to the reduction in wss.

```{r elbow, fig.width=6, fig.height=4, fig.cap="within sum of squares as a fnction of number of clusters obtained with K-means applyed to the example data set"}
#| fig.width=6
#| fig.height=4
fviz_nbclust(sparseMatrix.mini, kmeans,  method = "wss") +
  theme_Publication()
```

-   Silhouette method

Prior to this article I'm not sure I even heard about this metric: From Wikipedia, the silhouette is defined as:

> The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).
> The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.
> If most objects have a high value, then the clustering configuration is appropriate.
> If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.

```{r silhouette, fig.width=6, fig.height=4, fig.cap="Silhouette as a fnction of number of clusters obtained with K-means applyed to the example data set"}
fviz_nbclust(sparseMatrix.mini, kmeans, method = "silhouette") +
  theme_Publication()
```

Fig:\@ref(fig:elbow) and Fig:\@ref(fig:silhouette) shows the results from the described methods.
Using K-means the algorithm is suggesting us around 6 or 7 clusters.
We will discuss later the quality of these choices, for now we will focus on the algorithm per se.

K-means while a powerful tool in many fields doesn't seems to be the best choice for the archetype problem.
First of all, is requires the user to define a priori the number of cluster it needs to find.
This is almost the opposite of what we would like from our algorithm.
Ideally by clustering we would prefer some indication regarding which deck to aggregate into a single archetype, or even division of sub-archetypes if the region+champion combination is not enough to distinguish archetypes.
In addition, K-means present other problems like having to start with a randomized assignment of clusters, this, of course can be controlled by having fixed seeds, rng but it still hinder the reproducibility of our results.
The choice of k is also too much subjective, especially if n is very large and the knee is not as clear as in this example.
Lastly but not less important, there is less flexibility regarding the *measure/distance* to compare decks.
While we are not limited to the Euclidean distance we can't use any distance of choice as possible with other class of cluster algorithm.

## Hierarchical Clustering

One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K.
Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K.
Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendogram.

The hierarchical clustering dendogram is obtained via an extremely simple algorithm.
We begin by defining some sort of dissimilarity measure between each pair of observations.
Most often, Euclidean distance is used; The algorithm proceeds iteratively.
Starting out at the bottom of the dendogram, each of the n observations is treated as its own cluster, The two clusters that are most *similar* to each other are then fused so that there are now n-1 clusters.
The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendogram is complete.

This agorithm seems simple enough, but one issue has not been addressed.
Consider the aggregation step, how did we determine which a cluster should be fused with another cluster.
We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?
The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of groups of observations.
This extensions is archieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations.

The five most common types of linkage are:

1.  **Single Linkage**: In single linkage, we define the distance between two clusters as the minimum distance between any single data point in the first cluster and any single data point in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters with the smallest single linkage distance.

<!-- $\textit{l}_{SL}(A,B,d)=min_{a\in A,b\in B}d(a,b)$ -->

With d is a distance function

```{=tex}
\begin{equation}

\textit{l}_{SL}=min_{a\in A,b\in B}d(a,b) (\#eq:single)

\end{equation}
```
The resulting dendogram is;

```{r single, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Single linkage to the example data set"}
hc.single   <- hclust(DSim_mini, method = "single") 
fviz_dend( hc.single, show_labels = F ) +
  labs( title = "Single Linkage", y = element_blank() )
```

From Fig:\@ref(fig:single) it's possible to notice how Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.

2.  **Complete Linkage**: In complete linkage, we define the distance between two clusters to the maximum distance between any single data point in the first cluster and any single data point in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters that have the smallest/complete linkage distance.

<!-- $\textit{l}_{CL}(A,B,d)=max_{a\in A,b\in B}d(a,b)$ -->

```{=tex}
\begin{equation}

\textit{l}_{CL}(A,B,d)=max_{a\in A,b\in B}d(a,b) (\#eq:complete)

\end{equation}
```
The resulting dendogram is;

```{r complete, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Complete linkage to the example data set"}
hc.complete <- hclust(DSim_mini, method = "complete")
fviz_dend( hc.complete, show_labels = F ) +
  labs( title = "Complete Linkage", y = element_blank() )
```

Clusters resulting from Complete linkage tend to have a spherical structure and are usually more compact.

3.  **Average Linkage**: In average linkage, we define the distance between two clusters to be the average distance between data points in the first cluster and data points in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters that have the smallest average linkage distance.

$\textit{l}_{CL}(A,B,d)=\frac{\sum_{a\in A,b\in B}d(a,b)}{|A| \cdot |B| }$

```{=tex}
\begin{equation}

\textit{l}_{CL}(A,B,d)=\frac{\sum_{a\in A,b\in B}d(a,b)}{|A| \cdot |B| } (\#eq:average)

\end{equation}
```
The resulting dendogram is;

```{r average, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Average linkage to the example data set"}
hc.average  <- hclust(DSim_mini, method = "average")
fviz_dend( hc.average, show_labels = F ) +
  labs( title = "Average Linkage", y = element_blank() )
```

As intuitively as it is, clusters resulting from Average linkage is a less "extreme" version of both Single and Complete linkage

4.  **Centroid Linkage**: In centroid linkage, the distance between two clusters is the distance between the two mean vectors of the clusters also kown centroid, also kown as barycentre. At each stage of the process we combine the two clusters that have the smallest centroid distance. Centroid linkage can result in undesirable inversions, whereby two clusters are fused at a height below either of the individual clusters in the dendogram.

<!-- $\textit{l}_{Centroid}(A,B,d)= d(\overline{x},\overline{y})$ -->

```{=tex}
\begin{equation}

\textit{l}_{Centroid}(A,B,d)= d(\overline{x},\overline{y}) (\#eq:centroid)

\end{equation}
```
The resulting dendogram is;

```{r centroid, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Centroid linkage to the example data set"}
hc.centroid <- hclust(DSim_mini, method = "centroid")
fviz_dend( hc.centroid, show_labels = F ) +
  labs( title = "Centroid Linkage", y = element_blank() )
```

From Fig:\@ref(fig:centroid) it's possible to notice the inversion phenomenon that can occurs with Centroid linkage as the clusters are fused at a height below either of the individuals clusters in the dendogram.

5.  **Ward's Method**: This method does not directly define a measure of distance between two points or clusters. It is an ANOVA based approach. One-way univariate ANOVAs are done for each variable with groups defined by the clusters at the stage of the process. At each stage, two clusters merge that provide the smallest increase in the combined error sum of squares. ( We won't try this method )

To choose the number of clusters with hierarchical clusters can be done with the methods described with K-means so by looking at the wss or silhouette, but the strength of hierarchical clustering is being guided by the dendogram.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations.
The higher the height of the fusion, the less similar the observations are.

Similarly it's possible to cut the dendogram and define k clusters.
The height of the cut to the dendogram controls the number of clusters obtained.
It plays the same role as the k in k-means clustering.
In order to identify sub-groups (i.e. clusters).

From all dendograms it's easy to see that there is evidence of choosing around 5 or 6 clusters.
Again, we will discuss later the 'quality' of this choice.

# Defining the problem

## Upping the game

Up until now what has being described is the basic of the basic on clustering which could be found in any article/guide/course on the subject.
What will follows is more akin as a self-explanation of points that could be worth to evaluate in order to approach the algorithm problem.
Additional algorithms will be introduced with both their pro and cons, in this section we will explain the example used in this article to highlight what seems to be more interesting to explore for future analysis in the search of a way to define archetypes.

Let us introduce a couple of point from a discussion with <a href="https://twitter.com/drisoth/">Drisoth</a> and its effect:

> My philosophy is its not very helpful to have a cluster if I don't have the data to say anything about it and these low play rate clusters don't have many games, so even if I included them it wouldn't be helpful I would like to include a way for users to self define an archetype, saying include x y z card, so if someone wants to go looking they could yeah, I cluster unique decklists weighted by games on each unique decklist

From this approach two question comes to mind

1.  Should there exist outliers that's best no to assign to a cluster?

If the question is formulated simply like this with no other conditions, then yes, there is deny that outliers and/or noise exists in the archetype problem and the best way to deal with them would be not assigning them to clusters. The reason is obvious a random pile of 40 cards could turn into something resembling a possible archetype, but for the most cases it would likely turn into just that, a random pile of cards.

Their existence is certain, even worse many cluster algorithm are sensitive to noise and outliers and at various degree so are the methods described up until know.
If K-means starts on an outlier it won't be easily dealt with, also a reason why it's best practise to repeat K-means several times to avoid being too depending on the original random choice at the initialization step.
In (Agglomerative) Hierarchical Clustering (AHC), the algorithm described before, if an outlier is wrongly assigned to a cluster in some cases this error will be propagated on all the following steps with no way to intervene.[^1]

[^1]: This behaviour allows AHC to more easily identify smaller clusters. In contrast, Divisive Hierarchical clustering is more robust to noise and outliers.

A series of Density-based clustering algorithm are able to deal with outlier and DBSCAN is currently the most popular method which 

2.  Should the data be weighted?

While being developed only recently, the concept of weighted clustering framework can be traced back into the '70 [Fisher&VanNess(1971)].
A more formal definition has has been developed [Ackerman2016] while also introducing a series of mathematical properties related to the weights.
The use of weights can be seen as a more generalized definition of centroid if not even just its description as barycentre also called centre of mass.

It is easy to see that using unweighed data can be seen as having the same weight 1 on all data.
Applying weight would than be the same as having duplicates as multiple points with mass would correspond to a single point with weight equal to the sum of masses.
Similarly a point of zero mass can be seen as non-existent.
From this an algorithm would be equivalent to start in the middle of its steps of defining clusters and so not starting to work with the leaf but point-clusters with different masses.

Following the definition of weighted clustering, one may ask what effect does this have on the clustering algorithms?

[@Ackerman2016] introduced the properties related to the response to weighted data

Be it $(X,d)$ a dissimilarity function

And $|range(A(X,d,k))|$ the range of a partitional algorithm on a data set defined as the number of clusterings it outputs on that data over **all weights functions**

- **Weight Robust Algorithm** A partitional algorithm $A$ is weight-robust if for (X,d) and $1 < k < |X|$ $|range(A(X,d,k))|=1$.
  This means that the output of a weight robust algorithm is unaffected by the choice of weights

- **Weight Sensitive Algorithm** A partitional algorithm $A$ is weight-sensitive if for (X,d) and $1 < k < |X|$ $|range(A(X,d,k))|>1$.
  This case imply that no matter the original data, the result can be changed in response of the use of weights.
  Even if we would like to use weights to guide our output this properties doesn't seems desirables as we may want to not be too subjected to the weights.
  Of course it's not like we can't have the same results when applying different weights, and in most cases it can even be the same if the geometry of out data helps us.
  Yet, no matter the case we can't be sure that the algorithm won't be affected by the weights.

- **Weight Considering Algorithm** A partitional algorithm $A$ is weight-considering if
  * $\exists$ (X,d) and $1 < k < |X|$ so that $|range(A(X,d,k))| = 1$ and
  * $\exists$ (X,d) and $1 < k < |X|$ so that $|range(A(X,d,k))| > 1$

  With weight-considering we can have both the previous definition meaning cases where the algorithm will be influenced by weights and cases where it will ignore them.
  
Now that we defined the properties what does they mean in the practical use? The next figure gives an idea of what to expect for robust and sensitive cases.

![Different cluster structures based from the same data. All weight-sensitive methods select the clustering on the right while weight-robust methods select the one on the left](images/A02-weight-robust-sensitive.png)

We must remind how there is not correct partitioning, it all depends by the characteristics of each case study.

But let's assume the figure represents deck and the distance between each point is a deck. What should be the correct way to partition into clusters ? The denser region may represent a single archetypes which is further divided in sub-archetypes like the many ways to build a Viego deck with the two outlier-regions being two other completely archetypes so the left case, but it could also be like in the right that we have 3 archetypes that share a lot in common with some exotic build for one of them. it could also be that the right answer in 5 archetypes, or just two. Overall, again there is no pre-determined answer.

Of the algorithms showed up until know Single linkage and Complete linkage are weight robust (and not many others more generally possess this property)

Ward and K-means (as many other variant of the 'k-family') are weight sensitive and Average linkage is weight considering.

> So remember kids, don't try weighted data when applying Single-linkage at home (it's useless)

```{r}
# tribble(
#   ~` `,                    ~Partional,  ~Hiercarchical,
#   "Weight\nSensitive",    "k-<family>", "Ward's Method",
#   "Weight\nConsidering",  NA,           "Average Linkage",
#   "Weight\nRobust",       NA,           "Single/Complete Linkage"
# ) |>
#   gt() |>
#   tab_footnote(
#     footnote = md("k-means, k-medroids, k-medians and *almost* all similar algorithm"),
#     locations = cells_body(
#       columns = Partional,
#       rows = 1)
#   ) 
```
2.

```{r Example1}
set.seed(123)
hc.complete.n <- hclust(DSim_mini, method = "complete", members = sample(1:5000,NROW(DSim_mini) ))


set.seed(123)
hc.average.n  <- hclust(DSim_mini, method = "average", members = sample(1:5000,NROW(DSim_mini)))


set.seed(123)
hc.centroid.n <- hclust(DSim_mini, method = "centroid", members = sample(1:5000,NROW(DSim_mini)))

paste 

hc.complete$labels <- LoR.Archetype.Mini$archetype
fviz_dend(hc.complete, cex = 0.5 ) +
fviz_dend(hc.complete.n, cex = 0.5 )

fviz_dend(hc.average, cex = 0.5 ) +
fviz_dend(hc.average.n, cex = 0.5 )

fviz_dend(hc.centroid, cex = 0.5 ) +
fviz_dend(hc.centroid.n, cex = 0.5 )
```

# DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

# Affinity Propagation Clustering

```{r, fig.width=12, fig.height=8}
# Compute hierarchical clustering and cut into 4 clusters
# hcut(DSim_mini, k = 5, method = "single")
# hcut(DSim_mini, k = 5, method = "complete")
# hcut(DSim_mini, k = 5, method = "average")
# hcut(DSim_mini, k = 5, method = "centroid")

# Visualize
# fviz_dend(hcut(DSim_mini, k = 5, method = "centroid"),
#           k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
#           label_cols =  rep(1:5,each = 10),
#           rect = TRUE, cex = 0.5 )
# 
# linkage = c("single","complete","average","complete")
# p <- ggplot()
# 
# for ( i in 1:length(linkage) ) {
#    
#   # i = 1
#   
#   
#   hcluster <- hclust(DSim_mini, method = linkage[i] ) 
#   
#   p <- p + fviz_dend(hcluster, cex = 0.5 ) +
#     labs(title = glue::glue("{str_to_title(linkage[i])} Linkage"),
#          y = element_blank() )
#   
# } 
```

```{r}
# library("apcluster")
# s1 <- negDistMat(iris, r=2)
# 
# apres1a <- apcluster(negDistMat(DSim_mini,r=1))
# 
# table(apres1a@idx,LoR.Archetype.Mini$archetype)
# 
# apcluster::heatmap(apres1a,negDistMat(DSim_mini,r=1))
# 
# heatmap(apres1a,as.data.frame(DSim_mini))
# 
# apcluster::linSimMat
# 
# apcluster::corSimMat(DSim_mini,signed = F,r=1)
# linKernel(DSim_mini,normalize = T)
# 
# eisen_cos.sim(DSim_mini)
```

```{r}
# kNNdistplot(iris, k = 5)
# abline(h=.5, col = "red", lty=2)
# 
# res <- dbscan(iris, eps = .5, minPts = 5)
# res
# 
# pairs(iris, col = res$cluster + 1L)
# 
# hullplot(x = iris, cl = res) # grafico delle aree convesse

# if(!require(devtools)) install.packages("devtools")
# devtools::install_github("kassambara/factoextra")

# library(factoextra)
# data("multishapes")
# df <- multishapes[, 1:2]
# set.seed(123)
# km.res <- kmeans(df, 5, nstart = 25)
# fviz_cluster(km.res, df, frame = FALSE, geom = "point")

# install.packages("fpc")
# install.packages("dbscan")

# Load the data 
# Make sure that the package factoextra is installed
data("multishapes", package = "factoextra")
df <- multishapes[, 1:2]
# 
library("fpc")
# Compute DBSCAN using fpc package
set.seed(123)
db <- fpc::dbscan(df, eps = 0.15, MinPts = 5)
# # Plot DBSCAN results
plot(db, df, main = "DBSCAN", frame = FALSE)
# 
# library("factoextra")
# fviz_cluster(db, df, stand = FALSE, frame = FALSE, geom = "point")

# Print DBSCAN
# print(db)

hc.average.5 <- hcut(DSim_mini, k = 5, method = "average")

table(
  unname(hc.average.5$cluster),
  res.db$cluster
) |>
  kable()

# fviz_dend(hcut(DSim_mini, k = 5, method = "centroid"),
#           k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
#           label_cols =  rep(1:5,each = 10),
#           rect = TRUE, cex = 0.5 )

  
  
# Cluster membership. Noise/outlier observations are coded as 0
# A random subset is shown
# db$cluster[sample(1:1089, 50)]

# dbscan::kNNdistplot(df, k =  5)
# abline(h = 0.15, lty = 2)
```

```{r}
#The iris dataset is used:

# Load the data
data("iris")
iris <- as.matrix(iris[, 1:4])
#The optimal value of “eps” parameter can be determined as follow:

dbscan::kNNdistplot(DSim_mini, k =  7)
abline(h = 0.37, lty = 2)
# Compute DBSCAN using fpc::dbscan() and dbscan::dbscan(). Make sure that the 2 packages are installed:

res.db <- dbscan::dbscan(DSim_mini, 0.37, 5)

set.seed(123)
# fpc package
res.fpc <- fpc::dbscan(DSim_mini, eps = 0.37, MinPts = 5)
# dbscan package
res.db <- dbscan::dbscan(iris, 0.4, 4)
# The result of the function fpc::dbscan() provides an object of class ‘dbscan’ containing the following components:
# cluster: integer vector coding cluster membership with noise observations (singletons) coded as 0
# isseed: logical vector indicating whether a point is a seed (not border, not noise)
# eps: parameter eps
# MinPts: parameter MinPts
# The result of the function dbscan::dbscan() is an integer vector with cluster assignments. Zero indicates noise points.
# Note that the function dbscan:dbscan() is a fast re-implementation of DBSCAN algorithm. The implementation is significantly faster and can work with larger data sets than the function fpc:dbscan().

# Make sure that both version produce the same results:

all(res.fpc$cluster == res.db)
## [1] TRUE
#The result can be visualized as follow:

# fviz_cluster(res.fpc, iris, geom = "point")
```

# Data

The example used in this article is made out of 50 decks.
More details will be described

-   Five archetypes with Different regions & different play style

    1.  Azir/Irelia
    2.  Ashe/LeBlanc
    3.  Dragons (DE/MT)
    4.  Draven/Sion (NX/PZ)
    5.  Mistwraith Alligiance

More details about how they were chosen will be written later as results are shown

```{r twitter-meta, echo = FALSE}
# library(metathis)
# meta() %>%
#   meta_description(
#     "First entry on a series of article that will gather my explorations over different way to define archetypes in Legends of Runeterra"
#   ) %>% 
#   meta_viewport() %>% 
#   meta_social(
#     title = "Defining Archetypes #1: Looking at the similarity of Akshan/Sivir/Zed with similar archetypes",
#     url = "https://llorr-stats.netlify.app/",
#     image = "images/archetypes/A01-ASZSZ.png",
#     image_alt = "ASZSZ",
#     og_type = "website",
#     og_author = "Legna",
#     twitter_card_type = "summary",
#     twitter_creator = "@Maou_Legna"
#   )
```

# Legal bla bla {.unnumbered}

This content was created under Riot Games' "Legal Jibber Jabber" policy using assets owned by Riot Games.
Riot Games does not endorse or sponsor this project.
