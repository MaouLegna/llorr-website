---
params:
  ind: "02"
  patch: "Patch 2.17 - Week 1"
  title: "Defining Archetypes #2: xxx xxx xxx"
  description: "xxx xxx xxx xxx"
  cardlurl: "https://dd.b.pvp.net/latest/set5/en_us/img/cards/05BC116.png"
  # prev:  "2021-09-29 21:00:00" #UTC tz / 'current' previous week start
  start: "2021-09-01 21:00:00" #UTC tz / 'current' week start
  end:   "2021-09-15 21:00:00" #UTC tz / 'current' week end
  skip:  2800000  # ~ Patch 2.16
title: | 
  `r params$title`
description: |
  `r params$patch` - `r params$description`
base_url: https://www.llorr-stats.com
preview: |
  `r params$cardlurl`
author:
  - name: Valentino (Legna) Vazzoler
date: 10-06-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    self_contained: false
citation: false
bibliography: references.bib
draft: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=6,
  fig.height=4,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
options(scipen = 999)
source(file.path("C:","LlorR","scripts","lor_main.R" ))
xaringanExtra::use_panelset()

pacman::p_load(apcluster,dbscan,fpc,factoextra)
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(
  font_family = "Helvetica",
  active_foreground = "white",
  hover_foreground = "black",
  hover_border_color = "black",
  active_background = "#007fff"
  )
```

```{r functions}
#' extract the top n most played list for 'archetypes provided'
top_n_codes <- \(DT = LoR.Melt.Matches.RMD,archetypes,n,dopull=T) DT |>
  dplyr::filter(archetype%in%{{archetypes}}) |>
  dplyr::group_by(archetype) |>
  dplyr::count(deck_code) |>
  dplyr::slice_max(deck_code,n=n,with_ties=F) |>
  ungroup()|>{\(x) if( dopull==T ) dplyr::pull(x,deck_code) else x}()


#' distance Matrix given deck_codes
dsimMatrix <- function( codes ) {
  deck.matrix <- LoR.Card$cardCode |> purrr::map_dfc(setNames, object = list(numeric()))
  for (i in 1:length(codes)) {
    # if (i%%100==0) glue::glue("Fill parse matrix #{i} - {Sys.time()}") |> message()
    decklist <- codes[i] |> lordecks::get_decklist_from_code()
    deck.matrix[NROW(deck.matrix)+1, decklist$cardcode] <- as.list(decklist$count)
  }
  deck.matrix |> mutate( across(everything(), ~replace_na(.x, 0)) ) 
}

#' distance Matrix given deck_codes
cos.dsimMatrix <- function( codes ) {
  deck.matrix <- LoR.Card$cardCode |> purrr::map_dfc(setNames, object = list(numeric()))
  for (i in 1:length(codes)) {
    # if (i%%500==0) glue::glue("Fill parse matrix #{i} - {Sys.time()}") |> message()
    decklist <- codes[i] |> lordecks::get_decklist_from_code()
    deck.matrix[NROW(deck.matrix)+1, decklist$cardcode] <- as.list(decklist$count)
  }
  deck.matrix <- deck.matrix |> mutate( across(everything(), ~replace_na(.x, 0)) ) 
  eisen_cos.sim(deck.matrix)
}
```

```{r prepare-examples}
# fwrite(LoR.Archetype.Ex, "./data/example_archetye.csv")
# fwrite(LoR.Archetype.Mini, "./data/example_archetye_mini.csv")

archetypes <- c( 
  "Ashe/LeBlanc",           # 75/25 noMarauder/Marauder
  "Azir/Irelia",            # 100
  "Dragons (DE/MT)",        # 30/70 J4/PureDrake "Aurelion Sol/Jarvan IV/Shyvana",  "Aurelion Sol/Shyvana",
  "Draven/Sion (NX/PZ)",    # 80/20 DravenSion/RubinBait
  "Mistwraith Allegiance"   # 50/50 Targon/Piltover
)

LoR.Archetype.Ex   <- fread("./data/example_archetye.csv")
LoR.Archetype.Mini <- fread("./data/example_archetye_mini.csv")

# LoR.Archetype.Mini <- LoR.Archetype.Ex |>
#   filter(deck_code %in% mini.ex )

# #' distance matrix
sparseMatrix.Ex   <- dsimMatrix(codes =  LoR.Archetype.Ex$deck_code )
sparseMatrix.mini <- dsimMatrix(codes =  LoR.Archetype.Mini$deck_code )

rownames.archetype <- lapply(archetypes, function(x) sprintf("%s.%s",x,c(1:10))) |> unlist()

sparseMatrix.mini <- as.data.frame(sparseMatrix.mini)
rownames(sparseMatrix.mini) <- rownames.archetype

DSim_ex   <- eisen_cos.sim(sparseMatrix.Ex)
DSim_mini <- eisen_cos.sim(sparseMatrix.mini)
```

# Introduction

Defining archetypes on Legends of Runeterra is both a complex and simple problem.
It's simple if we consider that it's possible to define decks by the combination of champions and regions of choice but it's also complex by the fact that such definition is quite limited.

On our [previous article/analysis](https://llorr-stats.netlify.app/analysis/defining-archetypes-01/) we gave a possible method about how to compare archetypes and see if they can be considered from a shared common archetype or not.
The proposed method makes use of inferential statistical analysis to reach a conclusion.
Sadly, it's also a methodology that's more fitting a posterior analysis, when hypothetical archetypes are already defined, a tool more fitted to refine the results and not to define archetypes.

A more fitting methodology to find archetype is a form of exploratory data analysis (EDA) known as *Clustering Analysis* (CA).
Its aim is to find subgroups (or clusters) in our data without relying on a *response variable*, also the reason why it's called *unsupervised learning*.

As useful as it is a CA suffer from a fundamental problem of not being able to check out the quality of the results.
With a vast array of different algorithms and hyper-parameters this also means that finding the the "correct" way to use a CA to define archetypes (which was supposed to be the aim of this article) is not only impossible, it's also seeing the CA in the wrong way.
Surely some choices are better than others but there is no perfect answer and to be fair, this was making us, was making me, procrastinating the writing of this article.
The result, or maybe compromise was to reduce the scale on this article which will be small dive into the cluster analysis.
While I want to provide some food for thought for others in the end the main recipient of the article is myself, to provide me a more solid foundation on the topic and how to approach it knowing the basic limitations of what I'm planning to use.

# Clustering

Clustering is an heterogeneous domain. applied in a wide range of disciplines with the aim of finding subgroups or clusters, in a data set.

Clustering is considered a basic tools of data mining along side the supervised learning domain (clustering is defined as unsupervised learning) but differently from it, clustering suffer from a lack of theoretical understanding of its application.

Each example, even the most basics face the problem of selecting which algorithm to apply (known as "the user dilemma"), as there is no principled method to guide the selection.
While the theory is starting identify differences between clustering methods this field is still in it's early phase and the application are left to ad hoc decisions.
If decision are being are being made usually they are defined by engineering propetrties as run times or memory used.

This is also a consequence of clustering being a problem not well defined. As statistics is all about the question, it's not possible to define the 'correct' clustering if we have no defintion of what it is correct.

Of course, to make this concrete, we must define what it means for two or more observations to be *similar or different*. As a general rule of rule of thumb we try to aggregate similar observations in the same group while making it so that the differences between groups is overall high.

## K-Mean

K-means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters.
To perform K-means clustering, we must first specify the desired number of clusters K; then the K-means algorithm will assign each observation to exactly one of the K clusters.
After the initialization step, the algorithm iterate until cluster assignments stop changing:

-   For each of the K clusters, compute the *cluster centroid*.
    The kth cluster centroid is the vector of the p-feature means for the observations in the kth cluster

-   Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance)

As we have seen, to perform K-means clustering, we must decide how many clusters we expect in the data.
The problem of selecting K is far from simple.
This issue, along with other practical considerations that arise performing K-means clustering like is variables should be first be standardized in some way?

As you may recall the analyst specifies the number of clusters to use; preferably the analyst would like to use the optimal number of clusters.
To aid the analyst, the following explains the three popular methods for determining the optimal clusters, which includes

-   Elbow method

Recall that, the basic idea behind cluster partitioning methods, such as K-means clustrering, is to define clusters such that the total intra-cluster variation (known as total within-cluster variation or total within-cluster sum of square is minimized:)

$minimize(\sum_{k=1}^KW(C_k))$

The total within-cluster sum of squares (wss) measures the compactness of the clustering and we want it to be as small as possible.
Thus, we can use the following algorithm to define the optimal clusters

Plot the curve of wss according to the number of clusters k The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters

This is because normally the wss is related to the number of cluster by a monotone decreasing function, meaning that is the aim is simply to minimise wss then having k = n clusters each of them corresponding to a single element would give the min-wss.
One must search the optimal value we are searching for is in 1 \< k \< \|X\| finding a comprise bewtween the reduction of the objective function wss and the dimension of the hyper-parameter n.
The common practise is to use the *Elbow method* as it tells us when increasing n is less "worth" compared to the reduction in wss.

```{r elbow, fig.width=6, fig.height=4, fig.cap="within sum of squares as a fnction of number of clusters obtained with K-means applyed to the example data set"}
#| fig.width=6
#| fig.height=4
set.seed(123)
fviz_nbclust(sparseMatrix.mini, kmeans,  method = "wss") +
  theme_Publication()
```

-   Silhouette method

Prior to this article I'm not sure I even heard about this metric: From Wikipedia, the silhouette is defined as:

> The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).
> The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.
> If most objects have a high value, then the clustering configuration is appropriate.
> If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.

```{r silhouette, fig.width=6, fig.height=4, fig.cap="Silhouette as a fnction of number of clusters obtained with K-means applyed to the example data set"}
set.seed(123)
fviz_nbclust(sparseMatrix.mini, kmeans, method = "silhouette") +
  theme_Publication()
```

Fig:\@ref(fig:elbow) and Fig:\@ref(fig:silhouette) shows the results from the described methods.
Using K-means the algorithm is suggesting us around 6 or 7 clusters.
We will discuss later the quality of these choices, for now we will focus on the algorithm per se.

K-means while a powerful tool in many fields doesn't seems to be the best choice for the archetype problem.
First of all, is requires the user to define a priori the number of cluster it needs to find.
This is almost the opposite of what we would like from our algorithm.
Ideally by clustering we would prefer some indication regarding which deck to aggregate into a single archetype, or even division of sub-archetypes if the region+champion combination is not enough to distinguish archetypes.
In addition, K-means present other problems like having to start with a randomized assignment of clusters, this, of course can be controlled by having fixed seeds, rng but it still hinder the reproducibility of our results.
The choice of k is also too much subjective, especially if n is very large and the knee is not as clear as in this example.
Lastly but not less important, there is less flexibility regarding the *measure/distance* to compare decks.
While we are not limited to the Euclidean distance we can't use any distance of choice as possible with other class of cluster algorithm.

## Hierarchical Clustering

One potential disadvantage of K-means clustering is that it requires us to pre-specify the number of clusters K.
Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K.
Hierarchical clustering has an added advantage over K-means clustering in that it results in an attractive tree-based representation of the observations, called a dendogram.

The hierarchical clustering dendogram is obtained via an extremely simple algorithm.
We begin by defining some sort of dissimilarity measure between each pair of observations.
Most often, Euclidean distance is used; The algorithm proceeds iteratively.
Starting out at the bottom of the dendogram, each of the n observations is treated as its own cluster, The two clusters that are most *similar* to each other are then fused so that there are now n-1 clusters.
The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendogram is complete.

This agorithm seems simple enough, but one issue has not been addressed.
Consider the aggregation step, how did we determine which a cluster should be fused with another cluster.
We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations?
The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of groups of observations.
This extensions is archieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations.

The five most common types of linkage are:

1.  **Single Linkage**: In single linkage, we define the distance between two clusters as the minimum distance between any single data point in the first cluster and any single data point in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters with the smallest single linkage distance.

<!-- $\textit{l}_{SL}(A,B,d)=min_{a\in A,b\in B}d(a,b)$ -->

With d is a distance function

```{=tex}
\begin{equation}

\textit{l}_{SL}=min_{a\in A,b\in B}d(a,b) (\#eq:single)

\end{equation}
```
The resulting dendogram is;

```{r single, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Single linkage to the example data set"}
hc.single   <- hclust(DSim_mini, method = "single") 
fviz_dend( hc.single, show_labels = F ) +
  labs( title = "Single Linkage", y = element_blank() )
```

From Fig:\@ref(fig:single) it's possible to notice how Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time.

2.  **Complete Linkage**: In complete linkage, we define the distance between two clusters to the maximum distance between any single data point in the first cluster and any single data point in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters that have the smallest/complete linkage distance.

<!-- $\textit{l}_{CL}(A,B,d)=max_{a\in A,b\in B}d(a,b)$ -->

```{=tex}
\begin{equation}

\textit{l}_{CL}(A,B,d)=max_{a\in A,b\in B}d(a,b) (\#eq:complete)

\end{equation}
```
The resulting dendogram is;

```{r complete, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Complete linkage to the example data set"}
hc.complete <- hclust(DSim_mini, method = "complete")
fviz_dend( hc.complete, show_labels = F ) +
  labs( title = "Complete Linkage", y = element_blank() )
```

Clusters resulting from Complete linkage tend to have a spherical structure and are usually more compact.

3.  **Average Linkage**: In average linkage, we define the distance between two clusters to be the average distance between data points in the first cluster and data points in the second cluster. On the basis of this definition of distance between clusters, at each stage of the process we combine the two clusters that have the smallest average linkage distance.

$\textit{l}_{CL}(A,B,d)=\frac{\sum_{a\in A,b\in B}d(a,b)}{|A| \cdot |B| }$

```{=tex}
\begin{equation}

\textit{l}_{CL}(A,B,d)=\frac{\sum_{a\in A,b\in B}d(a,b)}{|A| \cdot |B| } (\#eq:average)

\end{equation}
```
The resulting dendogram is;

```{r average, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Average linkage to the example data set"}
hc.average  <- hclust(DSim_mini, method = "average")
fviz_dend( hc.average, show_labels = F ) +
  labs( title = "Average Linkage", y = element_blank() )
```

As intuitively as it is, clusters resulting from Average linkage is a less "extreme" version of both Single and Complete linkage

4.  **Centroid Linkage**: In centroid linkage, the distance between two clusters is the distance between the two mean vectors of the clusters also kown centroid, also kown as barycentre. At each stage of the process we combine the two clusters that have the smallest centroid distance. Centroid linkage can result in undesirable inversions, whereby two clusters are fused at a height below either of the individual clusters in the dendogram.

<!-- $\textit{l}_{Centroid}(A,B,d)= d(\overline{x},\overline{y})$ -->

```{=tex}
\begin{equation}

\textit{l}_{Centroid}(A,B,d)= d(\overline{x},\overline{y}) (\#eq:centroid)

\end{equation}
```
The resulting dendogram is;

```{r centroid, fig.width=6, fig.height=4, fig.cap="Dendogram obtained by applying Centroid linkage to the example data set"}
hc.centroid <- hclust(DSim_mini, method = "centroid")
fviz_dend( hc.centroid, show_labels = F ) +
  labs( title = "Centroid Linkage", y = element_blank() )
```

From Fig:\@ref(fig:centroid) it's possible to notice the inversion phenomenon that can occurs with Centroid linkage as the clusters are fused at a height below either of the individuals clusters in the dendogram.

5.  **Ward's Method**: This method does not directly define a measure of distance between two points or clusters. It is an ANOVA based approach. One-way univariate ANOVAs are done for each variable with groups defined by the clusters at the stage of the process. At each stage, two clusters merge that provide the smallest increase in the combined error sum of squares. ( We won't try this method )

To choose the number of clusters with hierarchical clusters can be done with the methods described with K-means so by looking at the wss or silhouette, but the strength of hierarchical clustering is being guided by the dendogram.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations.
The higher the height of the fusion, the less similar the observations are.

Similarly it's possible to cut the dendogram and define k clusters.
The height of the cut to the dendogram controls the number of clusters obtained.
It plays the same role as the k in k-means clustering.
In order to identify sub-groups (i.e. clusters).

From all dendograms it's easy to see that there is evidence of choosing around 5 or 6 clusters.
Again, we will discuss later the 'quality' of this choice.

# Defining the problem

## Upping the game

Up until now what has being described is the basic of the basic on clustering which could be found in any article/guide/course on the subject.
What will follows is more akin as a self-explanation of points that could be worth to evaluate in order to approach the algorithm problem.
Additional algorithms will be introduced with both their pro and cons, in this section we will explain the example used in this article to highlight what seems to be more interesting to explore for future analysis in the search of a way to define archetypes.

Let us introduce a couple of point from a discussion with <a href="https://twitter.com/drisoth/">Drisoth</a> and its effect:

> My philosophy is its not very helpful to have a cluster if I don't have the data to say anything about it and these low play rate clusters don't have many games, so even if I included them it wouldn't be helpful I would like to include a way for users to self define an archetype, saying include x y z card, so if someone wants to go looking they could yeah, I cluster unique decklists weighted by games on each unique decklist

> The point of clustering is to increase sample size by being able to talk about a large section of decks in aggregate rather than each one individually.
With that goal in mind, clustering  off-meta decks with low sample size as noise is not a large loss of information, since even if we clustered similar decks together the cluster would still have too little data for any meaningful analysis to be done on it.

From this approach two question comes to mind

### Should there exist outliers that's best no to assign to a cluster?

If the question is formulated simply like this with no other conditions, then yes, there is deny that outliers and/or noise exists in the archetype problem and the best way to deal with them would be not assigning them to clusters. The reason is obvious a random pile of 40 cards could turn into something resembling a possible archetype, but for the most cases it would likely turn into just that, a random pile of cards.

Their existence is certain, even worse many cluster algorithm are sensitive to noise and outliers and at various degree so are the methods described up until know.
If K-means starts on an outlier it won't be easily dealt with, also a reason why it's best practise to repeat K-means several times to avoid being too depending on the original random choice at the initialization step.
In (Agglomerative) Hierarchical Clustering (AHC), the algorithm described before, if an outlier is wrongly assigned to a cluster in some cases this error will be propagated on all the following steps with no way to intervene.[^1]

[^1]: This behaviour allows AHC to more easily identify smaller clusters. In contrast, Divisive Hierarchical clustering is more robust to noise and outliers.

Among the class of clustering algorithms that are able to deal with outlier and noise DBSCAN is probably the most popular and cited. The algorithm will be explained in the following paragraph.

### Should the data be weighted?

While being developed only recently, the concept of weighted clustering framework can be traced back into the '70 [@10.1093/biomet/58.1.91].
A more formal definition has has been developed while also introducing a series of mathematical properties related to the weights [Ackerman2016].
The use of weights can be seen as a more generalized definition of centroid if not even just its description as barycentre also called centre of mass.

It is easy to see that using unweighed data can be seen as having the same weight 1 on all data.
Applying weight would than be the same as having duplicates as multiple points with mass would correspond to a single point with weight equal to the sum of masses.
Similarly a point of zero mass can be seen as non-existent.
From this an algorithm would be equivalent to start in the middle of its steps of defining clusters and so not starting to work with the leaf but point-clusters with different masses.

Following the definition of weighted clustering, one may ask what effect does this have on the clustering algorithms?

@ackerman2016weighted introduced the properties related to the response to weighted data

Be it $(X,d)$ a dissimilarity function

And $|range(A(X,d,k))|$ the range of a partitional algorithm on a data set defined as the number of clusterings it outputs on that data over **all weights functions**

- **Weight Robust Algorithm** A partitional algorithm $A$ is weight-robust if for (X,d) and $1 < k < |X|$ $|range(A(X,d,k))|=1$.
  This means that the output of a weight robust algorithm is unaffected by the choice of weights

- **Weight Sensitive Algorithm** A partitional algorithm $A$ is weight-sensitive if for (X,d) and $1 < k < |X|$ $|range(A(X,d,k))|>1$.
  This case imply that no matter the original data, the result can be changed in response of the use of weights.
  Even if we would like to use weights to guide our output this properties doesn't seems desirables as we may want to not be too subjected to the weights.
  Of course it's not like we can't have the same results when applying different weights, and in most cases it can even be the same if the geometry of out data helps us.
  Yet, no matter the case we can't be sure that the algorithm won't be affected by the weights.

- **Weight Considering Algorithm** A partitional algorithm $A$ is weight-considering if
  * $\exists$ (X,d) and $1 < k < |X|$ so that $|range(A(X,d,k))| = 1$ and
  * $\exists$ (X,d) and $1 < k < |X|$ so that $|range(A(X,d,k))| > 1$

We took this detour of explaining how an algorithm can react to the addition of weights as we needed to point out that in some cases their inclusion doesn't bring any difference to the results.

In addition these properties can tell us a bit more of the algorithm in question.

The next figure gives an idea of what to expect for robust and sensitive cases.

![Different cluster structures based from the same data. All weight-sensitive methods select the clustering on the right while weight-robust methods select the one on the left](images/A02-weight-robust-sensitive.png)

We must remind how there is not correct partitioning, it all depends by the characteristics of each case study.

What this example wants to convey is that if we know something of the underlying geometry of our data and what we would like our algorithm to favour, while there is no correct clustering we can still reduce a little "the user's dilemma" of which algorithm to choose from.

Of the algorithms showed up until know Single linkage and Complete linkage are weight robust (and not many others more generally possess this property).
Fig:\@ref(fig:weight-hclust-single) shows an example with Single linkage

Ward and K-means (as many other variant of the 'k-family') are weight sensitive and Average linkage is weight considering.
Fig:\@ref(fig:weight-hclust-average) shows an example with Average linkage. While the difference is not big it's possible to see on the second 'main' cluster

```{r weight-hclust-single, fig.width=6, fig.height=4, fig.cap="Dendogram obtained with unweighted and weighted using Single linkage with the example data set"}
set.seed(123)
hc.single.n <- hclust(DSim_mini, method = "single", members = sample(1:5000,NROW(DSim_mini) ))
fviz_dend(hc.single, cex = 0.5 , show_labels = F ) +
  labs( title = "Unweighted Data", y = element_blank() ) +
  theme_Publication() +
  theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank() ) +
fviz_dend(hc.single.n, cex = 0.5, show_labels = F ) +
  labs( title = "Weighted Data", y = element_blank() ) +
  theme_Publication() +
  theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank() )
```

```{r weight-hclust-average, fig.width=6, fig.height=4, fig.cap="Dendogram obtained with unweighted and weighted using Average linkage with the example data set"}
set.seed(123)
hc.average.n  <- hclust(DSim_mini, method = "average", members = sample(1:5000,NROW(DSim_mini)))
fviz_dend(hc.average, cex = 0.5 , show_labels = F ) +
  labs( title = "Unweighted Data", y = element_blank() ) +
  theme_Publication() +
  theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank() ) +
fviz_dend(hc.average.n, cex = 0.5, show_labels = F ) +
  labs( title = "Weighted Data", y = element_blank() ) +
  theme_Publication() +
  theme(axis.line.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank() )
```

> So remember kids, don't try weighted data when applying Single-linkage at home (it's useless)

```{r weight-behaviour}
tribble(
  ~` `,                    ~Partional,  ~Hiercarchical,
  "Weight\nSensitive",    "k-means, k-medoids,k-median, Min-sum", "Ward's Method, Bisecting k-means",
  "Weight\nConsidering",  "Ratio-cut",           "Average Linkage",
  "Weight\nRobust",       "Min-diameter, k-center",           "Single/Complete Linkage"
) |>
  gt() |>
  tab_source_note("A classification of clustering algorithms based on their response to weighted data.
                  Source: Ackerman2016")
```
### Is there any other way to use the metagame data?

This question is more a result of seeing the Drisoth philosophy in a different way. Just because we want to be guided by the metagame in order to define archetypes doesn't mean the weighting of data is the only option.

From out perspective if we want to use the games data the reason is because we have from the games a clear idea about what to expect from certain decks.
As we have an idea of what can identify a specific archetype, sometimes we may even have what is defined as an *ideal* deck-list, a deck-list that is *exemplar*.

The easiest way to introduce this concept is by describing how it's possible to use starting from the K-means algorithm.

In K-means a cluster's cente is defined by the centroid obtained from its points but aside for this they have no constraints about their position in space. 

If we the constraints that the centre is an actual data point, they would be called **examplars**. The algorithm that can be defined from here is called **K-medoids**.

K-Medoids only slightly differ from K-Means as:

- Instead of just computing the cluster centroid we find the existing data point whose average dissimilarity between it and all other members of the cluster is minimal.

- We can use any distance to find the medoid.

An even more interesting approach is applied by a message-based cluster algorithm called **Affinity Propagation**.

What is interesting about these methods is that they can associate a real data point to each cluster.

This open for a variety of option like being able to evaluate the quality of the identified clusters, checking the similarity within-cluster or differences between-clusters with a reduced number of points/references.

As it is more complex, affinity propagation will be both described in the next paragraph.

# DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a fairly recent algorithm as it was introduced in 1996.

It's the most popular density-based method.
It features a well-defined cluster model called *density reachability*.

This type of clustering techinque connects data points that satisfy particular density criteria (minimum number of objects within a radius). After DBSCAN clustering is complete, there are three types of points: **core,border,and noise**.

Core is a point that has some (m) points within a particular (n) distance from itself.
The Border is a point that has at least one core point at distance n.
Noise is a point that is nether border nor core. Data points in sparse area required to separate clusters are considered noise and broader points.

DBSCAN uses two paramters to determine how clusters are defined:

* minPts: for a region to be considered dense, the minimum number of points requires is *minPts*
* eps: to locate data points in the neighbourhood of any points, *eps $\epsilon$* is used a distance measure

Step by step

* DBSCAN starts with a random data point (not-visited)
* The neighbourhood of this point is extracted using a distance $\epsilon$
* The clustering procedure starts if there are sufficient data points within this area and the current data point becomes the first point in the newest cluster, or else the point is marked as noise and visited
* The point within its $\epsilon$ distance neighborhood also becames a part of the same cluster for the first point in the new cluster. For all the new data points added to the cluster above, the procedure for making all the data points belong to the same cluster is repeated.
* The above two steps are repeated until all points in the cluster are determined. All points within the $\epsilon$ neighbourhood of the cluster have been visited and labelled. Once we're done with the current cluster, a new unvisited point is retrieved and processed, leading to further discovery of the cluster or noise. The procedure is repeated until all the data points are marked as visited.

[@ackerman2010towards,@ackerman2011discerning]

---

Fundumentally, all clustering methods use the same approach i.e. first we calculate similarities and then we use it to cluster the data points into groups or batches. Here we will focus on Density-based spatal clustering of applications with noise (DBSCAN) clustering methods.

Density-Based Clustering refers to unsupervised learning methods that identify distinctive group/clusters in the data, based on the idea that a cluster in data space in a continguous region of high point density, separated from other such clusters by contiguous regions of low point density.

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base algorithm for density-based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.

The DBSCAN algorithm uses two paramters: 

* minPts: The minimum number of points (a threshold) clustered toghether for a region to be considered dense.

* eps: A distance measure that will be used to locate the points in the neighbourhood of any point.

These paramters can be understood if we explore two concept called Density Reachability and Density Connectivity

Reachability in terms of density establishes a point to be reachable from another if it lies within a paritcular distance $\epsilon$ from it.

Connectivity, on the other hand involves a transitivity based chaining-approach to determine whether points are located in a particular cluster.

There are three types

Core - a point that has at least m points within distance n from itself
Border a point that has at least one core point at a distance n
Noise this is a pointthat is neither core nor a border and it has less than m points within distance n from itself

---

Usually to find the the value for $\epsilon$ we use the a plot of the k-nearest neighbor (kNN) distances for the dataset with all the distances being plotted from the smallest to largest.

In R, for the package 'dbscan' the euristic method to find $\epsilon$ is a follows:

> A suitable neighborhood size parameter eps given a fixed value for minPts can be found visually by inspecting the kNNdistplot of the data using k = minPts -1 (minPts includes the point itself, while the k-nearest neighbors distance does not).
The k-nearest neighbor distance plot sorts all data points by their k-nearest neighbor distance.
A sudden increase of the kNN distance (a knee) indicates that the points to the right are most likely outliers.
Choose eps for DBSCAN where the knee is.

Since the example data set data is quite small we will assign minPts=2  and so using k=1

```{r dbscan-opt, fig.width=6, fig.height=4}
dbscan::kNNdistplot(DSim_mini, k =  1)
abline(h = 0.2775, lty = 2, col = "red")
```
We will do the clustering with $\epsilon$=0.2775

```{r}
res.db <- dbscan::dbscan(DSim_mini, 0.2775, 2)
res.db.Drisoth <- dbscan::dbscan(dist(sparseMatrix.mini, method = "manhattan", diag = FALSE, upper = FALSE), 37, 2)
```

# K-Medoids (results)

We report the results in seeking the optimal number of clusters using K-medoid using two different metric, the Manhattan distance (or L1 distance) and the Cosine similarity to the example data set.

As with the other methods the results will be discussed at the end of the article.

```{r elbow-medoid, fig.width=12, fig.height=6, fig.cap="within sum of squares as a fnction of number of clusters obtained with K-medoids and manhattan & cosine metric applyed to the example data set"}
set.seed(123)
fviz_nbclust(sparseMatrix.mini, pam, method = "wss", metric="manhattan") +
  labs(subtitle = "Manhattan distance") +
  theme_Publication() +
fviz_nbclust(as.matrix(DSim_mini), pam, method = "wss") +
  labs(subtitle = "Cosine similarity") +
  theme_Publication()
```

# Affinity Propagation Clustering

```{r apcluster, fig.width=12, fig.height=8}
# identical(
#   linKernel(DSim_mini,normalize = T)[1:5,1:5],
#   1-as.matrix(eisen_cos.sim(DSim_mini))[1:5,1:5]
# )

ap.res <- apcluster(linKernel(DSim_mini,normalize = T))

table(ap.res@idx,LoR.Archetype.Mini$archetype)

apcluster::heatmap(
  ap.res,linKernel(DSim_mini,normalize = T),
  # Rowv=NA, Colv=NA,
  cexRow= 0.75, cexCol = 0.75,
  # sideColors=c("darkgreen", "yellowgreen"),
  # col=terrain.colors(12)
  )
```



# Data

The example used in this article is made out of 50 decks.

-   Five archetypes with different regions & different play style have been selected as 'core'

    1.  Ashe/LeBlanc
    2.  Azir/Irelia
    3.  Dragons (DE/MT)
    4.  Draven/Sion (NX/PZ)
    5.  Mistwraith Alligiance

Does this mean that the correct answer of k-cluster we should have expected was 5? Well... not necessarily, there is fact a catch that I didn't mention. Among the 'archetypes' I also selected some of their sub-archetypes.

To help explain what I actually choose I'll also report the code I used to samle the deck_list

```{r select-archetype, echo=TRUE, eval=FALSE}
# how I selected the decks, it is reproducible as more decks will be inserted in the DB

#' archetypes names
archetypes <- c( 
  "Ashe/LeBlanc",           # 75/25 noMarauder/Marauder
  "Azir/Irelia",            # 100
  "Dragons (DE/MT)",        # 30/70 J4/PureDrake "Aurelion Sol/Jarvan IV/Shyvana",  "Aurelion Sol/Shyvana",
  "Draven/Sion (NX/PZ)",    # 80/20 DravenSion/RubinBait
  "Mistwraith Allegiance"   # 50/50 Targon/Piltover
)

#' sampling a bigger number of decks for a bigger example I ended up not using
set.seed(123)
Rep1 <- LoR.Deck.RMD |>
  filter( archetype == "Ashe/LeBlanc" & is.na(archetype_pretty) ) |>
  slice_sample(n = 75) |>
  pull(deck_code)

set.seed(123)
Rep2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Marauder" ) |>
  slice_sample(n = 25) |>
  pull(deck_code)

set.seed(123)
AI <- LoR.Deck.RMD |>
  filter( archetype == "Azir/Irelia", ) |>
  slice_sample(n = 100) |>
  pull(deck_code)

set.seed(123)
Dragon1 <- LoR.Deck.RMD |>
  filter( archetype == "Aurelion Sol/Jarvan IV/Shyvana" ) |>
  slice_sample(n = 30) |>
  pull(deck_code)

set.seed(123)
Dragon2 <- LoR.Deck.RMD |>
  filter( archetype == "Aurelion Sol/Shyvana" ) |>
  slice_sample(n = 70) |>
  pull(deck_code)

set.seed(123)
Sion1 <- LoR.Deck.RMD |>
  filter( archetype == "Draven/Sion (NX/PZ)" & is.na(archetype_pretty) ) |>
  slice_sample(n = 80) |>
  pull(deck_code)

set.seed(123)
Sion2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "RubinBait - Draven/Sion", ) |>
  slice_sample(n = 20) |>
  pull(deck_code)

set.seed(123)
Mist1 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Targon") ) |>
  slice_sample(n = 50) |>
  pull(deck_code)

set.seed(123)
Mist2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Piltover") ) |>
  slice_sample(n = 50) |>
  pull(deck_code)

#' sub-sampling the decks I ended up not using
LoR.Archetype.Ex <- rbind(
  LoR.Deck.RMD[ deck_code %in% Rep1, ],
  LoR.Deck.RMD[ deck_code %in% Rep2, ],
  LoR.Deck.RMD[ deck_code %in% AI, ],
  LoR.Deck.RMD[ deck_code %in% Dragon1, ],
  LoR.Deck.RMD[ deck_code %in% Dragon2, ],
  LoR.Deck.RMD[ deck_code %in% Sion1, ],
  LoR.Deck.RMD[ deck_code %in% Sion2, ],
  LoR.Deck.RMD[ deck_code %in% Mist1, ],
  LoR.Deck.RMD[ deck_code %in% Mist2, ]
)
  
mini.ex <- c(Rep1[1:7], # Ashe/LB
             Rep2[1:3], # Marauder
             AI[1:10],  # AI
             Dragon1[1:3], # with J4
             Dragon2[1:7], # no J4
             Sion1[1:8],   # OG
             Sion2[1:2],   # fake-burn
             Mist1[1:5],   # MT
             Mist2[1:5])   # PnZ


```

So if I had to better specify what the sample contains:

-   Five archetypes with different regions & different play style have been selected as 'core'. Almost all archetypes contains some variants of the same decks 

    1.  Ashe/LeBlanc
      * 7 decks of 'standard' Ashe/LB
      * 3 decks of the 'marauder' variant which is defined by playing 3 copies of Strength in Numbers in a Freljord/Noxus deck. No Ashe/LB could be sampled from this subgroup
    2.  Azir/Irelia
      * 10 decks of Azir/Irelia
    3.  Dragons (DE/MT)
      * 3 decks with at least Jarvan IV/Shyvana/Aureion Sol
      * 7 decks with at least Shyvana/Aureion Sol (but no Jarvan IV)
    4.  Draven/Sion (NX/PZ)
      * 8 decks of 'standard' Draven/Sion
      * 2 decks of the Rubin-Bait variant
    5.  Mistwraith Alligiance - Mistwraith Alligiance decks are defined by having 3 copies Mistwraith and overall 37 SI cards
      * 5 decks of a Mistwraith decks with Mount Targon and 3 copies of Pale Cascade
      * 5 decks of a Mistwraith decks with PnZ and 3 copies of Iterative Improvements

Something that I want to highlight of the Mistwraith decks is that the option for the champions of choice was completely free as they were in the end not relevant for the strategy of the deck and the following table shows how the champions combination was distributed:

```{r}
LoR.Archetype.Mini[41:50,] |> tabyl(archetype) |> gt()
```

The Mistwraith example could be said if a key for why and  how I want to approach the archetype problem. SI is probably the region who is less tied by its champions per se. Thresh is more an exception but for example Kallista and Elise were included in a wide variety of decks that couldn't be simply identified by their use.

In addition to the 'champions problem' the Mistwraith has the additional problem of being the first archetype to this is not bound by the regions it use. As of now Bandle expanded this concept with a variety of BandleTree Decks, but Mistwraith was the OG case.

```{r}
fviz_dend(hcut(DSim_mini, k = 5, method = "average"),
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
          label_cols =  rep(1:5,each = 10),
          rect = TRUE, cex = 0.5 )
```


```{r}
hc.average.5 <- hcut(DSim_mini, k = 5, method = "average")

table(
  unname(hc.average.5$cluster),
  res.db$cluster
) |>
  kable(row.names = T)
```

```{r}

```



```{r twitter-meta, echo = FALSE}
library(metathis)
meta() %>%
  meta_description(
    "First entry on a series of article that will gather my explorations over different way to define archetypes in Legends of Runeterra"
  ) %>%
  meta_viewport() %>%
  meta_social(
    title = "Defining Archetypes #1: Looking at the similarity of Akshan/Sivir/Zed with similar archetypes",
    url = "https://llorr-stats.netlify.app/",
    image = "images/archetypes/A01-ASZSZ.png",
    image_alt = "ASZSZ",
    og_type = "website",
    og_author = "Legna",
    twitter_card_type = "summary",
    twitter_creator = "@Maou_Legna"
  )
```

# Legal bla bla {.unnumbered}

This content was created under Riot Games' "Legal Jibber Jabber" policy using assets owned by Riot Games.
Riot Games does not endorse or sponsor this project.
