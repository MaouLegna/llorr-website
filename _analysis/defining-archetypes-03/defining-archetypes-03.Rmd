---
params:
  ind: "03"
  title: "Defining Archetypes #3: "
  description: "Part1: Introduction and Methods"
title: | 
  `r params$title`
description: |
  `r params$description`
# preview: 
base_url: https://www.llorr-stats.com
author:
  - name: Valentino (Legna) Vazzoler
date: 11-11-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    self_contained: false
citation: false
slug: legna2021archetype03
bibliography: references.bib
draft: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=12,
  fig.height=8,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
options(scipen = 999)
source(file.path("C:","LlorR","scripts","lor_main.R" ))
xaringanExtra::use_panelset()

pacman::p_load(apcluster,dbscan,fpc,factoextra)
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(
  font_family = "Helvetica",
  active_foreground = "white",
  hover_foreground = "black",
  hover_border_color = "black",
  active_background = "#007fff"
  )
```

# Introduction

In the [previous article/analysis](https://llorr-stats.netlify.app/analysis/defining-archetypes-02/) on defining archetypes we introduced the basic theory of Cluster Analysis (CA) and applied to a simple toy-example of Legends of Runeterra (LoR) decks.

Along side the general theory we introduced and applied a few widely-used algorithms and some of the "newer" ones like Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Affinity Propagation Clustering (APcluster).

The example was tailored to give a general idea about the methods, some of their strong point and also potential weaknesses with this specific problem.

As the previous analysis was mostly based around the mathematical properties and a well defined example it can't translate directly to a generalized use with the commonly analysed data.

Here, we want to continue to use the Cluster Analysis but this times applied to a set of data on a scale more similar to a 'real case'.

```{r load-data}
LoR.Deck.RMD       <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))

fread(file.path("C:","LlorR","data","raw","games","LoR_MatchDT_S10_218_220.csv"), select=c("game_type","deck_code_1","deck_code_2"), header = T, na.strings = c("", NA)) |>
  filter(game_type=="Ranked") |>
  select(-game_type) |>
  pivot_longer(cols = contains("deck_code"),values_to = "deck_code",names_to = NULL) |>
  distinct(deck_code) |>
  as.data.table() -> S10Decks
```

To give a reference the Between Worlds Season (S10) from patch 2.18 to 2.20 featured at Master rank ~`r floor(NROW(S10Decks)/1000)*1000` decks.

### An Engineering Problem

```{r create-example, eval=FALSE, include=FALSE}
# set.seed(123)
# LoR.Deck.RMD |>
#   group_by(factions) |>
#   # more than 800 would have trouble for some factions
#   slice_sample( n = 800 ) |>
#   ungroup() |>
#   fwrite("./data/example_archetye_03.csv")
# 
# set.seed(123)
# Archetype3.20 <- example_archetye_03 |>
#   group_by(factions) |>
#   slice_sample(n=20) |>
#   ungroup()
# 
# fwrite(Archetype3.20, "./data/example_archetye_03_20.csv")
# cos.dsim.arch3.20  <- cos.dsimMatrix(Archetype3.20$deck_code)
# fwrite(as.matrix(cos.dsim.arch3.20),"./data/example_cosDist_03_20.csv")
# 
# set.seed(123)
# Archetype3.100 <- example_archetye_03 |>
#   group_by(factions) |>
#   slice_sample(n=100 ) |>
#   ungroup()
# 
# fwrite(Archetype3.100,"./data/example_archetye_03_100.csv")
# cos.dsim.arch3.100 <- cos.dsimMatrix(Archetype3.100$deck_code)
# fwrite(as.matrix(cos.dsim.arch3.100),"./data/example_cosDist_03_100.csv")
```

The main issue to use 'real-case' amount of decks is related to the computational complexity.

Distance Matrices and so, all algorithm derived by them have a quadratic growth ($o^2$) so scalability is a problem.

This makes the problem not feasible at high dimension but that's not all. This project to "define archetypes" is not meant to be purely theoretical, the aim is to also apply it to the collected data and provide the results, be it in the reports or any other content.

One may not be used to these context so let us give some numbers of the problem at hand:

At the moment of the writing of this article our Deck's dataset is made of neatly 1M decks.

Realistically we would never try to works with all of them at the same time, at least not when using a cluster analysis (CA).

A proper time-frame can a single competitive Season.

The tenth competitive Season as we mentioned had ~`r floor(NROW(S10Decks)/1000)*1000` decks at Master rank. We will start from here but let us remember that

* The Season by itself didn't see a huge amount of games and so of deck being played compared to others.

* It's just Master players so usually a more homogeneous population.

* With more and more cards being released the single instance of decks is more likely to increase considerably.

```{r read-example-03}
example_archetye_03 <- data.table::fread("C:/Users/Valentino Vazzoler/Documents/R/llorr-website/data/example_archetye_03.csv",na.strings = c("",NA))
```

For this article we created an example made of `r NROW(example_archetye_03)` decks.

The creation criteria will be explained more in detail later. The size is clearly to have an order of magnitude similar to the S10-MasterRank numbers.

Now, let us explain we it would be impossible to even simple use the algorithm applied in the previous article, because of the distance matrix and it's size.

We wouldn't be able to even allocate enough memory to work with it.

```{r read-cos-dsim}
cos.dsim.arch3.20 <- fread("./data/example_cosDist_03_20.csv",header=T) |> 
  as.matrix() |> as.dist()

cos.dsim.arch3.100 <- fread("./data/example_cosDist_03_100.csv",header=T) |>
  as.matrix() |> as.dist()
```

To give some context:

* If we used only a fraction of the chosen decks, just 1100, then we would have a $1100 \times 1100$ distance matrix that requires: `r format(object.size(cos.dsim.arch3.20), units = "Mb")`

* If we increased the choice to 5500 decks, the resulting 5500x5500 distance matrix would occupy: `r format(object.size(cos.dsim.arch3.100), units = "Mb")`.

Quite the increase but consistent to the fact that the second matrix is 25 times the smaller one and we can see the issue with the quadratic growth that comes with the matrix.

* By using all the 44k decks we can't even try to recreate the distance matrix, it would require quite the machine and we can infer that just that matrix would be of around  `r 116*(44000^2/5500^2)` Mb.

### Needles in the Shruiman (vast) Desert

```{r fig.width=12, fig.height=8}
# factoextra::fviz_dist(cos.dsim.arch3.20,show_labels = F,
#                       gradient = list(low = "yellow", high = "firebrick3")) +
#   theme(legend.position = "none")
```

Let's assume that we solve the computation and allocation of the distance matrix.

We now wants to highlight that it would be almost a waste.
This would not be enough, because, every aspect that uses it will still be affected by the same computational burden.

And this is not worth/necessary

To reason can be explained with the following step:
A typical step in applying a clustering analysis is even just looking for the potential number of cluster we will find by displaying the heatmap of the distance matrix.

Again, we will show the results with 1100 and 5500 decks. With more than 5500 our machine would struggle to just not crash.

The resulting 1100 decks heatamap is:

```{r Heat1100, fig.width=8, fig.height=8, fig.cap="heatmap of a 1100 x 1100 distance matrix of LoR decks"}
lattice::levelplot(as.matrix(cos.dsim.arch3.20), main="1100 X 1100 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)))
```
The resulting 5500 decks heatamap is:

```{r Heat5500, fig.width=8, fig.height=8, fig.cap="heatmap of a 5500 x 5500 distance matrix of LoR decks"}
lattice::levelplot(as.matrix(cos.dsim.arch3.100), main="5500 X 5500 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)),colorkey=NULL)

# heatmap3 version
# heatmap3::heatmap3(as.matrix(cos.dsim.arch3.20),useRaster=T,Rowv=NA,Colv=NA, col = colorRampPalette(c("yellow","firebrick3"))(1024))
# heatmap3::heatmap3(as.matrix(cos.dsim.arch3.100),useRaster=TRUE,Rowv=NA,Colv=NA, col=colorRampPalette(c("yellow","firebrick3"))(1024))
```

As we can see in Fig:\@ref(fig:Heat1100) and Fig:\@ref(fig:Heat5500) there are clear indication of the presence of several small clusters (the yellow areas) but the majority, the vast majority of the matrix is actually null. While it's true that part of this is related by how the example matrix was created, even assuming actual "real-data" the resulting plot wouldn't differ by much.

The main reason simply because by the limitation imposed by the deck building just can't allow to have huge clusters compared to the one if the diagonal from our example. There could be exceptions but for the most part it would imply that certain regions and cards would vastly dominate their playrate across a huge amount of decks.

While some players may think that this condition is already applied with Regions like BandleCity being played way more than the other and some cards being always present in some decks, no that is still not enough. We would require the decks to be overall even way, way more similar to each other.

# Methods

BUT if the "whole" problem related to the distance matrix is too big an approach comes to help: to "Divide et Impera" the problem

We are going to refer to the similarity matrix instead of the normalized distance matrix.

First of all, we knows that the similarity matrix has a structure so that it is defined $0_{n \times n}$ null-matrix for most of sub-matrix defined in it.

To be more precise, everywhere all regions from both deck are different the similarity between these deck is 0

There are sadly exception to this property: when we deal with dual region cards we can have non overlapping "factions" but still common cards

> Example: a BandleCity/Noxus (BC/NX) deck and a Demacia/Ionia (DE/IO) deck both sharing "Poppy" as a card

> BandleCity Ruined more than the Ruination

For now everything we will do will ignore this factor. While we can take into account the second-region of a card and simply expand the cards pool with them there other factor that would increase the complexity of the problem so that it's left for future works.


## The Shuriman Desert is Sparse

Let's assume we don't have the problem of the Dual-Region cards, what do we know about the structure of the similarity matrix ?

We know that there are 55 unique combination of region

When comparing two decks we so have 55^2 possible sub-matrix of confrontation between combination of regions. Of course we as the Matrix is symmetrical it's reduced to just 1540 cases.

But even among those 1540, the cases different from zero are not that many.

As we mentioned only cases with deck that share at least a common region can be different from zero.

This makes it so the number of sub-matrix to consider follows the following formula:

$\sum_i^n((i-1)^2+\frac{i(i+1)}{2})$

```{r table-sparse}
#' region to find the percentage all cases of "interceptions" between elements like factions. For example BC/NX does correlate with all BC and all NX cases
regionComboInterception <- function(n) {
  # n = 10
  dim <- n*(n+1)/2
  total = 0
  for ( i in 1:n ) {
    p1 <- i*(i+1)/2
    p2 <- (i-1)^2
    total <- total + p1 + p2
  }
  total/(dim*(dim+1)/2)
}
  
tibble::tibble(
  region = 1:10,
  coverage = map_dbl(1:10, ~regionComboInterception(.x)) ) |>
  gt::gt() |>
  fmt_percent(
    columns = coverage,
    decimals = 2
  ) |>
  tab_header(
    title = "Sparsity of Similarity Matrix",
    subtitle = md("Percent of sub-matrix that share a common region <br> by number of existing regions.")
  ) |>
  gtExtras::gt_theme_538()
```

<caption>

(\#tab:table-sparse)

</caption>

From Tab:\@ref(tab:table-sparse) we can see that in the current setting with n=10 existing regions of the 1540 cases only the 32% actually meaningful.

But this is not over as only a small subset of this 32% is actually relevant. 

What follows is a simplification which works for most cases.

When we compare decks with a common shared region not all comparison are meaningful.

If decks with only one shared regions have to be similar either the key cards are unique enough to identify the decks and so the regions are not important, or the shared region presence must be overwhelming compared to the second one. If it wasn't by definition of the similarity we used up until know (a measure defined by number of copies of a card in a deck) than the similarity will be small giving indication of different archetypes/clusters.

In other words the decks that matters are bridge among the regions with a main region that mostly identify the deck.

As easy example is the Mistwraith decks that we also used in the previous article.

Normally Mistwraith decks are made mostly of Shadow Isles cards while the remaining cards are often from a region of choice that synergies with the rest of the deck but are not essential / key-cards. This cards act are the bridge between regions.

Is the deck using as the only not-SI cards 3 copies of Pale Cascade? Then it's a SI/MT deck

Is the deck using as the only not-SI cards any copies of Raz Bloodmane? Then it's a SI/SH deck and SI/MT and SI/SH are connected

Is the deck using as the only not-SI cards 3 copies of Iterative Improvement? Then it's a SI/PZ deck and we connected SH,MT and PZ by SI.

And so on

Of course as mentioned this is more an approximation as one could that a Mistwraith deck with just 20 SI cards and a lot of duplication cards can be defined as Mistwraith decks but here we assume that that by itself is an over-simplification and it's more likely to be defined as a sub-archetype of Mistwraith Allegiance that may have sense to the connected/related the 20-SI cards Mistwraith deck would most likely while having a similar goal and strategy would have a play-pattern that differs enough from the Mistwraith Allegiance so that in this initial clustering it is more correct to separate them.

In a second step, when trying to find more generalized archetypes (be it something like aggro/control/initiative/resource) then there is reason to try aggregated these different "aggregated version" of Mistwraith decks but as of now, with 

## Cross-Region Comparisons

We so choose to propose to set a rule so that only these bridge decks are clustered. This open for a consequential step, as we restricted the between regions cases to decks with an overwhelming presence of a region to the other they can be all aggregated into the ~Mono region

What benchmark and rules should we use ? This is tricky but the idea is to use both an hard defined benchmark and the presence of Allegiance cards

```{r allegiance-mess}
S10Decks |>
  left_join(LoR.Deck.RMD, by = "deck_code") |>
  mutate(cards.region = ifelse(!is.na(cards.region.fix), cards.region.fix, cards.region ) )  |>
  filter(!is.na(allegiance)) |>
  mutate(
    alle = map(allegiance, ~str_split(.x,pattern = ",")[[1]] ),
    alle = map(alle,unique),
    alle = map_chr(alle,~str_flatten(.x,collapse = ",")),
    alle.region = str_sub(alle,3,4)
  ) |>
  separate_rows( cards.region,cards.region.freq, sep = ",",convert = T ) |>
  select(alle,alle.region,cards.region,cards.region.freq) |>
  filter(alle.region==cards.region) |>
  rename(cards.region.num=cards.region.freq) |>
  filter(str_detect(alle,",")) |>
  tabyl(cards.region.num)
  # as_tibble() |>
  # rename_with(~c("cards","n","cumfreq")) |>
  # arrange(desc(cards)) |>
  # mutate(cumfreq=cumsum(cumfreq)) |>
  # gt()
```

An idea was to use the inclusion of Allegiance cards as their effect is not a gamble only in deck that either can heavily control their draw or are made mostly of a single region. As LoR offer some card draw control options they are far from common so usually use of Allegiance cards do predict the use of an ~Mono deck.



Sadly this is true, but not entirely. Not only these cards have being played in decks where their activation is very risky but there are even cases there more the Allegiance card for both region has being played in the same deck.

There is also an additional potential failing point of this strategy: the "Theseus's Archetype".

This is related to the problem with Mistwraith deck mentioned before. We said that there can be a ~Mono SI version and others SI/XX when the second region has way too many cards to assume it's still the same deck.

Overall this problem is being left for another moment to refine the result of a cluster analysis but conceptually we believe there is merit both in saying there can be more version of the same base-archetype and then we can work on an aggregated one

```{r monoDecks}
alleDeck <- example_archetye_03 |>
  select(deck_code,archetype,allegiance,factions,cards.region.freq) |>
  separate_rows(factions,cards.region.freq) |>
  mutate(cards.region.freq = as.numeric(cards.region.freq) ) |>
  filter(cards.region.freq >=  33 | !is.na(allegiance) ) |>
  as.data.table()
```

# Analysis

```{r monoIonia, fig.width=12, fig.height=8}
dist.IO <- dsimMatrix(alleDeck[factions == "IO" & str_detect(allegiance,"IO"),deck_code])

ap.IO <- apcluster(linKernel(dist.IO,normalize = T),details = T)

# dend.IO <- hclust(dist.IO,method = "average")

set.seed(123)


linKernel(dist.IO[1:5,1:5],normalize = T)

dsimMatrix(alleDeck[factions == "IO" & str_detect(allegiance,"IO"),deck_code][1:5])

cos.dsimMatrix(alleDeck[factions == "IO" & str_detect(allegiance,"IO"),deck_code][1:5])

linKernel(
  dsimMatrix(alleDeck[factions == "IO" & str_detect(allegiance,"IO"),deck_code][1:5]),
  normalize = T)


apcluster::heatmap(ap.IO,linKernel(dist.IO,normalize = T), cexRow= 0, cexCol = 0, legend = "col" )

factoextra::fviz_dend(dend.IO)
```


```{r}
# table(example_archetye_03[deck_code %in% alleDeck[factions == "IO" & str_detect(allegiance,"IO"),deck_code], archetype],ap.IO@idx)
```

```{r}
NXPZ <- cos.dsimMatrix(example_archetye_03[factions == "NX,PZ",deck_code])

example_archetye_03[factions == "NX,PZ",tabyl(archetype)]

dend.NXPZ <- hclust(NXPZ,method = "average")

factoextra::fviz_dend(dend.NXPZ)

set.seed(123)
ap.NXPZ <- apcluster(linKernel(NXPZ,normalize = T),details = T, q= 0)

apcluster::heatmap(
  ap.NXPZ,linKernel(NXPZ,normalize = T),
  # Rowv=NA, Colv=NA,
  # cexRow= 0.75, cexCol = 0.75,
  cexRow= 0, cexCol = 0,
  # sideColors=c("darkgreen", "yellowgreen"),
  # col=terrain.colors(12),
  legend = "col"
  )

print( table(example_archetye_03[deck_code %in% example_archetye_03[factions == "NX,PZ",deck_code], archetype],ap.NXPZ@idx), zero.print = ".")


getOption("max.print")

options(max.print=2000)

example_archetye_03[factions == "NX,PZ",deck_code][which(ap.NXPZ@idx==173)]
example_archetye_03[factions == "NX,PZ",deck_code][which(ap.NXPZ@idx==280)]
example_archetye_03[factions == "NX,PZ",deck_code][which(ap.NXPZ@idx==293)]
```

