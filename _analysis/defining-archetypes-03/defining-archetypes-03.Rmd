---
params:
  ind: "03"
  title: "Defining Archetypes #3: "
  description: ""
title: | 
  `r params$title`
description: |
  `r params$description`
# preview: 
base_url: https://www.llorr-stats.com
author:
  - name: Valentino (Legna) Vazzoler
date: 11-11-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    self_contained: false
citation: false
bibliography: references.bib
draft: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=6,
  fig.height=4,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
options(scipen = 999)
source(file.path("C:","LlorR","scripts","lor_main.R" ))
xaringanExtra::use_panelset()

pacman::p_load(apcluster,dbscan,fpc,factoextra)
```

```{r panelset-style}
xaringanExtra::style_panelset_tabs(
  font_family = "Helvetica",
  active_foreground = "white",
  hover_foreground = "black",
  hover_border_color = "black",
  active_background = "#007fff"
  )
```

# Introduction

In the previous article on archetypes [previous article/analysis](https://llorr-stats.netlify.app/analysis/defining-archetypes-02/) we introduced the concept/general theory of Cluster Analysis (CA) and applied to a simple toy-example of Legends of Runeterra (LoR) decks.

Along side the general theory we introduced and applied a few commonly used algorithm and some new ones like Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Affinity Propagation Clustering (APcluster).

The example was tailored to give a general idea

Here, we want to continue to use the Cluster Analysis but this times applied to a set of data on the magnitude equal or at least similar to a real 

Decklist from "Last Season Master" games in patch 2.18 ~90k decks
Decklist from "Master" games in patch 2.18 ~10k decks

---

Distance Matrix and so, all algorithm derived by them have a quadratic growth ($o^2$) so scalability is a problem.

Questo rende l'applicazione di certe scelte/metodi/ecc non appropriate/non fattibili ad alte numerosità.

BUT riguardo la matrice di distanza ed i possibili cluster contenuti in essa abbiamo delle informazioni e possiamo semplificare il problema.

Per quanto segue andremo a riferirci alla matrice di similarità in contesto in cui la matrice di distanza è definita per valore $in$ [0,1]

Innanzitutto sappiamo che la matrice di distanza ha una struttura per cui è definita $0_{n \times n}$ null-matrix for most of sub-matrix defined in it.

Nello specifico, se tutte le regioni tra due deck differiscono tra di loro la similarità tra i due deck sarà necessariamente nulla.

There are sadly exception to this property: when we deal with dual region cards we can have non overlapping "factions" but still common cards

> Example: a BandleCity/Noxus (BC/NX) deck and a Demacia/Ionia (DE/IO) deck both sharing "Poppy" as a card

> BandleCity Ruined more than the Ruination

Per ora le considerazioni andranno ad ignorare la questione DualRegion perché andrebbe ad aggiungere un grado di complessità non indifferente per quella che sarebbe per ora una correzzione minimale.

## Cosa sappiamo delle "Zone"?

### The Shuriman Desert is Sparse

Assumiamo non ci sia il problema delle carte DualRegion, cosa sappiamo della struttura della matrice di similarità e della sua struttura?

Possiamo considerare che esistono 55 combinazini uniche di scelte di regioni (45 Doppia Regione e 10 MonoRegione)

Di per se esistono quindi `r 55^2` possibili sotto-matrici di confronti tra regioni, ma siccome la matrice è simmetrica "solo" 1540 casi sono da valutare.

A questo punto entra in gioco la questione regioni in comune

<!-- Ipotizzando di non dover fare controlli  -->

$\sum_i^n((i-1)^2+\frac{i(i+1)}{2})$

Non solo, ma di questo 32%, solo una piccola parte ha senso. Cioè, quei deck al di fuori della coppia di regioni quello che vogliamo controllare sono i casi che possono essere "bridge" between regions as Mistwratih.

Se devono essere "bridge" ci deve essere un collegamento tra le regioni e quindi deck in una delle due regioni diventa irrelevante, e quindi ha senso guardare solo deck che sono "Quasi-Alligiance"

> Esempio, possono essere dei dati Mistwraith the usano più carte PnZ e quindi anche se la strategia è simile, è forse più corretto considerare il deck come un sub-archetipo (?)

Quindi non farò tutti i confronti compresi DE/SH with DE/BW? No, non proprio. Questo confronto si farà solo tra deck DE/SH e DE/BW che sono "quasi-alligiance" e DE è ponte tra gli archetipi, quindi solo quei deck con qualcosa come 35 carte DE

Quali problemi ci sono con questa assunzione? Che si presume che deck con troppe carte di un archetipo ad un certo punto il deck diventa "specifico" di regione

What if we have a Draven/Ez that use even more PnZ cards than usual? Yes, it should still works as Draven/Ez but for now it would most likely differentiate. E se avessimo per esempio un AzIrelia quasi "Alligiance"?

```{r create-example}

# load DeckDT
#'###########
LoR.Deck.RMD        <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))

# set.seed(123)
# Archetype3 <- LoR.Deck.RMD |>
#   group_by(factions) |>
#   slice_sample( n = 300 )
# fwrite(Archetype3,"./data/example_archetye_03.csv")

saveRDS(DeckArchetype3,example_archetye_03) 


# set.seed(123)
# Archetype3.20 <- Archetype3 |>
#   group_by(factions) |>
#   slice_sample( n = 20 )
# cos.dsim.arch3.20  <- cos.dsimMatrix(Archetype3.20$deck_code)
# fwrite(Archetype3.20, "./data/example_archetye_03_20.csv")
# fwrite(as.matrix(cos.dsim.arch3.20),"./data/example_cosDist_03_20.csv")
# 
# set.seed(123)
# Archetype3.100 <- Archetype3 |>
#   group_by(factions) |>
#   slice_sample( n = 100 )
# cos.dsim.arch3.100 <- cos.dsimMatrix(Archetype3.100$deck_code)
# fwrite(Archetype3.100,"./data/example_archetye_03_100.csv")
# fwrite(as.matrix(cos.dsim.arch3.100),"./data/example_cosDist_03_100.csv")


```


```{r}
#' region to find the percentage all cases of "interceptions" between elements like factions. For example BC/NX does correlate with all BC and all NX cases
regionComboInterception <- function(n) {
  # n = 10
  dim <- n*(n+1)/2
  total = 0
  for ( i in 1:n ) {
    p1 <- i*(i+1)/2
    p2 <- (i-1)^2
    total <- total + p1 + p2
  }
  total/(dim*(dim+1)/2)
}
  
tibble( region = 1:10,
        coverage = map_dbl(1:10, ~regionComboInterception(.x)) ) |>
  gt()
```



```{r raw-data}
# Archetype3 <- fread("./data/example_archetye_03.csv")
# Archetype3.20  <- fread("./data/example_archetye_03_20.csv")
# Archetype3.100 <- fread("./data/example_archetye_03_100.csv")

cos.dsim.arch3.20 <- fread("./data/example_cosDist_03_20.csv",header=T) |>
  as.matrix() |> as.dist()

# cos.dsim.arch3.100 <- fread("./data/example_cosDist_03_100.csv",header=T) |>
#   as.matrix() |> as.dist()
```

Trivia: the dimension of the distance matrix when I used 20 example for possible faction combination so a 1100x1100[^1] matrix is

[^1]: 20*55=1100

```{r}
object.size(cos.dsim.arch3.20)
```

While the size for a case with 100 decks, a 5500x5500 decks is:

```{r}
object.size(cos.dsim.arch3.100)
```

This doesn't seems too bad, but there are factors to consider:
- The increase is size scale badly with the increase in rows/columns. 5500 are hardly a lot, a patch can easily have ~20k different deck-list played.
- The computation fatigue scales even worse -> explain prodotto matriciale

To this we have to add 

The first thing to do in any cluster analysis that we actually forgot to even do the most basic step and it is to display the distance matrix

```{r fig.width=12, fig.height=8}
# library("factoextra")
factoextra::fviz_dist(cos.dsim.arch3.20,show_labels = F,
                      gradient = list(low = "yellow", high = "firebrick3")) +
  theme(legend.position = "none")
```

```{r fig.width=12, fig.height=8}
lattice::levelplot(as.matrix(cos.dsim.arch3.20), main="1100 X 1100 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)))
```

```{r fig.width=12, fig.height=8}
lattice::levelplot(as.matrix(cos.dsim.arch3.100), main="5500 X 5500 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)))
```

```{r fig.width=12, fig.height=8}
heatmap3::heatmap3(as.matrix(cos.dsim.arch3.20),useRaster=T,Rowv=NA,Colv=NA, col = colorRampPalette(c("yellow","firebrick3"))(1024))
                   # legendfun=function() NA )
```

```{r fig.width=12, fig.height=8}
heatmap3::heatmap3(as.matrix(cos.dsim.arch3.100),useRaster=TRUE,Rowv=NA,Colv=NA, col=colorRampPalette(c("yellow","firebrick3"))(1024))
```

```{r}
# factions <- LoR.Deck.RMD |>
#   distinct(factions) |>
#   arrange(factions) |>
#   pull(factions) 
# 
# prova <- tibble(
#   f1 = rep(factions, each = length(factions)),
#   f2 = rep(factions, times = length(factions)),
#   f3 = map2(.x=f1,.y=f2,~c(.x,.y)),
#   # f3 = glue("{f1},{f2}")
# )
# 
# prova$f3 <- lapply(prova$f3, function(x) sort(x) )
# 
# prova |>
#   distinct(f3,.keep_all = T) |>
#   mutate(
#     f1 = map_chr(.x = f1, ~str_remove_all(.x, paste(c("_Name","faction_", ""), collapse = "|") ) ),
#     f2 = map_chr(.x = f2, ~str_remove_all(.x, paste(c("_Name","faction_", ""), collapse = "|") ) )
#   ) |>
#   separate(f1,sep = ",",into = c("f11","f12"))
```

```{r fig.width=12, fig.height=8}
# install.packages("heatmap3")


nrowcol <- 1000
dat <- matrix(ifelse(runif(nrowcol*nrowcol) > 0.5, 1, 0), nrow=nrowcol)



heatmap3(dat,useRaster=TRUE,Rowv=NA,Colv=NA)

heatmap3(matrix(rnorm(10),ncol=2),legendfun=function()
plot(0,0,bty="n",xaxt="n",yaxt="n",type="n"))

# install.packages("gplots")


# heatmap.2(x=as.matrix(cos.dsim.arch3))
```

```{r fig.width=12, fig.height=8}
# install.packages("lattice")
library(lattice)

#Build the data
nrowcol <- 1000
dat <- matrix(ifelse(runif(nrowcol*nrowcol) > 0.5, 1, 0), nrow=nrowcol)

#Build the palette and plot it




x <- seq(pi/4, 5 * pi, length.out = 100)
y <- seq(pi/4, 5 * pi, length.out = 100)
r <- as.vector(sqrt(outer(x^2, y^2, "+")))
grid <- expand.grid(x=x, y=y)
grid$z <- cos(r^2) * exp(-r/(pi^3))
levelplot(z ~ x * y, grid, cuts = 50, scales=list(log="e"), xlab="",
          ylab="", main="Weird Function", sub="with log scales",
          colorkey = FALSE, region = TRUE)

levelplot(z ~ x * y, grid, cuts = 50, scales=list(x=list(at=NULL),y=list(at=NULL)), xlab="",
          ylab="", main="Weird Function", sub="with log scales",
          colorkey = FALSE, region = TRUE)


## triangular end-points in color key, with a title
levelplot(z ~ x * y, grid, col.regions = topo.colors(10),
          at = c(-Inf, seq(-0.8, 0.8, by = 0.2), Inf))
```



