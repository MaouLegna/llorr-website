---
params:
  ind: "03"
  title: "Defining Archetypes #3: A Clustering Analysis to a Season worth of Data"
  description: "Part1: Correction, Introduction and Methods"
title: | 
  `r params$title`
description: |
  `r params$description`
# preview: 
base_url: https://www.llorr-stats.com
author:
  - name: Valentino (Legna) Vazzoler
date: 12-20-2021
output:
 distill::distill_article:
    toc: true
    toc_float: true
    toc_depth: 3
    self_contained: false
citation: false
# slug: legna2021archetype03
# bibliography: references.bib
draft: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  eval = TRUE,
  warning = FALSE,
  error = FALSE,
  message = FALSE,
  comment = NA,
  R.options = list(width = 140,
                   digits.secs=6),
  dev.args = list(bg = 'transparent'), # make graphics with transparent background
  fig.align = 'center',
  fig.width=12,
  fig.height=8,
  engine.path = list(
    python = 'C:/anaconda/'   # -> use_python("C:/anaconda/")
  ),
  #'distill options
  layout="l-body-outset",
  preview=FALSE
)

#' R Option
options(scipen = 999)
source(file.path("C:","LlorR","scripts","lor_main.R" ))
library(captioner)
xaringanExtra::use_panelset()
```

```{r style}
xaringanExtra::style_panelset_tabs(
  font_family = "Helvetica",
  active_foreground = "white",
  hover_foreground = "black",
  hover_border_color = "black",
  active_background = "#007fff"
  )

table_nums  <- captioner::captioner(prefix = "Tab.")
figure_nums <- captioner::captioner(prefix = "Fig.")

f.ref <- function(x) {
  stringr::str_extract(table_nums(x), "[^:]*")
}
```

# Introduction

In the [previous article/analysis](https://llorr-stats.netlify.app/analysis/defining-archetypes-02/) on defining archetypes we introduced the basic theory of Cluster Analysis (CA) and applied to a simple toy-example of Legends of Runeterra (LoR) decks.

We introduced the general theory along side a few widely-used algorithms and some of the "newer" ones like Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Affinity Propagation Clustering (APcluster).

The example was tailored to give a general idea about the methods, some of their strong point and also potential weaknesses with this specific problem.

As the previous analysis was mostly based around the basic approach to the methods and a well defined example it can't translate directly to a generalized use with the commonly analysed data like the weekly reports.

Here, we want to continue to use the Cluster Analysis but this times applied to a set of data on a scale more similar to a 'real-case-study'.

```{r load-data}
LoR.Deck.RMD       <- fread(file.path("C:","LlorR","data","raw","LoR_DECK.csv"),na.strings = c("",NA))

fread(file.path("C:","LlorR","data","raw","games","LoR_MatchDT_S10_218_220.csv"), select=c("game_type","deck_code_1","deck_code_2"), header = T, na.strings = c("", NA)) |>
  filter(game_type=="Ranked") |>
  select(-game_type) |>
  pivot_longer(cols = contains("deck_code"),values_to = "deck_code",names_to = NULL) |>
  distinct(deck_code) |>
  as.data.table() -> S10Decks

fread(file.path("C:","LlorR","data","raw","LoR_MatchDT_S11.csv"), select=c("game_type","deck_code_1","deck_code_2"), header = T, na.strings = c("", NA)) |>
  filter(game_type=="Ranked") |>
  select(-game_type) |>
  pivot_longer(cols = contains("deck_code"),values_to = "deck_code",names_to = NULL) |>
  tabyl(deck_code) |>
  as.data.table() -> S11Decks
```

To give a reference of a 'real-case' number, the Between Worlds (S10) Season which lasted from patch 2.18 to 2.20 featured at Master rank \~`r floor(NROW(S10Decks)/1000)*1000` unique decks.

### An Engineering Problem

What is the main issue when switching to data from a 'real-case study'? It mainly relates to two factor: the *dirtiness* of the data but mostly the amount of decks to use and the resulting computational load.

Distance matrices and consequently all algorithm derived by them have a quadratic growth $O(n^2)$ so scalability is a problem, even more so as this project to "define archetypes" is not meant to be purely theoretical. The aim of this project is to also apply it to the collected data and provide the results, be it in the reports or any other content.

Yet, the problem is not feasible at high dimension even assuming the analysis is self contained and we have an infinite amount of time to apply any algorithm. Time is not the only factor, *Space* is too.  One may not be used to these context so let us give some numbers of the problem at hand.

At the moment of the writing of this article our Deck's dataset is made of about 1M decks. The dataset contains only 40-cards decks which has been played in Constructed or PvP modes. [^1]

Realistically, we would never try to works with all of them at the same time, at least not when using a cluster analysis (CA). A more appropriate time-frame can be a single competitive Season and restrict ourselves only to the decks played around that time.

As we are not ready yet to just simply apply any methods to any kind of decks data we will create a dataset so that is has a similar scale of the S10-MasterRank (`r floor(NROW(S10Decks)/1000)*1000`) decks mentioned before.

For this article we created an example made of `r 55*800` decks. The creation criteria will be explained more in detail later but the code can be seen right ahead. Its size is clearly to have an order of magnitude similar to the S10-MasterRank numbers.

```{r create-example, eval=FALSE, include=FALSE}
set.seed(123)
LoR.Deck.RMD |>
  group_by(factions) |>
  # more than 800 would have trouble for some factions
  slice_sample( n = 800 ) |>
  ungroup() |>
  fwrite("./data/example_archetye_03.csv")

set.seed(123)
Archetype3.20 <- example_archetye_03 |>
  group_by(factions) |>
  slice_sample(n=20) |>
  ungroup()

fwrite(Archetype3.20, "./data/example_archetye_03_20.csv")
cos.dsim.arch3.20  <- cos.dsimMatrix(Archetype3.20$deck_code)
fwrite(as.matrix(cos.dsim.arch3.20),"./data/example_cosDist_03_20.csv")

set.seed(123)
Archetype3.100 <- example_archetye_03 |>
  group_by(factions) |>
  slice_sample(n=100 ) |>
  ungroup()

fwrite(Archetype3.100,"./data/example_archetye_03_100.csv")
cos.dsim.arch3.100 <- cos.dsimMatrix(Archetype3.100$deck_code)
fwrite(as.matrix(cos.dsim.arch3.100),"./data/example_cosDist_03_100.csv")
```

```{r read-example-03}
example_archetye_03 <- data.table::fread("C:/Users/Valentino Vazzoler/Documents/R/llorr-website/data/example_archetye_03.csv",na.strings = c("",NA))
```

```{r read-cos-dsim}
cos.dsim.arch3.20 <- fread("./data/example_cosDist_03_20.csv",header=T) |> 
  as.matrix() |> as.dist()

cos.dsim.arch3.100 <- fread("./data/example_cosDist_03_100.csv",header=T) |>
  as.matrix() |> as.dist()
```

Regarding the space required for this problem, let's talk about the memory allocation of the distance matrix:

-   If we used only a fraction of the chosen decks, just 1100, then we would have a $1100 \times 1100$ distance matrix that requires: `r format(object.size(cos.dsim.arch3.20), units = "Mb")`

-   If we increased the choice to 5500 decks, the resulting 5500x5500 distance matrix would occupy: `r format(object.size(cos.dsim.arch3.100), units = "Mb")`.

Quite the increase but consistent to the fact that the second matrix is 25 times the smaller one and we can see the issue with the quadratic growth that comes with the matrix.

-   By using all the 44k decks we can't even try to recreate the distance matrix, it would require a very good machine and we can infer that just the distance matrix size with its $44000^2$ elements would be of around `r 116*(44000^2/5500^2)` Mb. We wouldn't be able to allocate enough memory to save it, let alone work with it.

[^1]: Constructed, SeasonalTournamentLobby, Bo3ChallengeLobby, StandardGauntletLobby, LastCallQualifierGauntletLobby

Overall, while it would be nice to work with any available data (assuming it's meaningful) one can't ignore the time/cost that comes from each choice and so we need to search for a way to circumnavigate the issue. While we don't have the perfect solution, we are going to propose a way to approach the problem making it a bit more feasable.

# Methods

### Needles in the Shuriman (vast) Desert

```{r fig.width=12, fig.height=8}
# factoextra::fviz_dist(cos.dsim.arch3.20,show_labels = F,
#                       gradient = list(low = "yellow", high = "firebrick3")) +
#   theme(legend.position = "none")
```

Let us assume that we are able to solve the computation and allocation of the distance matrix; this would solve the engineering-side of the problem it would most likely be an incredible 'a waste' of resources.

To explain this we will show the resulting heatmaps of a couple of subsets of the example dataset. One with a subset of 1100 decks and another one with 5500 decks. In both cases the subset decks are chosen not completely at random.

The resulting heatamap with 1100 decks is:

```{r Heat1100, fig.width=8, fig.height=8, fig.cap="heatmap of a 1100 x 1100 distance matrix of LoR decks chosen not completely at random"}
# lattice::levelplot(as.matrix(cos.dsim.arch3.20), main="1100 X 1100 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)))
```

The resulting heatamap with 5500 decks is:

```{r Heat5500, fig.width=8, fig.height=8, fig.cap="heatmap of a 5500 x 5500 distance matrix of LoR decks chosen not completely at random"}
# lattice::levelplot(as.matrix(cos.dsim.arch3.100), main="5500 X 5500 Heatmap", xlab="", ylab="", col.regions=colorRampPalette(c("yellow","firebrick3"), space = "rgb")(10), cuts=9, at=seq(0,1,0.1), scales=list(x=list(at=NULL),y=list(at=NULL)))

# ,colorkey=NULL

# heatmap3 version
# heatmap3::heatmap3(as.matrix(cos.dsim.arch3.20),useRaster=T,Rowv=NA,Colv=NA, col = colorRampPalette(c("yellow","firebrick3"))(1024))
# heatmap3::heatmap3(as.matrix(cos.dsim.arch3.100),useRaster=TRUE,Rowv=NA,Colv=NA, col=colorRampPalette(c("yellow","firebrick3"))(1024))
```

As we can see in Fig:\@ref(fig:Heat1100) and Fig:\@ref(fig:Heat5500) there are clear indication of the presence of several small clusters (the yellow areas) but in the majority, the vast majority of the matrix, the distance is at its max (1) and the similarity is at its min (0). [^2]

[^2]: [0,1] being the domain of the cosine-similarity.

While we need to account for the decks that compose the example, in a *typical real-case* the results would be similar in most cases.

There is a structural reason behind this and that's a result of the limitation imposed by the deck building process which doesn't allow the use of cards from more than two different regions (not counting Dual Region cards). This makes almost all pair of decks decks with no common region having a similarity that has to be zero (again, dual region cards aside).

Our proposed solution is to use this information and start simplify the problem to a series of smaller one cases.

Because of the limitation in deck building we know that the similarity of decks with no common faction has to be zero. A Noxus-PnZ deck has to have a similarity equal to zero with a ShadowIsles-Shurima deck and if we order the deck by faction it means that the distance matrix is made of several $0_{n \times n}$ sub null-matrix.

To be more precise, there are sadly exception to this property: when we deal with dual region cards we can have non overlapping factions but still common cards[^3]

[^3]: BandleCity Ruined more than the Ruination

> Example: a BandleCity/Noxus (BC/NX) deck and a Demacia/Ionia (DE/IO) deck both sharing "Poppy" as a card

For now everything we will do will ignore the dual-region issue and leave it for for future works.

### The Lazy Approach

Let's assume we don't have the problem of the Dual-Region cards and all the 10 regions, what do we know about the structure of the similarity matrix?

We know that there are 55 unique combination of region and so when comparing two decks it can fall into one of $55^2$ possible confrontation. Of course as we know that the Matrix is symmetrical it's reduced to just `r 55*56/2` cases.

Of these `r 55*56/2` cases, as we already explained, most of them are made of null-matrix. But how many exactly?

As we mentioned only when there is at least a common region the similarity can be different from zero. This makes it so the number of sub-matrix to consider follows the following formula:

$$
\sum_i^n((i-1)^2+\frac{i(i+1)}{2})
$$

```{r}
tab.1_cap <- table_nums(name = "tab_1", caption = "Percent of sub-matrix that share a common region by amount of by number of existing regions.")
```

The following table shows the percentage of sub-matrix that can be not-null depending on the number of existing regions:

```{r table-sparse, fig.cap="tab_1"}
#' region to find the percentage all cases of "interceptions" between elements like factions. For example BC/NX does correlate with all BC and all NX cases
regionComboInterception <- function(n) {
  # n = 10
  dim <- n*(n+1)/2
  total = 0
  for ( i in 1:n ) {
    p1 <- i*(i+1)/2
    p2 <- (i-1)^2
    total <- total + p1 + p2
  }
  total/(dim*(dim+1)/2)
}
  
tibble::tibble(
  region = 1:10,
  coverage = map_dbl(1:10, ~regionComboInterception(.x)) ) -> regionSparsity

regionSparsity |>
  gt::gt() |>
  fmt_percent(
    columns = coverage,
    decimals = 2
  ) |>
  tab_header(
    title = "Sparsity of Similarity Matrix",
    subtitle = md("Percent of sub-matrix that share a common region <br> by number of existing regions.")
  ) |>
  gtExtras::gt_theme_538()



# Tab:\@ref(tab:table-sparse)
```
`r table_nums('tab_1')`

From `r f.ref("tab_1")` we can see that in the current setting with n=10 existing regions, of the `r 55*56/2` cases only the `r scales::percent(pull(regionSparsity[10,2]),accuracy = 0.1)` of pairing of combination of regions is actually meaningful with a similarity that can different from zero.

But this is not over as again only a small subset of this `r scales::percent(pull(regionSparsity[10,2]),accuracy = 0.1)` is actually relevant. 

When we compare decks with a single shared region not all comparison are meaningful and what follows is a simplification which works for most cases.

If pairs of decks shares only one regions, for the similarity not to be zero that very same region needs to contains all the common cards. Usually it means that the shared cards are key component to a strategy and unless they are mainly staples they would probably identify an archetype. In the most extreme case the shared region presence may be overwhelming compared to the second one which is used only for the second region unique strength offered by some cards. Like the Rally effects from Demacia.

In other words, when comparing pair of decks with a single shared region, since we aim to look for clusters, the decks that matters are the "bridge" among the regions with a main region that mostly identify the deck.

As easy example is the Mistwraith decks that we also used in the previous article.

Normally Mistwraith decks are made mostly of Shadow Isles cards while the remaining cards are often from a region of choice that synergies with the rest of the deck but are not essential / key-cards. This cards act are the bridge between regions.

Is the deck using as the only not-SI cards 3 copies of Pale Cascade? Then it's a SI/MT deck

Is the deck using as the only not-SI cards any copies of Raz Bloodmane? Then it's a SI/SH deck and SI/MT and SI/SH are connected

Is the deck using as the only not-SI cards 3 copies of Iterative Improvement? Then it's a SI/PZ deck and we connected SH,MT and PZ by SI.

And so on

Of course as mentioned this is more an approximation as one could that a Mistwraith deck with just 20 SI cards and a lot of duplication cards, the deck could still be defined as Mistwraith decks; here we are going to assume that such identification is an over-simplification and it's more likely it would be better to define that '20 SI cards Mistwraith deck' as a sub-archetype of Mistwraith Allegiance. While the main archetype and the sub-archetype would have a similar strategy, we believe it would have a play-pattern that differs enough from the 'main Mistwraith Allegiance' as the amount of cards from the second region would have a greater impact.

In a second step, when trying to find more generalized archetypes (be it something like aggro/control/initiative/resource) then there is reason to try aggregated these different "aggregated version" of Mistwraith decks but as of now, with

Addendum: At the moment of the writing the Ionia-Allegiance decks provides an even more easier example as similar to Mistwraith decks they are often made of only Ionia cards and differ mostly by the choice of the second region.

## Cross-Region Comparisons

We explained how  propose to set a rule so that only these bridge decks are clustered. This open for a consequential step, as we restricted the between regions cases to decks with an overwhelming presence of a region to the other they can be all aggregated into the \~Mono region

What benchmark and rules should we use ?

This is tricky but the idea is to use both an hard defined benchmark and the presence of Allegiance cards.

The choice to include "the use of Allegiance cards" is meant to be a clear and easy to understand rule that at the same times allows for a more "fuzzy separation" of \~Mono decks with the not-mono decks.

In other words the allegiance rule should be able to include some decks that may still point toward a certain strategy that is connected to the region while not investing as much cards from that region.

We will now provide a couple of characteristics of the presence of Allegiance cards in the "Between Worlds Master rank" decks.

```{r allegiance-dataset}
S10Decks |>
  left_join(LoR.Deck.RMD, by = "deck_code") |>
  mutate(cards.region = ifelse(!is.na(cards.region.fix), cards.region.fix, cards.region ) )  |>
  filter(!is.na(allegiance)) |>
  mutate(
    alle = map(allegiance, ~str_split(.x,pattern = ",")[[1]] ),
    alle = map(alle,unique),
    alle = map_chr(alle,~str_flatten(.x,collapse = ",")),
    alle.region = str_sub(alle,3,4)
  ) -> AlleDT
```

```{r table-alle-card}
AlleDT |>
  tabyl(allegiance) |>
  gt::gt() |>
  tab_header(
    title = "Sparsity of Similarity Matrix",
    subtitle = md("Percent of sub-matrix that share a common region <br> by number of existing regions.")
  ) |>
  gtExtras::gt_theme_538()
```

```{r table-alle-freq}
AlleDT |>
  separate_rows(cards.region,cards.region.freq, sep = ",",convert = T ) |>
  select(alle,alle.region,cards.region,cards.region.freq) |>
  filter(alle.region==cards.region) |>
  rename(cards.region.num=cards.region.freq) |>
  filter(!str_detect(alle,",")) |>
  as_tibble() |>
  tabyl(cards.region.num) |>
  rename_with(~c("cards","n","cumfreq")) |>
  arrange(desc(cards)) |>
  mutate(cumfreq=cumsum(cumfreq)) |>
  gt::gt() |>
  tab_header(
    title = "Sparsity of Similarity Matrix",
    subtitle = md("Percent of sub-matrix that share a common region <br> by number of existing regions.")
  ) |>
  gtExtras::gt_theme_538()
```

An idea was to use the inclusion of Allegiance cards as their effect is not a gamble only in deck that either can heavily control their draw or are made mostly of a single region. As LoR offer some card draw control options they are far from common so usually use of Allegiance cards do predict the use of an \~Mono deck.

Sadly this is true, but not entirely. Not only these cards have being played in decks where their activation is very risky but there are even cases there more the Allegiance card for both region has being played in the same deck.

There is also an additional potential failing point of this strategy: the "Theseus Archetype".

This is related to the problem with Mistwraith deck mentioned before. We said that there can be a \~Mono SI version and others SI/XX when the second region has way too many cards to assume it's still the same deck.

Overall this problem is being left for another moment to refine the result of a cluster analysis but conceptually we believe there is merit both in saying there can be more version of the same base-archetype and then we can work on an aggregated one
