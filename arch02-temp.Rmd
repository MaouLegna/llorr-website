# DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a fairly recent algorithm as it was introduced in 1996.

It's the most popular density-based method.
It features a well-defined cluster model called *density reachability*.

This type of clustering techinque connects data points that satisfy particular density criteria (minimum number of objects within a radius). After DBSCAN clustering is complete, there are three types of points: **core,border,and noise**.

Core is a point that has some (m) points within a particular (n) distance from itself.
The Border is a point that has at least one core point at distance n.
Noise is a point that is nether border nor core. Data points in sparse area required to separate clusters are considered noise and broader points.

DBSCAN uses two paramters to determine how clusters are defined:

* minPts: for a region to be considered dense, the minimum number of points requires is *minPts*
* eps: to locate data points in the neighbourhood of any points, *eps $\epsilon$* is used a distance measure

Step by step

* DBSCAN starts with a random data point (not-visited)
* The neighbourhood of this point is extracted using a distance $\epsilon$
* The clustering procedure starts if there are sufficient data points within this area and the current data point becomes the first point in the newest cluster, or else the point is marked as noise and visited
* The point within its $\epsilon$ distance neighborhood also becames a part of the same cluster for the first point in the new cluster. For all the new data points added to the cluster above, the procedure for making all the data points belong to the same cluster is repeated.
* The above two steps are repeated until all points in the cluster are determined. All points within the $\epsilon$ neighbourhood of the cluster have been visited and labelled. Once we're done with the current cluster, a new unvisited point is retrieved and processed, leading to further discovery of the cluster or noise. The procedure is repeated until all the data points are marked as visited.

[@ackerman2010towards,@ackerman2011discerning]

---

Fundumentally, all clustering methods use the same approach i.e. first we calculate similarities and then we use it to cluster the data points into groups or batches. Here we will focus on Density-based spatal clustering of applications with noise (DBSCAN) clustering methods.

Density-Based Clustering refers to unsupervised learning methods that identify distinctive group/clusters in the data, based on the idea that a cluster in data space in a continguous region of high point density, separated from other such clusters by contiguous regions of low point density.

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a base algorithm for density-based clustering. It can discover clusters of different shapes and sizes from a large amount of data, which is containing noise and outliers.

The DBSCAN algorithm uses two paramters: 

* minPts: The minimum number of points (a threshold) clustered toghether for a region to be considered dense.

* eps: A distance measure that will be used to locate the points in the neighbourhood of any point.

These paramters can be understood if we explore two concept called Density Reachability and Density Connectivity

Reachability in terms of density establishes a point to be reachable from another if it lies within a paritcular distance $\epsilon$ from it.

Connectivity, on the other hand involves a transitivity based chaining-approach to determine whether points are located in a particular cluster.

There are three types

Core - a point that has at least m points within distance n from itself
Border a point that has at least one core point at a distance n
Noise this is a pointthat is neither core nor a border and it has less than m points within distance n from itself

---

Usually to find the the value for $\epsilon$ we use the a plot of the k-nearest neighbor (kNN) distances for the dataset with all the distances being plotted from the smallest to largest.

In R, for the package 'dbscan' the euristic method to find $\epsilon$ is a follows:

> A suitable neighborhood size parameter eps given a fixed value for minPts can be found visually by inspecting the kNNdistplot of the data using k = minPts -1 (minPts includes the point itself, while the k-nearest neighbors distance does not).
The k-nearest neighbor distance plot sorts all data points by their k-nearest neighbor distance.
A sudden increase of the kNN distance (a knee) indicates that the points to the right are most likely outliers.
Choose eps for DBSCAN where the knee is.

Since the example data set data is quite small we will assign minPts=2  and so using k=1

```{r dbscan-opt, fig.width=6, fig.height=4}
set.seed(123)
dbscan::kNNdistplot(DSim_mini, k =  1)
abline(h = 0.2775, lty = 2, col = "red")
```
We will do the clustering with $\epsilon$=0.2775 and using both the cosine similarity and manhattan distance.

The manhattan distance is to use the same methodology of Drisoth

```{r}
set.seed(123)
res.db <- dbscan::dbscan(DSim_mini, 0.2775, 2)
set.seed(123)
res.db.Drisoth <- dbscan::dbscan(dist(sparseMatrix.mini, method = "manhattan", diag = FALSE, upper = FALSE), 37, 2)
```

When applied to the example data set we get:

```{r}
res.db
```

DBSCAN with the cosine similarity, $\epsilon$=0.2775 and minPts=2 return 5 cluster with 2 noise points

```{r}
res.db.Drisoth
```

DBSCAN with the manhattan distance, $\epsilon$=0.2775 and minPts=2 returns again 5 clusters but only 1 noise point


### Does DBSCAN scale well?

In this article we introduced the problem of a lack of consideration of a clustering properties in order to decide which clustering algorithm to apply to our data set.

A series of weight-related properties were introduced to make aware how some linkage-based algorithm can be influenced or not by weighted data.

More generally it's possible to define a wide series of clustering methods properties [@ackerman2010towards,@ackerman2011discerning]. Among them there is property that we believe should be present for any clustering methods we are planning to use for the archetype problem: Locality

```{=tex}
\begin{equation}

$\forall$ domain (X,d) and number clusters, k, if X' is the union of k' clusters in F(X,d,k) for some $k'\leq k $ then, applying F to (X',d) and asking for a k'-clustering, will yield the same clusters that started with (\#eq:locality)

\end{equation}
```

It is pretty clear why we wants this property. It we cluster a subset of an archetype, we should still get the a result consistent with the same archetype.

The reason we wondered for DBSCAN is this property was at risk it the presence of the minPts hyper-paramrter. Does it imply that we can take a subset of the data set maintaining the same setting and getting a different result.

We can say that DBSCAN satisfy this property.
This is easily proved by the existance of the DBSCAN extension HDBSCAN which convert DBSCAN into an hierarchical algorithm.
Because of HDBSCAN we can then use the following Lemma introduced by Ackerman [@ackerman2011discerning]

```{=tex}
\begin{lemma}

Every linkage-based hierarchical clustering function satisfies locality and outer-consistency (\#eq:lemma8)

\end{lemma}
```

What is the linkage method applied by DBSCAN? Simple-linkage. In fact, DBSCAN search the the minPts at a $\epsilon$ distance which is a more constrained use of single-linkage.

We can also point that locality specify that the sub-set in made of k' clusters so it exclude the choice of picking a subset of a mixture of noise points and a partial subset of a cluster. Because of this we know that the cluster will satisfy again the definition of cluster by DBSCAN allowing locality to be satisfied.

> TLDR: DBSCAN is a fine good as an algorithm to define archetypes. It may have other reasons to make us not completely sure about using this algorithm but this key property is satisfied.

# K-Medoids (results)

We report the results in seeking the optimal number of clusters using K-medoid using two different metric, the Manhattan distance (or L1 distance) and the Cosine similarity to the example data set.

As with the other methods the results will be discussed at the end of the article.

```{r elbow-medoid, fig.width=12, fig.height=6, fig.cap="within sum of squares as a fnction of number of clusters obtained with K-medoids and manhattan & cosine metric applyed to the example data set"}
set.seed(123)
fviz_nbclust(sparseMatrix.mini, pam, method = "wss", metric="manhattan") +
  labs(subtitle = "Manhattan distance") +
  theme_Publication() +
fviz_nbclust(as.matrix(DSim_mini), pam, method = "wss") +
  labs(subtitle = "Cosine similarity") +
  theme_Publication()
```

# Affinity Propagation Clustering

Affinity Propagation Clustering (APcluste) is a fairly recently introduced clustering methods @frey2007clustering.

It is based on the passage of messagges between data points to detect patterns in data.

As K-Medoids it uses examplars data point.

Two messagges are exchanged between data points:

* **Responsibility** r(i,k) is sent from *i to k* and correspond to the *accumulated evidence* for how well-suited k is to server as the examplar for i, taking into account other potential exemplars for point i

* **Availability** a(i,k) is sent from *k to i* and correspond to the *accumulated evidence* for how appropriate it is for i to choose k as its examplar, taking into account the support from other points that point k should be examplar

The iteration steps updates the following values:

* $R(i,k) \leftarrow S(i,k) - max_{k' \neq k}(A(i,k')+S(i,k'))$

* $A(i,k) \leftarrow min{(0,R(k,k')+\sum_{j \notin {i,k}}max(0,R(j,k) ) )}$

* $A(k,k) \leftarrow max_{j \neq k}(0,R(j,k))$

The examplar are chosen as the point from whom responsibility+availability for themselves is positive $r(i,i)+a(i,i)>0$

S(i,k) is the similarity matrix we provide which doesn't even need to satisfy the usual metric property, not only it doesn't have to satisfy the triangular inequality but it doesn't even need to be symmetric.

In this example we will provide the similarity matrix provided by the use of the cosine similarity. What follows in the heatmap obtained by applying APcluster to the example data set.

As APcluster is less intuitive to understand compared to the other methods

```{r heat-apcluster, fig.width=12, fig.height=8, fig.cap="Heatmap obtained by applying APcluster to the example data set", preview=T}
# identical(
#   linKernel(DSim_mini,normalize = T)[1:5,1:5],
#   1-as.matrix(eisen_cos.sim(DSim_mini))[1:5,1:5]
# )
set.seed(123)
ap.res <- apcluster(linKernel(DSim_mini,normalize = T),details = T)

ap.heat <- apcluster::heatmap(
  ap.res,linKernel(DSim_mini,normalize = T),
  # Rowv=NA, Colv=NA,
  # cexRow= 0.75, cexCol = 0.75,
  cexRow= 0, cexCol = 0,
  # sideColors=c("darkgreen", "yellowgreen"),
  # col=terrain.colors(12),
  legend = "col"
  )

# png(filename = "./images/archetypes/A02_APheatmap.png", width = 640, height = 640)
```

Similarly to most of the other methods the algorithm is suggesting us the presence of 5 archetypes with one of them being a little less homogeneous compared to the other 4.

As we finally showed all the algorithm we planned to introduce and explain we can finally describe the data set used an show more in the detail the results we had.

# Data

The example used in this article is made out of 50 decks.

-   Five archetypes with different regions & different play style have been selected as 'core'

    1.  Ashe/LeBlanc
    2.  Azir/Irelia
    3.  Dragons (DE/MT)
    4.  Draven/Sion (NX/PZ)
    5.  Mistwraith Alligiance

Does this mean that the correct answer of k-cluster we should have expected was 5? Well... not necessarily, there is fact a catch that I didn't mention. Among the 'archetypes' I also selected some of their sub-archetypes.

To help explain what I actually choose I'll also report the code I used to samle the deck_list

```{r select-archetype, echo=TRUE, eval=FALSE}
# how I selected the decks, it is reproducible as more decks will be inserted in the DB

#' archetypes names
archetypes <- c( 
  "Ashe/LeBlanc",           # 75/25 noMarauder/Marauder
  "Azir/Irelia",            # 100
  "Dragons (DE/MT)",        # 30/70 J4/PureDrake "Aurelion Sol/Jarvan IV/Shyvana",  "Aurelion Sol/Shyvana",
  "Draven/Sion (NX/PZ)",    # 80/20 DravenSion/RubinBait
  "Mistwraith Allegiance"   # 50/50 Targon/Piltover
)

#' sampling a bigger number of decks for a bigger example I ended up not using
set.seed(123)
Rep1 <- LoR.Deck.RMD |>
  filter( archetype == "Ashe/LeBlanc" & is.na(archetype_pretty) ) |>
  slice_sample(n = 75) |>
  pull(deck_code)

set.seed(123)
Rep2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Marauder" ) |>
  slice_sample(n = 25) |>
  pull(deck_code)

set.seed(123)
AI <- LoR.Deck.RMD |>
  filter( archetype == "Azir/Irelia", ) |>
  slice_sample(n = 100) |>
  pull(deck_code)

set.seed(123)
Dragon1 <- LoR.Deck.RMD |>
  filter( archetype == "Aurelion Sol/Jarvan IV/Shyvana" ) |>
  slice_sample(n = 30) |>
  pull(deck_code)

set.seed(123)
Dragon2 <- LoR.Deck.RMD |>
  filter( archetype == "Aurelion Sol/Shyvana" ) |>
  slice_sample(n = 70) |>
  pull(deck_code)

set.seed(123)
Sion1 <- LoR.Deck.RMD |>
  filter( archetype == "Draven/Sion (NX/PZ)" & is.na(archetype_pretty) ) |>
  slice_sample(n = 80) |>
  pull(deck_code)

set.seed(123)
Sion2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "RubinBait - Draven/Sion", ) |>
  slice_sample(n = 20) |>
  pull(deck_code)

set.seed(123)
Mist1 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Targon") ) |>
  slice_sample(n = 50) |>
  pull(deck_code)

set.seed(123)
Mist2 <- LoR.Deck.RMD |>
  filter( archetype_pretty == "Mistwraith Allegiance" & str_detect(factions,"Piltover") ) |>
  slice_sample(n = 50) |>
  pull(deck_code)

#' sub-sampling the decks I ended up not using
LoR.Archetype.Ex <- rbind(
  LoR.Deck.RMD[ deck_code %in% Rep1, ],
  LoR.Deck.RMD[ deck_code %in% Rep2, ],
  LoR.Deck.RMD[ deck_code %in% AI, ],
  LoR.Deck.RMD[ deck_code %in% Dragon1, ],
  LoR.Deck.RMD[ deck_code %in% Dragon2, ],
  LoR.Deck.RMD[ deck_code %in% Sion1, ],
  LoR.Deck.RMD[ deck_code %in% Sion2, ],
  LoR.Deck.RMD[ deck_code %in% Mist1, ],
  LoR.Deck.RMD[ deck_code %in% Mist2, ]
)
  
mini.ex <- c(Rep1[1:7], # Ashe/LB
             Rep2[1:3], # Marauder
             AI[1:10],  # AI
             Dragon1[1:3], # with J4
             Dragon2[1:7], # no J4
             Sion1[1:8],   # OG
             Sion2[1:2],   # fake-burn
             Mist1[1:5],   # MT
             Mist2[1:5])   # PnZ

```
```{r}
archetype.v2 <- c(
  rep("Ashe/LeBlanc",7),
  rep("Marauder",3),
  rep("Azir/Irelia",10),
  rep("Dragons+J4",3),
  rep("Dragons",7),
  rep("Draven/Sion",8),
  rep("RubinBait-Sion",2),
  rep("Mistwraith(MT)",5),
  rep("Mistwraith(PnZ)",5)
)

# archetype.v2
```


So if I had to better specify what the sample contains:

-   Five archetypes with different regions & different play style have been selected as 'core'. Almost all archetypes contains some variants of the same decks 

    1.  Ashe/LeBlanc
      * 7 decks of 'standard' Ashe/LB
      * 3 decks of the 'marauder' variant which is defined by playing 3 copies of Strength in Numbers in a Freljord/Noxus deck. No Ashe/LB could be sampled from this subgroup
    2.  Azir/Irelia
      * 10 decks of Azir/Irelia
    3.  Dragons (DE/MT)
      * 3 decks with at least Jarvan IV/Shyvana/Aureion Sol
      * 7 decks with at least Shyvana/Aureion Sol (but no Jarvan IV)
    4.  Draven/Sion (NX/PZ)
      * 8 decks of 'standard' Draven/Sion
      * 2 decks of the Rubin-Bait variant
    5.  Mistwraith Alligiance - Mistwraith Alligiance decks are defined by having 3 copies Mistwraith and overall 37 SI cards
      * 5 decks of a Mistwraith decks with Mount Targon and 3 copies of Pale Cascade
      * 5 decks of a Mistwraith decks with PnZ and 3 copies of Iterative Improvements

Something that I want to highlight of the Mistwraith decks is that the option for the champions of choice was completely free as they were in the end not relevant for the strategy of the deck and the following table shows how the champions combination was distributed:

```{r}
LoR.Archetype.Mini[41:50,] |> tabyl(archetype) |> gt()
```

The Mistwraith example could be said if a key for why and  how I want to approach the archetype problem. SI is probably the region who is less tied by its champions per se. Thresh is more an exception but for example Kallista and Elise were included in a wide variety of decks that couldn't be simply identified by their use.

In addition to the 'champions problem' the Mistwraith has the additional problem of being the first archetype to this is not bound by the regions it use. As of now Bandle expanded this concept with a variety of BandleTree Decks, but Mistwraith was the OG case.

## Resuls

### K-Means

```{r res-kmeans}
set.seed(123)
km.5 <- kmeans(sparseMatrix.mini, 5, nstart = 25)
set.seed(123)
km.7 <- kmeans(sparseMatrix.mini, 7, nstart = 25)

table(
  archetype.v2,
  unname(km.5$cluster)
) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,6), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7,8), bold = TRUE, col = "darkgreen") |>
  row_spec(c(5,9), bold = TRUE, col = "purple")

table(
  archetype.v2,
  unname(km.7$cluster)
) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,6), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7,8), bold = TRUE, col = "darkgreen") |>
  row_spec(c(5,9), bold = TRUE, col = "purple")

LoR.Archetype.Mini[39,deck_code]
```
We can see why the K-Means wasn't returning 5 as the number of suggested clusters as the other methods, because it was more prone to notice the differences between the Reputation decks and the SI Mistwraith decks. Strangely enough it ddoesn't seems to notice the differences between the Sion decks.

### Hierachical Clustering

We report only the results with Average-linkage as we don't have reasons to suggest out data would work well with Single/Complete linkage and we showed that these choice would impact the results in many ways. So we opt for Average-linkage as it is a compromise of most methods.

```{r res-hclust, fig.width=9, fig.height=6}
fviz_dend(hcut(DSim_mini, k = 5, method = "average"),
          k_colors = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07"),
          label_cols =  rep(1:5,each = 10),
          rect = TRUE, cex = 0.5 )
```

With the hierarchical clustering we have can not only how clearly the method grouped the archetypes but the similarities between the groups and how some archetypes may suggest the presence of sub-archetypes similarly by how we choose them, at least for Dragons and Sion. As Mistwraith decks contain an higher range in similarity cutting the dendogram in more than 5 archetypes would first and foremost identify the differences in SI decks.

Overall the methods can provide a useful tool to explore the cluster but suffer from the fact that it doesn't scale well at the increase of sample size, not just in term of memory/cost but as how to find a proper level to cut the tree.

### K-Medoid

The K-Medoid applied with 5 cluster had the same results of all the other methods but differently from the other it was able to identify a couple of the Marauder decks when he used 7 clusters. As it is probably the more interesting result we will report the one with 7 cluster.

```{r}
set.seed(123)
kmed <- pam(DSim_mini, k = 7)

table(
  archetype.v2,
  kmed$clustering
) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,6), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7,8), bold = TRUE, col = "darkgreen") |>
  row_spec(c(5,9), bold = TRUE, col = "purple")
```
With K-Medoids the algorithm also provide the examplars that identified correctly most (2/3) of the Marauder decks and assigned the Mistwraith-Viego deck to its own cluster.

```{r}
LoR.Archetype.Mini[kmed$id.med,.(archetype,deck_code)] |>
  add_column(archetype.v2 = c("Ashe/LB","Marauder","Azir/Irelia","Dragons","Dragon/Sion","Mistwraith","Mistwraith")) |>
  gt() |>
  gtExtras::gt_theme_nytimes()
```



### DBSCAN

As DBSCAN also introduce noise points we want to check which points have been assigned as such.

Not surprisingly the noise was found in the Mistwraith decks always including as noise the 'Viego' variant. With DBSCAN we have to remember it could be wiser to test for a wider range of hyper-paramters minPts and $\epsilon$.

Yet, the main cons of this method is exactly how to define minPts expecially if we used a bigger/real dataset as we have no idea about the distribution of the archetypes lists.

Overall this method can be a valuable source to check for outliers in the archetypes list but we are not sure its definition of noise of appropriate for the archetype problem.


```{r res-DBSCAN}
table(
  LoR.Archetype.Mini$archetype,
  res.db$cluster
) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,2), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7:8,10), bold = TRUE, col = "darkgreen") |>
  row_spec(c(9,11), bold = TRUE, col = "cadetblue") |>
  row_spec(c(6), bold = TRUE, col = "purple")

table(
  LoR.Archetype.Mini$archetype,
  res.db.Drisoth$cluster
) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,2), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7:10), bold = TRUE, col = "darkgreen") |>
  row_spec(c(11), bold = TRUE, col = "cadetblue") |>
  row_spec(c(6), bold = TRUE, col = "purple")
```

### APcluster

Lastly the APcluster algorithm

```{r res-apclister}
table(archetype.v2,
      ap.res@idx) |>
  kable(row.names = T) |>
  kable_paper(full_width = F) |>
  row_spec(c(1,6), bold = TRUE, col = "red") |>
  row_spec(c(3,4), bold = TRUE, col = "blue") |>
  row_spec(c(7,8), bold = TRUE, col = "darkgreen") |>
  row_spec(c(5,9), bold = TRUE, col = "purple")
```
The column names refers to the id of the examplars chosen to represent the archetype

```{r heat-apcluster.v2, fig.width=12, fig.height=8, fig.cap="Heatmap with labels obtained by applying APcluster to the example data set"}
apcluster::heatmap(
  ap.res,linKernel(DSim_mini,normalize = T),
  Rowv=NA, Colv=NA,
  cexRow= 0.75, cexCol = 0.75,
  # cexRow= 0, cexCol = 0,
  # sideColors=c("darkgreen", "yellowgreen"),
  # col=terrain.colors(12),
  # legend = "col"
  )
```

As now it could be expected the 'less homogeneous' cluster was the one from Mistwraith decks.

If we wanted we can apply an agglomerative clustering on top of AP clustering.

```{r dendo-apcluster, fig.width=12, fig.height=6, fig.cap="Dendogram obtained by applying APcluster to the example data set"}
set.seed(123)
aggres <- aggExCluster(linKernel(DSim_mini,normalize = T))
plot(aggres, cex=0.1)
```

And lastly as K-Medoid we can see the examplars provided by the algorithm

```{r}
LoR.Archetype.Mini[unname(ap.res@exemplars),.(archetype,deck_code)] |>
  add_column(archetype.v2 = c("Ashe/LB","Azir/Irelia","Dragons","Dragon/Sion","Mistwraith")) |>
  gt() |>
  gtExtras::gt_theme_nytimes()
```
Again, the Burn variant of Draven/Sion looks way confounded in its own archetype, the remaining results are similar to the other methods. The results could overall be more optimzed of affinity propagation clustering is an extremely flexible method but we lack for now experience with it.

# Conclusion

Clustering is truly a wide and troublesome (and fascinating) domain. As  there is no clear definition about how to define an archetype in Legends of Runeterra finding an appropriate approach to classify them based on clustering analysis is no easy task.

While most cluster algorithm used in this articles were able to differentiate for the macro-archetypes selected for this example the example of Draven/Sion decks shows that some variant that is recognized by the community as a different deck.

A better definition of the problem seems to be crucial in order to approach the archetype problem as, as of now it mostly relies on intuition and shared agreements about how to consider certain decks.

Even if the results did agreed for the most part among all the algorithm the degree of freedom left to the user that could hinder the results has to be taken into account for which methods are worth exploring with bigger/real examples.
K-Means is the method that overall we would consider the worst as it's affect by the randomized initialisation step, the choice of k-cluster that is guided 'not as good' as the methods that can provide dendograms or exemplars to further explore the choices and results.
K-Medoids while a more robust version of K-Means still suffer from the initialisation step.
Hierarchical clustering another basic clustering algorithm is a promising option to check for possible sub-archetypes or differences within cluster. The main concern is the scale with bigger data-set as the required dissimilarity matrix has a quadratic increase of the sample size.

```{r twitter-meta, echo = FALSE}
# metathis::meta() |>
#   metathis::meta_description(params$description) |> 
#   metathis::meta_viewport() |> 
#   metathis::meta_social(
#     title = params$title,
#     url = "https://www.llorr-stats.com/",
#     image = ,
#     image_alt = "images/archetypes/A02_APheatmap.png",
#     og_type = "website",
#     og_author = "Legna",
#     twitter_card_type = "summary",
#     twitter_creator = "@Maou_Legna"
#   )
```

# Legal bla bla

This content was created under Riot Games' "Legal Jibber Jabber" policy using assets owned by Riot Games.
Riot Games does not endorse or sponsor this project.
